{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cristobalperezp/CC6205-NLP/blob/main/Tarea_3_Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckbt7VPDhBwb"
      },
      "source": [
        "# **Tarea 3 - Word Embeddings üìö**\n",
        "\n",
        "**Integrantes:**\n",
        "\n",
        "**Fecha l√≠mite de entrega üìÜ:** 16 de mayo.\n",
        "\n",
        "**Tiempo estimado de dedicaci√≥n:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-19T18:30:18.109327Z",
          "start_time": "2020-03-19T18:30:18.103344Z"
        },
        "id": "q5CSRY4oNCHK"
      },
      "source": [
        "\n",
        "**Instrucciones:**\n",
        "- El ejercicio consiste en:\n",
        "    - Responder preguntas relativas a los contenidos vistos en los v√≠deos y slides de las clases.\n",
        "    - Implementar el m√©todo de la Word Context Matrix. \n",
        "    - Entrenar Word2Vec y FastText sobre un peque√±o corpus.\n",
        "    - Evaluar los embeddings obtenidos en una tarea de clasificaci√≥n.\n",
        "- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo **Jupyter Notebook**.\n",
        "- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n. \n",
        "\n",
        "\n",
        "**Referencias**\n",
        "\n",
        "V√≠deos: \n",
        "\n",
        "- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n",
        "- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n",
        "- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4wYf0vgnbTv"
      },
      "source": [
        "## **Preguntas te√≥ricas üìï (3 puntos).** ##\n",
        "Para estas preguntas no es necesario implementar c√≥digo, pero pueden utilizar pseudo c√≥digo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5hUG6-8ngoK"
      },
      "source": [
        "### **Parte 1: Modelos Lineales (1.5 ptos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yRvZbhsoi8f"
      },
      "source": [
        "Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categor√≠as: pol√≠tica, deporte, negocios y otros. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irsqBVmCnx3M"
      },
      "source": [
        "**Pregunta 1**: Dise√±e un modelo lineal capaz de clasificar un documento seg√∫n estas categor√≠as donde el output sea un vector con una distribuci√≥n de probabilidad con la pertenencia a cada clase. \n",
        "\n",
        "Especifique: representaci√≥n de los documentos de entrada, par√°metros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y funci√≥n de p√©rdida escogida. **(0.75 puntos)**\n",
        "\n",
        "**Respuesta**: \n",
        "\n",
        "Del dataset dado, se obtiene una representaci√≥n TF-IDF del mismo que corresponder√° al input del modelo, denominado por $\\vec{X}$, d√≥nde en las columnas se encuentra el vocabulario del dataset, en las filas los documentos y en cada entrada el TF-IDF correspondiente.\n",
        "\n",
        "Para la clasificaci√≥n de los documentos se utilizar√° el modelo log-lineal multiclase, d√≥nde el output viene dado por:\n",
        "$$\\vec{\\hat{y} } = softmax( \\vec{X}*W + \\vec{b})$$\n",
        "con $softmax(\\vec{z})_{[i]} = \\frac{e^{\\vec{x}_{[i]}}}{ \\sum_{j} e^{\\vec{x}_{[j]}} }$\n",
        "\n",
        "As√≠:\n",
        "$$\\vec{\\hat{y}}_{[i]} = \\frac{e^{(\\vec{x}*W + \\vec{b})_{[i]}}}{ \\sum_{j} e^{(\\vec{x}*W + \\vec{b})_{[j]}} }$$\n",
        "\n",
        "Con la transformaci√≥n softmax se forza a que los valores de las componentes de $\\vec{\\hat{y}}$ sean positivas y sumen uno, siendo as√≠ la probabilidad de pertenencia a cada clase.\n",
        "\n",
        "Los par√°metros del modelo son $W$ y $\\vec{b}$, y la funci√≥n de p√©rdida corresponder√° a la cross-entropy, dada por:\n",
        "\n",
        "$$L_{cross-entropy}(\\vec{\\hat{y}},\\vec{y}) = -\\sum_{i} \\vec{y}_{[i]}*log(\\vec{\\hat{y}}_{[i]})$$\n",
        "\n",
        "Se utiliza esta funci√≥n de p√©rdida, ya que e objetivo buscado es obtener una interpretaci√≥n probabil√≠stica dede los scores de cada clase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5FaWqBVvL90"
      },
      "source": [
        "**Pregunta 2**: Explique c√≥mo funciona el proceso de entrenamiento en este tipo de modelos y su evaluaci√≥n. **(0.75 puntos)**\n",
        "\n",
        "**Respuesta**:\n",
        "\n",
        "Para el entrenamiento del modelo log-lineal se encontrar los par√°metros $W$ y $\\vec{b}$ que minimizan la p√©rdida, esto queda escrito como:\n",
        "$$\\hat{\\Theta} = (W,\\vec{b}) = argmin_{\\Theta} \\frac{1}{n} \\sum_{i=1}^{n} L_{cross-entropy}(\\vec{\\hat{y}},\\vec{y})$$\n",
        "\n",
        "$$= argmin_{(W,\\vec{b})} \\frac{1}{n} \\sum_{i=1}^{n} L_{cross-entropy}(softmax( \\vec{X}*W + \\vec{b}),\\vec{y}) $$\n",
        "\n",
        "En consecuencia de no tener una soluci√≥n cerrada, se utiliza el algoritmo de descenso de gradiente estoc√°stico, que consiste de:\n",
        "\n",
        "* Inicializar los par√°metros con valores aleatorios o cero.\n",
        "* Por cada iteraci√≥n se calcula la p√©rdida con los valores actuales de los par√°metros.\n",
        "* Luego se actualizan siguiendo la siguiente regla hasta su convergencia:\n",
        "* $\\Theta = \\Theta - ≈ã*\\nabla_{\\Theta}L_{cross-entropy}(\\vec{\\hat{y}},\\vec{y})$\n",
        "\n",
        "Los criterios de convergencia se definen antes, y se pueden utilizar: n√∫mero de iteraciones determinado o el tama√±o del paso dado entre iteraciones es menor a un umbral especificado.\n",
        "\n",
        "Como se busca obtener un modelo capaz de generalizar, se utiliza un split de la data, generando sub sets para entrenamiento, validaci√≥n y test, de esta forma, con el dataset de entrenamiento se entrena el modelo, con el de validaci√≥n se obtienen los valores √≥ptimos de los hiperpar√°metros, y en testing se pone a prueba la performance del modelo utilizando una m√©trica de desempe√±o ad-hoc al problema de clasificaci√≥n como *accuracy, presicion o recall*, entre otras.\n",
        "\n",
        "---\n",
        "El valor de ≈ã corresponde a un hiperpar√°metro del modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkK7pc54njZq"
      },
      "source": [
        "### **Parte 2: Redes Neuronales (1.5 ptos)** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUbJjlj_9AFC"
      },
      "source": [
        "Supongamos que tenemos la siguiente red neuronal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obUfuOYB_TOC"
      },
      "source": [
        "![image.png](https://drive.google.com/uc?export=view&id=1nV1G0dOeVGPn40qGcGF9l_pVEFNtLU-w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2z-8zKW0_6q"
      },
      "source": [
        "**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matem√°tica. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en funci√≥n del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n",
        "\n",
        "Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.75 Puntos)**\n",
        "\n",
        "**Respuesta**: \n",
        "\n",
        "Formula:\n",
        "\n",
        "Si $f,g,h$ son las funciones de activaci√≥n desde la primera capa a la tercera, y utilizando la notaci√≥n vista en clases:\n",
        "$$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) = (h(g(f(\\vec{x}W^1 + \\vec{b^1})W^2 + \\vec{b^2}))W^3 + \\vec{b^3})W^4$$\n",
        "\n",
        "Dimensiones: \n",
        "\n",
        "Como $\\vec{x}$ est√° en $\\Re^3$:\n",
        "\n",
        "$dim(W^1)={3x2}$ y $dim(\\vec{b^1})={2}$\n",
        "\n",
        "$dim(W^2)={2x3}$ y $dim(\\vec{b^2})={3}$\n",
        "\n",
        "$dim(W^3)={3x1}$ y $dim(\\vec{b^1})=1$\n",
        "\n",
        "$dim(W^4)={1x4}$\n",
        "\n",
        "**Pregunta 2**: Explique qu√© es backpropagation. ¬øCuales ser√≠an los par√°metros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n",
        "\n",
        "**Respuesta**:\n",
        "\n",
        "Es un m√©todo para evaluar el gradiente de una funci√≥n de p√©rdida $L$, para una red neuronal feed-forward, con respecto a todos sus par√°metros, que utiliza la regla de la cadena y programaci√≥n din√°mica.\n",
        "\n",
        "Los par√°metros a evaluar en la raed anterior durante backpropagation, son $W^1,W^2,W^3,W^4$ y $\\vec{b^1},\\vec{b^2},\\vec{b^3}$ .\n",
        "\n",
        "**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.5 puntos)**\n",
        "\n",
        "**Respuesta**:\n",
        "\n",
        "El algoritmo puede ser resumido en los siguientes pasos:\n",
        "1.  Aplicar un vector de entrada $\\vec{x}$ a la red y propagar hacia adelante a trav√©s de la red utilizando (eq.1)* y (eq.2) para encontrar las activaciones de todas las unidades ocultas y de salida.\n",
        "$$eq.1: \\vec{h_{[j]}^{l}}=(\\sum_{i} W_{[i,j]}^{l}\\ x\\ \\vec{z_{[i]}^{l-1}})+\\vec{b_{j}^{l}}$$\n",
        "$$eq.2:  \\vec{z_{[j]}^{l}}=g(\\vec{h_{[j]}^{l}})$$ con $\\vec{z_{[j]}^{0}} = \\vec{x_{[j]}}$\n",
        "\n",
        "2. Evaluar $\\vec{\\delta_{[j]}^{m}}$ para todas las unidades de salida.\n",
        "\n",
        "3. Hacer backpropagation en $\\vec{\\delta_{[k]}^{l+1}}$ utilizando (eq.3) para obtener $\\vec{\\delta_{[j]}^{l}}$ para cada capa oculta de la red. Pasando de las capas superiores a las inferiores de la red**.\n",
        "$$eq.3: \\vec{\\delta_{[j]}^{l}}=g^{'}(\\vec{h_{[j]}^{l}})\\ x\\ \\sum_{k}(\\vec{\\delta_{[k]}^{l+1}}\\ x\\  W_{[j,k]}^{l+1})$$\n",
        "\n",
        "4. Evaluar las derivadas requeridas utilizando $\\frac{\\delta L}{\\delta W_{[i,j]}^{l}}=\\vec{\\delta_{[j]}^{l}}\\ x\\ \\vec{z_{[i]}^{l-1}}$\n",
        "\n",
        "\n",
        "---\n",
        "*Los sesgos $\\vec{b}$ pueden ser excluidos de (eq.1), incluy√©ndolos en las matrices $W$ introduciendo una unidad adicional, o entrada, cuya activaci√≥n se fija en +1.\n",
        "\n",
        "**Aqu√≠ vemos las derivadas que hay que computar para obtener $\\vec{\\delta_{[j]}^{l}}$ en cada capa, es decir las derivadas de las funciones $f,g,h$ ($g$ en la ecuaci√≥n son las activaciones).\n",
        "\n",
        "As√≠:\n",
        "$$\\vec{\\delta^4} = \\frac{\\delta L}{\\delta \\vec{\\hat{y}}}$$\n",
        "$$\\vec{\\delta^3} = h^{'}(\\vec{h^3})\\vec{\\delta^4}W^4 $$ \n",
        "$$\\vec{\\delta^2} = g^{'}(\\vec{h^2})\\vec{\\delta^3}W^3$$\n",
        "$$\\vec{\\delta^1} = f^{'}(\\vec{h^1})\\vec{\\delta^2}W^2$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocS_vQhR1gcU"
      },
      "source": [
        "## **Preguntas pr√°cticas üíª (3 puntos).** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parte 3 A (1 Punto): Word Contex Matrix"
      ],
      "metadata": {
        "id": "D0wk5GBkSE73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template (para esta parte puede utilizar las librer√≠as ```numpy``` y/o ```scipy```). Hint: revise como utilizar matrices sparse de ```scipy```\n",
        "\n",
        "```python\n",
        "class WordContextMatrix:\n",
        "\n",
        "  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Utilice el constructor para definir los parametros.\n",
        "    \"\"\"\n",
        "\n",
        "    # se sugiere agregar un una estructura de datos para guardar las\n",
        "    # palabras del vocab y para guardar el conteo de coocurrencia\n",
        "    # si lo necesita puede agregar m√°s parametros pero no puede cambiar el resto\n",
        "    ...\n",
        "    \n",
        "  def build_vocab(self, word):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para construir el vocabulario\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    # Le puede ser √∫til considerar un token unk al vocab\n",
        "    # para palabras fuera del vocab\n",
        "    ...\n",
        "  \n",
        "  def build_matrix(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para crear la palabra contexto\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  def get_matrix(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para obtener la matriz palabra contexto. \n",
        "    \"\"\"\n",
        "\n",
        "    # se recomienda transformar la matrix a un diccionario de embedding.\n",
        "    # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n",
        "    ...\n",
        "\n",
        "```\n",
        "\n",
        "puede modificar los par√°metros o m√©todos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n",
        "\n",
        "```python\n",
        "corpus = [\n",
        "  \"I like deep learning.\",\n",
        "  \"I like NLP.\",\n",
        "  \"I enjoy flying.\"\n",
        "]\n",
        "```\n",
        "\n",
        "Obteniendo una matriz parecia a esta:\n",
        "\n",
        "***Resultado esperado***: \n",
        "\n",
        "| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n",
        "|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n",
        "| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n",
        "| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n",
        "| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n",
        "| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n",
        "| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n",
        "| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n",
        "| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n",
        "| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n",
        "\n",
        "``\n",
        "\n",
        "Verifique si su matrix es igual a esta utilizando el corpus de ejemplo. Ojo que este es s√≥lo un ejemplo, su algoritmo debe **generalizar** a otros ejemplos."
      ],
      "metadata": {
        "id": "e_mh12Z9SF-J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol82nJ0FnmcP"
      },
      "source": [
        "### **Parte 3 B (1 Punto): Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgmeSFqKLpFL"
      },
      "source": [
        "En la auxiliar 2 aprendieron como entrenar Word2Vec utilizando gensim. El objetivo de esta parte es comparar los embeddings obtenidos con dos modelos diferentes: Word2Vec y [FastText](https://radimrehurek.com/gensim/models/fasttext.html) (utilizen size=200 en FastText) entrenados en el mismo dataset de di√°logos de los Simpson. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecCvnryeQiG7"
      },
      "source": [
        "import re  \n",
        "import pandas as pd \n",
        "from time import time  \n",
        "from collections import defaultdict \n",
        "import string \n",
        "import multiprocessing\n",
        "import os\n",
        "import gensim\n",
        "import sklearn\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
        "\n",
        "# word2vec\n",
        "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZgN06q4QPi3"
      },
      "source": [
        "Utilizando el dataset adjunto con la tarea:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone 'https://github.com/cristobalperezp/CC6205-NLP.git'"
      ],
      "metadata": {
        "id": "Axt9vKFBUITT",
        "outputId": "8fdf195b-0a87-4d1d-e866-4bfda8b002c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CC6205-NLP'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 72 (delta 31), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (72/72), 6.68 MiB | 5.11 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY3kmg4onnsu"
      },
      "source": [
        "data_file = \"/content/CC6205-NLP/dialogue-lines-of-the-simpsons.zip\"\n",
        "df = pd.read_csv(data_file)\n",
        "stopwords = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
        ").values\n",
        "stopwords = Counter(stopwords.flatten().tolist())\n",
        "df = df.dropna().reset_index(drop=True) # Quitar filas vacias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAg5a5bmWk3T"
      },
      "source": [
        "**Pregunta 1**: Ayud√°ndose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec y FastText sobre el dataset anterior. **(1 punto)** (Hint, le puede servir explorar un poco los datos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWw2fXFRXe5Y"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvwplz7yTNcr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lr8U5wOTNcr"
      },
      "source": [
        "**Pregunta 2**: Encuentre las palabras mas similares a las siguientes: Lisa, Bart, Homer, Marge. C√∫al es la diferencia entre ambos resultados? Por qu√© ocurre esto? Intente comparar ahora Liisa en ambos modelos (doble i). Cuando escoger√≠a uno vs el otro? **(0.5 puntos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMLyGffVTNcs"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6RvJGpbTNcs"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRCB-jqgTNcs"
      },
      "source": [
        "### **Parte 4 (1 Punto): Aplicar embeddings para clasificar**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlqzlJRSTNcs"
      },
      "source": [
        "Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n",
        "\n",
        "Para esto ocuparemos el lexic√≥n AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotaci√≥n es positiva y un -1 si es negativa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMskFDmHTNcs"
      },
      "source": [
        "AFINN = 'AFINN_full.csv'\n",
        "df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaKl8hsCTNcs"
      },
      "source": [
        "Hint: Para w2v son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendr√°n una representaci√≥n en AFINN. Pueden utilizar esta funci√≥n auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWSSuctiTNcs"
      },
      "source": [
        "def try_apply(model,word):\n",
        "    try:\n",
        "        aux = model[word]\n",
        "        return True\n",
        "    except KeyError:\n",
        "        #logger.error('Word {} not in dictionary'.format(word))\n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrVPeEzgTNcs"
      },
      "source": [
        "**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representaci√≥n en embedding que acabamos de calcular (con ambos modelos). \n",
        "\n",
        "Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n",
        "\n",
        "Para ambos modelos, separar train y test de acuerdo a la siguiente funci√≥n. **(0.75 puntos)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bkt26BwTNcs"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDcq5czXTNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upAn_eT4TNct"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDKe4gA3TNct"
      },
      "source": [
        "**Pregunta 2**: Entrenar una regresi√≥n log√≠stica (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qu√© se obtienen estos resultados? C√≥mo los mejorar√≠as? **(0.75 puntos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJMzq_dETNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izppruGQTNct"
      },
      "source": [
        "# Bonus: +0.25 puntos en cualquier pregunta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW0aeK2KTNct"
      },
      "source": [
        "**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset m√°s grande y obtener mejores resultados. Les puede servir [√©sta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvHcVS3sTNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSc8p-T8TNcu"
      },
      "source": [
        "AFINN = '/content/CC6205-NLP/AFINN_full.csv'\n",
        "df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_afinn.head(10)"
      ],
      "metadata": {
        "id": "64530ZVoUmgV",
        "outputId": "7d78221a-ae67-4c7d-9791-4783425e63db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0  1\n",
              "0          tops  1\n",
              "1         groan -1\n",
              "2      perfects  1\n",
              "3       spammer -1\n",
              "4      saluting  1\n",
              "5  transgresses -1\n",
              "6      punishes -1\n",
              "7       wasting -1\n",
              "8      virulent -1\n",
              "9   complaining -1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-abaa4b07-4d01-4339-ace5-87d6b400c1da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tops</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>groan</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>perfects</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>spammer</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>saluting</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>transgresses</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>punishes</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>wasting</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>virulent</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>complaining</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-abaa4b07-4d01-4339-ace5-87d6b400c1da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-abaa4b07-4d01-4339-ace5-87d6b400c1da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-abaa4b07-4d01-4339-ace5-87d6b400c1da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "modelo = api.load(\"glove-twitter-25\") "
      ],
      "metadata": {
        "id": "SZ_9Ne47WAow",
        "outputId": "8377bc32-d04d-4ca5-b27c-66faea999050",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def try_apply(model,word):\n",
        "    try:\n",
        "        aux = model[word]\n",
        "        return True\n",
        "    except KeyError:\n",
        "        #logger.error('Word {} not in dictionary'.format(word))\n",
        "        return False"
      ],
      "metadata": {
        "id": "hpUdDRvAWDIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def obtener_embedding(palabra):\n",
        "    if try_apply(modelo, palabra):\n",
        "        return modelo[palabra]\n",
        "    else:\n",
        "        return np.zeros(modelo.vector_size)"
      ],
      "metadata": {
        "id": "lDckJneHYc-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_afinn['embedding'] = df_afinn.loc[:,0].apply(obtener_embedding)"
      ],
      "metadata": {
        "id": "1ZuMzpigZZtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_afinn"
      ],
      "metadata": {
        "id": "KKsaBeLqaeYm",
        "outputId": "e952c3b0-5f2a-4d78-a9fb-44aec9f8d29f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                0  1                                          embedding\n",
              "0            tops  1  [0.49105, -0.87081, 0.49447, -1.1929, 0.29789,...\n",
              "1           groan -1  [-0.76202, 0.175, -0.28268, 1.2073, -0.25543, ...\n",
              "2        perfects  1  [-0.036265, -0.22563, 0.54104, 0.00027051, -0....\n",
              "3         spammer -1  [0.03925, 0.93774, -0.71203, 0.23106, 0.20442,...\n",
              "4        saluting  1  [-1.6916, 0.9639, -0.05709, 0.73704, -0.29311,...\n",
              "...           ... ..                                                ...\n",
              "3377   mediocrity -1  [-0.62844, 0.34362, -2.1548, 1.0754, -0.64261,...\n",
              "3378         bold  1  [0.12088, 0.13523, -0.37796, -0.16371, 0.84471...\n",
              "3379       hating -1  [-0.011326, 1.4475, 0.24511, 0.13498, -0.18077...\n",
              "3380  unfavorable -1  [0.53319, 0.19846, -0.7779, 0.13871, 0.42557, ...\n",
              "3381   scapegoats -1  [0.1627, 0.2338, -0.70562, 0.07637, 0.39722, -...\n",
              "\n",
              "[3382 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5f238294-cbd9-4c89-8f06-0f250a46e642\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tops</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.49105, -0.87081, 0.49447, -1.1929, 0.29789,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>groan</td>\n",
              "      <td>-1</td>\n",
              "      <td>[-0.76202, 0.175, -0.28268, 1.2073, -0.25543, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>perfects</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.036265, -0.22563, 0.54104, 0.00027051, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>spammer</td>\n",
              "      <td>-1</td>\n",
              "      <td>[0.03925, 0.93774, -0.71203, 0.23106, 0.20442,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>saluting</td>\n",
              "      <td>1</td>\n",
              "      <td>[-1.6916, 0.9639, -0.05709, 0.73704, -0.29311,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3377</th>\n",
              "      <td>mediocrity</td>\n",
              "      <td>-1</td>\n",
              "      <td>[-0.62844, 0.34362, -2.1548, 1.0754, -0.64261,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3378</th>\n",
              "      <td>bold</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.12088, 0.13523, -0.37796, -0.16371, 0.84471...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3379</th>\n",
              "      <td>hating</td>\n",
              "      <td>-1</td>\n",
              "      <td>[-0.011326, 1.4475, 0.24511, 0.13498, -0.18077...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3380</th>\n",
              "      <td>unfavorable</td>\n",
              "      <td>-1</td>\n",
              "      <td>[0.53319, 0.19846, -0.7779, 0.13871, 0.42557, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3381</th>\n",
              "      <td>scapegoats</td>\n",
              "      <td>-1</td>\n",
              "      <td>[0.1627, 0.2338, -0.70562, 0.07637, 0.39722, -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3382 rows √ó 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f238294-cbd9-4c89-8f06-0f250a46e642')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5f238294-cbd9-4c89-8f06-0f250a46e642 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5f238294-cbd9-4c89-8f06-0f250a46e642');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_afinn.copy()"
      ],
      "metadata": {
        "id": "EN4Q1VqYZeHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.stack(df['embedding'])\n",
        "y = df[1]"
      ],
      "metadata": {
        "id": "pJF3FDcbZh32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)"
      ],
      "metadata": {
        "id": "tun0BfxWZlId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "qH6xs8NxZvPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logit = LogisticRegression(max_iter=1000000).fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "JiWVoYdqZo0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_train, logit.predict(X_train)))"
      ],
      "metadata": {
        "id": "S2N8lMxNbiBq",
        "outputId": "84bfcbbb-8057-4c62-bf92-efd6504cc498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.85      0.93      0.89      1985\n",
            "           1       0.84      0.70      0.77      1058\n",
            "\n",
            "    accuracy                           0.85      3043\n",
            "   macro avg       0.85      0.82      0.83      3043\n",
            "weighted avg       0.85      0.85      0.85      3043\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, logit.predict(X_test)))"
      ],
      "metadata": {
        "id": "Zcl67q8lbuQc",
        "outputId": "d70514db-549b-4c70-bb88-f334bfd991c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.87      0.95      0.90       221\n",
            "           1       0.88      0.73      0.80       118\n",
            "\n",
            "    accuracy                           0.87       339\n",
            "   macro avg       0.87      0.84      0.85       339\n",
            "weighted avg       0.87      0.87      0.87       339\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(y_train, logit.predict(X_train))\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "jU-FfnDobYtl",
        "outputId": "a7dfb4e1-22d6-438a-a7da-a8c65d45788f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1846  139]\n",
            " [ 316  742]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(y_test, logit.predict(X_test))\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "LUc45uOEbaB8",
        "outputId": "15bf8b13-b213-4559-a5d8-c7e367a7731b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[209  12]\n",
            " [ 32  86]]\n"
          ]
        }
      ]
    }
  ]
}