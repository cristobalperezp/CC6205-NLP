{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cristobalperezp/CC6205-NLP/blob/main/Competencia2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0tyIsliieNr"
      },
      "source": [
        "# **Competencia 2 - CC6205 Natural Language Processing 游닄**\n",
        "\n",
        "## Enunciado actualizado gracias a Ignacio Meza\n",
        "\n",
        "Integrantes:\n",
        "\n",
        "Usuario del equipo en CodaLab (Obligatorio):\n",
        "\n",
        "Fecha l칤mite de entrega 游늱: 7 de Julio.\n",
        "\n",
        "Tiempo estimado de dedicaci칩n:\n",
        "\n",
        "Link competencia: Poner el link [aqu칤](https://codalab.lisn.upsaclay.fr/competitions/13646?secret_key=c2dbdef5-9869-4b0a-845a-2dc529b026fb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MocJN22HSJ1x"
      },
      "source": [
        "### **Objetivo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwdgXS8FSLvc"
      },
      "source": [
        "El objetivo de esta competencia es resolver una de las tareas m치s importantes en el 치rea del procesamiento de lenguage natural, relacionada con la extracci칩n de informaci칩n: [Named Entity Recognition (NER)](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf).\n",
        "\n",
        "En particular, y al igual que en la competencia anterior, deber치n crear distintos modelos que apunten a resolver la tarea de NER en Espa침ol. Para esto, les entregaremos un dataset real perteneciente a la lista de espera NO GES en Chile. Es importante destacar que existe una falta de trabajos realizados en el 치rea de NER en Espa침ol y a칰n m치s en el contexto cl칤nico, por ende puede ser considerado como una tarea bien desafiante y quiz치s les interesa trabajar en el 치rea m치s adelante en sus carreras.\n",
        "\n",
        "En este notebook les entregaremos un baseline como referencia de los resultados que esperamos puedan obtener. Recuerden que el no superar a los baselines en alguna de las tres m칠tricas conlleva un descuento de 0.5 puntos hasta 1.5 puntos.\n",
        "\n",
        "Como hemos estado viendo redes neuronales tanto en catedras, tareas y auxiliares (o pr칩ximamente lo har치n), esperamos que (por lo menos) utilicen Redes Neuronales Recurrentes (RNN) para resolverla.\n",
        "\n",
        "Nuevamente, hay total libertad para utilizar el software y los modelos que deseen, siempre y cuando estos no traigan los modelos ya implementados. (De todas maneras como es un corpus nuevo, es dif칤cil que haya alg칰n modelo ya implementado con estas entidades)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnjgmvjBSReb"
      },
      "source": [
        "### **Explicaci칩n de la competencia**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH4HqnCjSWs-"
      },
      "source": [
        "La tarea **NER** que van a resolver en esta competencia es com칰nmente abordada como un problema de Sequence Labeling.\n",
        "\n",
        "**쯈u칠 es Sequence Labeling?**\n",
        "\n",
        "En breves palabras, dada una secuencia de tokens (oraci칩n) sequence labeling tiene por objetivo asignar una etiqueta a cada token de dicha secuencia. En pocas palabras, dada una lista de tokens esperamos encontrar la mejor secuencia de etiquetas asociadas a esa lista. Ahora veamos de qu칠 se trata este problema.\n",
        "\n",
        "**Named Entity Recognition (NER)**\n",
        "\n",
        "NER es un ejemplo de un problema de Sequence Labeling. Pero antes de definir formalmente esta tarea, es necesario definir algunos conceptos claves para poder entenderla de la mejor manera:\n",
        "\n",
        "- *Token*: Un token es una secuencia de caracteres, puede ser una palabra, un n칰mero o un s칤mbolo.\n",
        "\n",
        "- *Entidad*: No es m치s que un trozo de texto (uno o m치s tokens) asociado a una categor칤a predefinida. Originalmente se sol칤an utilizar categor칤as como nombres de personas, organizaciones, ubicaciones, pero actualmente se ha extendido a diferentes dominios.\n",
        "\n",
        "- *L칤mites de una entidad*: Son los 칤ndices de los tokens de inicio y f칤n dentro de una entidad.\n",
        "\n",
        "- *Tipo de entidad*: Es la categor칤a predefinida asociada a la entidad.\n",
        "\n",
        "Dicho esto, definimos formalmente una entidad como una tupla: $(s, e, t)$, donde $s, t$ son los l칤mites de la entidad (칤ndices de los tokens de inicio y fin, respectivamente) y t corresponde al tipo de entidad o categor칤a. Ya veremos m치s ejemplos luego de describir el Dataset.\n",
        "\n",
        "**Corpus de la Lista de espera**\n",
        "\n",
        "Trabajaran con un conjunto de datos reales correspondiente a interconsultas de la lista de espera NO GES en Chile. Si quieren saber m치s sobre c칩mo fueron generados los datos pueden revisar el paper publicado hace unos meses atr치s en el workshop de EMNLP, una de las conferencias m치s importantes de NLP: [https://www.aclweb.org/anthology/2020.clinicalnlp-1.32/](https://www.aclweb.org/anthology/2020.clinicalnlp-1.32/).\n",
        "\n",
        "Este corpus Chileno est치 constituido originalmente por 7 tipos de entidades pero por simplicidad en esta competencia trabajar치n con las siguientes:\n",
        "\n",
        "- **Disease**\n",
        "- **Body_Part**\n",
        "- **Medication**\n",
        "- **Procedures**\n",
        "- **Family_Member**\n",
        "\n",
        "Si quieren obtener m치s informaci칩n sobre estas entidades pueden consultar la [gu칤a de anotaci칩n](https://plncmm.github.io/annodoc/). Adem치s, mencionar que este corpus est치 restringido bajo una licencia que permite solamente su uso acad칠mico, as칤 que no puede ser compartido m치s all치 de este curso o sin permisos por parte de los autores en caso que quieran utilizarlo fuera. Si este 칰ltimo es el caso entonces pueden escribir directamente al correo: pln@cmm.uchile.cl. Al aceptar los t칠rminos y condiciones de la competencia est치n de acuerdo con los puntos descritos anteriormente.\n",
        "\n",
        "\n",
        "**Formato ConLL**\n",
        "\n",
        "Los archivos que ser치n entregados a ustedes vienen en un formato est치ndar utilizado en NER, llamado ConLL. No es m치s que un archivo de texto, que cumple las siguientes propiedades.\n",
        "\n",
        "- Un salto de linea corresponde a la separaci칩n entre oraciones. Esto es importante ya que al entrenar una red neuronal ustedes pasaran una lista de oraciones como input, m치s conocidos como batches.\n",
        "\n",
        "- La primera columna del archivo contiene todos los tokens de la partici칩n.\n",
        "\n",
        "- La segunda columna del archivo contiene el tipo de entidad asociado al token de la primera columna.\n",
        "\n",
        "- Los tipos de entidades siguen un formato cl치sico en NER denominado *IOB2*. Si un tipo de entidad comienza con el prefijo \"B-\" (Beginning) significa que es el token de inicio de una entidad, si comienza con \"I-\" (Inside) es un token distinto al de inicio y si un token est치 asociado a la categor칤a O (Outside) significa que no pertenece a ninguna entidad.\n",
        "\n",
        "Aqu칤 va un ejemplo:\n",
        "\n",
        "```\n",
        "PACIENTE O\n",
        "PRESENTA O\n",
        "FRACTURA B-Disease\n",
        "CORONARIA I-Disease\n",
        "COMPLICADA I-Disease\n",
        "EN O\n",
        "PIE B-Body_Part\n",
        "IZQUIERDO I-Body_Part\n",
        ". O\n",
        "SE O\n",
        "REALIZA O\n",
        "INSTRUMENTACION B-Procedure\n",
        "INTRACONDUCTO I-Procedure\n",
        ". O\n",
        "```\n",
        "\n",
        "Seg칰n nuestra definici칩n tenemos las siguientes tres entidades (enumerando desde 0):\n",
        "\n",
        "- $(2, 4, Disease)$\n",
        "- $(6, 7, Body Part)$\n",
        "- $(11, 12, Procedure)$\n",
        "\n",
        "Repasen un par de veces todos estos conceptos antes de pasar a la siguiente secci칩n del notebook.\n",
        "Es importante entender bien este formato ya que al medir el rendimiento de sus modelos, consideraremos una **m칠trica estricta**. Esta m칠trica se llama as칤 ya que considera correcta una predicci칩n de su modelo, s칩lo si al compararlo con las entidades reales **coinciden tanto los l칤mites de la entidad como el tipo.**\n",
        "\n",
        "Para ejemplificar, tomando el caso anterior, si el modelo es capaz de encontrar la siguiente entidad: $(2, 3, Disease)$, entonces se considera incorrecto ya que pudo predecir dos de los tres tokens de dicha enfermedad. Es decir, buscamos una m칠trica que sea alta a nivel de entidad y no a nivel de token.\n",
        "\n",
        "Antes de pasar a explicar las reglas, se recomienda visitar los siguientes links para entender bien el baseline de la competencia:\n",
        "\n",
        "-  [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)\n",
        "-  [Recurrent Neural Networks](slides/NLP-RNN.pdf) | [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)\n",
        "\n",
        "\n",
        "Recuerden que todo el material se encuentra disponible en el [github del curso](https://github.com/dccuchile/CC6205)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWlfabmkaSE7"
      },
      "source": [
        "### **Reglas de la competencia**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w9Dw4CSaSE8"
      },
      "source": [
        "**texto en negrita**- Para que su competencia sea evaluada, deben participar en la competencia y enviar este notebook con su informe.\n",
        "- Para participar, deben registrarse en la competencia en Codalab en grupos de m치ximo 4 alumnos. Cada grupo debe tener un nombre de equipo. (춰Y deben reportarlo en su informe, por favor!)\n",
        "- Las m칠tricas usadas ser치n m칠tricas estrictas (ya explicado anteriormente) utilizando m칠tricas cl치sicas como lo son precisi칩n, recall y micro f1-score.\n",
        "- En esta tarea se recomienda usar GPU. Pueden ejecutar su tarea en colab (lo cual trae todo instalado) o pueden intentar ejecut치ndolo en su computador. En este caso, deber치 ser compatible con cuda y deber치n instalar todo por su cuenta.\n",
        "- En total pueden hacer un **m치ximo de 5 env칤os**.\n",
        "- Por favor, todas sus dudas haganlas por el canal de Discord. Los emails que lleguen al equipo docente ser치n remitidos a ese medio. Recuerden el 치nimo colaborativo del curso.\n",
        "- Estar top 5 en alguna de las tres m칠tricas equivale a una bonificaci칩n en su nota final.\n",
        "\n",
        "칄xito!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZyHBjU-R-wi"
      },
      "source": [
        "### **Baseline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WZ8G01aSBYX"
      },
      "source": [
        "# En este punto esperamos que tengan conocimiento sobre redes neuronales y en particular redes neuronales recurrentes (RNN), si no siempre pueden escribirnos por el canal de Discord para aclarar dudas. La RNN del baseline adjunto a este notebook est치 programado en la librer칤a [`pytorch`](https://pytorch.org/) pero ustedes pueden utilizar keras, tensorflow si as칤 lo desean. El c칩digo contiene lo siguiente:\n",
        "\n",
        "- La carga de los datasets, creaci칩n de batches de texto y padding (esto es importante ya que si utilizan redes neuronales tienen que tener el mismo largo los inputs).\n",
        "\n",
        "- La implementaci칩n b치sica de una red `LSTM` simple de solo un nivel y sin bi-direccionalidad.\n",
        "\n",
        "- La construcci칩n del formato del output requerido para que lo puedan probar en la tarea en codalab.\n",
        "\n",
        "Se espera que como m칤nimo ustedes puedan experimentar con el baseline utilizando (pero no limit치ndose) estas sugerencias:\n",
        "\n",
        "*   Probar la t칠cnica de early stopping.\n",
        "*   Variar la cantidad de par치metros de la capa de embeddings.\n",
        "*   Variar la cantidad de capas RNN.\n",
        "*   Variar la cantidad de par치metros de las capas de RNN.\n",
        "*   Inicializar la capa de embeddings con modelos pre-entrenados. (word2vec, glove, conceptnet, etc...). [Embeddings en espa침ol aqu칤](https://github.com/dccuchile/spanish-word-embeddings). Tambi칠n aqu칤 pueden encontrar unos embeddings cl칤nicos en Espa침ol: [https://zenodo.org/record/3924799](https://zenodo.org/record/3924799)\n",
        "*   Variar la cantidad de 칠pocas de entrenamiento.\n",
        "*   Variar el optimizador, learning rate, batch size, usar CRF loss, etc.\n",
        "*   Probar una capa de CRF para garantizar el     formato IOB2.\n",
        "*   Probar bi-direccionalidad.\n",
        "*   Incluir dropout.\n",
        "*   Probar modelos de tipo GRU.\n",
        "*   Probar usando capas de atenci칩n.\n",
        "*   Probar Embedding Contextuales (les puede ser de utilidad [flair](https://github.com/flairNLP/flair))\n",
        "*   Probar modelos de transformers en espa침ol usando [Huggingface](https://github.com/huggingface/transformers) o el framework Flair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rr2mzxPTzNd"
      },
      "source": [
        "### **Reporte**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEf33mnxT0rf"
      },
      "source": [
        "Este debe cumplir la siguiente estructura:\n",
        "\n",
        "1.\t**Introducci칩n**: Presentar brevemente el contexto, problema a resolver, incluyendo la formalizaci칩n de la task (c칩mo son los inputs y outputs del problema) y los desaf칤os que ven al analizar el corpus entregado. (**0.5 puntos**)\n",
        "\n",
        "2.\t**Modelos**: Describir brevemente los modelos, m칠todos e hiperpar치metros utilizados. (**1.0 puntos**)\n",
        "\n",
        "4.\t**M칠tricas de evaluaci칩n**: Describir las m칠tricas utilizadas en la evaluaci칩n indicando qu칠 miden y cu치l es su interpretaci칩n en este problema en particular. (**0.5 puntos**)\n",
        "\n",
        "5.  **Dise침o experimental**: Esta es una de las secciones m치s importantes del reporte. Deben describir minuciosamente los experimentos que realizar치n en la siguiente secci칩n. Describir las variables de control que manejar치n, algunos ejemplos pueden ser: Los hiperpar치metros de los modelos, tipo de embeddings utilizados, tipos de arquitecturas. Ser claros con el conjunto de hiperpar치metros que probar치n, la decisi칩n en las funciones de optimizaci칩n, funci칩n de p칠rdida,  regulaci칩n, etc. B치sicamente explicar qu칠 es lo que veremos en la siguiente secci칩n.\n",
        "(**1 punto**)\n",
        "\n",
        "6.\t**Experimentos**: Reportar todos sus experimentos y c칩digo en esta secci칩n. Comparar los resultados obtenidos utilizando diferentes modelos. 춰Es vital haber realizado varios experimentos para sacar una buena nota! (**2.0 puntos**)\n",
        "\n",
        "7.\t**Conclusiones**: Discutir resultados, proponer trabajo futuro. (**1 punto**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaoU1EXfUDbl"
      },
      "source": [
        "# **Entregable.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzQlYlmGaSFH"
      },
      "source": [
        "## **Introducci칩n**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVL0-01AUOzL"
      },
      "source": [
        "En el marco de la Competencia 1 del curso de Procesamiento de Lenguaje Natural, se abord칩 un desaf칤o crucial en el campo de la extracci칩n de informaci칩n: el reconocimiento de entidades nombradas (NER, por sus siglas en ingl칠s) en espa침ol. En este informe, se presenta el trabajo realizado para resolver esta tarea.\n",
        "\n",
        "El NER es una tarea fundamental en el procesamiento de lenguaje natural, cuyo objetivo es identificar y clasificar entidades con nombres propios en un texto. En el caso espec칤fico de esta competencia, se entrega un conjunto de datos reales relacionados con la lista de espera NO GES en Chile. Es importante destacar que existe una escasez de trabajos previos en NER en espa침ol, y a칰n m치s en el contexto cl칤nico, lo que convierte a esta tarea en un desaf칤o particularmente desafiante.\n",
        "\n",
        "Los inputs de los modelos a desarrollar consisten en textos en espa침ol, extra칤dos del corpus proporcionado, en los cuales se deben identificar y clasificar las entidades nombradas relevantes,  para esta competencia se presenta en formato ConLL.\n",
        "\n",
        "Los outputs esperados son las etiquetas asignadas a cada entidad nombrada, siguiendo un formato cl치sico en NER denominado IOB2, con las siguientes entidades esperadas: Disease, Body_Part, Medication, Procedures, Family_Member. En resumen, se busca desarrollar modelos capaces de procesar autom치ticamente los textos de entrada y asignar las etiquetas adecuadas a las entidades identificadas.\n",
        "\n",
        "Los experimentos para diferentes modelos se basar치n en redes neuronales, desarrollando las redes tipo LSTM, uni y bidireccional, adem치s de modelos tipo GRU y transformers, para luego obtener sus m칠tricas y compararlos. Por otro lado, a partir de los mejores modelos se obtendr치n los scores obtenidos para un conjunto de prueba, los cuales ser치n subidos a Codalab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbA1EmhCaSFI"
      },
      "source": [
        "## **Modelos**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HsvlfPJUSId"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaVhZ5iaaSFK"
      },
      "source": [
        "## **M칠tricas de evaluaci칩n**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXl3GaVMUYA7"
      },
      "source": [
        "Como m칠tricas de desempe침o de los modelos, y adem치s de evaluaci칩n para generar el ranking en la competencia se utiliz칩:\n",
        "\n",
        "* Precision: se refiere a la proporci칩n de entidades nombradas correctamente identificadas entre todas las entidades etiquetadas como tales por el modelo. Es decir, mide la exactitud de las predicciones positivas. Una alta precisi칩n indica que el modelo tiene una baja tasa de falsos positivos, es decir, de identificar incorrectamente entidades.\n",
        "\n",
        "* Recall: se refiere a la proporci칩n de entidades nombradas correctamente identificadas en relaci칩n con todas las entidades nombradas presentes en los datos de prueba. Mide la capacidad del modelo para encontrar todas las entidades relevantes y evitar falsos negativos. Un alto recall indica que el modelo puede detectar correctamente la mayor칤a de las entidades nombradas.\n",
        "\n",
        "* micro F1-score: combina la precisi칩n y el recall en una sola m칠trica que proporciona una medida equilibrada del rendimiento general del modelo. El F1-score se calcula como la media arm칩nica de la precisi칩n y el recall, y el enfoque micro considera la suma total de las predicciones y verdaderos positivos para calcular el resultado general. Un alto micro F1-score indica que el modelo tiene un equilibrio entre precisi칩n y recall, logrando una buena capacidad para identificar correctamente las entidades nombradas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u27WffRVUj4v"
      },
      "source": [
        "## **Dise침o experimental**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O228DbeUmE7"
      },
      "source": [
        "    Descripci칩n de la metodolog칤a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFM-wNt8aSFM"
      },
      "source": [
        "## **Experimentos**\n",
        "\n",
        "\n",
        "El c칩digo que les entregaremos servir치 de baseline para luego implementar mejores modelos.\n",
        "En general, el c칩digo asociado a la carga de los datos, las funciones de entrenamiento, de evaluaci칩n y la predicci칩n de los datos de la competencia no deber칤an cambiar.\n",
        "Solo deben preocuparse de cambiar la arquitectura del modelo, sus hiperpar치metros y reportar, lo cual lo pueden hacer en las subsecciones *modelos*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0YbB0Y6cvMz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMgKjfYC_Go-"
      },
      "source": [
        "###  **Carga de datos y Preprocesamiento**\n",
        "\n",
        "Para cargar los datos y preprocesarlos usaremos la librer칤a [`torchtext`](https://github.com/pytorch/text). Tener cuidado ya que hace algunos meses esta librer칤a tuvo cambios radicales, quedando las funcionalidades pasadas en un nuevo paquete llamado legacy. Esto ya que si quieren usar m치s funciones de la librer칤a entonces vean los cambios en la documentaci칩n.\n",
        "\n",
        "En particular usaremos su m칩dulo `data`, el cual seg칰n su documentaci칩n original provee:\n",
        "\n",
        "    - Ability to describe declaratively how to load a custom NLP dataset that's in a \"normal\" format\n",
        "    - Ability to define a preprocessing pipeline\n",
        "    - Batching, padding, and numericalizing (including building a vocabulary object)\n",
        "    - Wrapper for dataset splits (train, validation, test)\n",
        "\n",
        "\n",
        "El proceso ser치 el siguiente:\n",
        "\n",
        "1. Descargar los datos desde github y examinarlos.\n",
        "2. Definir los campos (`fields`) que cargaremos desde los archivos.\n",
        "3. Cargar los datasets.\n",
        "4. Crear el vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NwZSFl0dV4Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27csY87GaSFO",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "!pip install -U torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng7wRGEyawjM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext import data, datasets\n",
        "\n",
        "# Garantizar reproducibilidad de los experimentos\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BehSou6rCvwg"
      },
      "source": [
        "#### **Obtener datos**\n",
        "\n",
        "Descargamos los datos de entrenamiento, validaci칩n y prueba en nuestro directorio de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbT0g_kC18Jb"
      },
      "outputs": [],
      "source": [
        "#%%capture\n",
        "\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt -nc # Dataset de Entrenamiento\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt -nc    # Dataset de Validaci칩n (Para probar y ajustar el modelo)\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/test.txt -nc  # Dataset de la Competencia. Estos datos solo contienen los tokens. 춰춰SON LOS QUE DEBEN SER PREDICHOS!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "id": "DMHUyK-_vmX5"
      },
      "outputs": [],
      "source": [
        "# NUEVO DATALOADER Y OTRAS COSAS NECESARIAS\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "class TaggingDataset(Dataset):\n",
        "    def __init__(self, path, lower=True,#lower=False,\n",
        "                 separator=\" \", encoding=\"utf-8\"):\n",
        "\n",
        "        with open(path, 'r', encoding=encoding) as file:\n",
        "          text, tag, data = [], [], []\n",
        "          for line in file:\n",
        "              line = line.strip()\n",
        "              if line == \"\":\n",
        "                  data.append(dict({'text':text, 'nertags':tag}))\n",
        "                  text, tag = [], []\n",
        "              else:\n",
        "                  line_content = line.split(separator) # .rstrip('\\n')\n",
        "                  if lower:\n",
        "                    text.append(line_content[0].lower())\n",
        "                  else:\n",
        "                    text.append(line_content[0])\n",
        "                  tag.append(line_content[1])\n",
        "        data.append(dict({'text':text, 'nertags':tag}))\n",
        "\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "        text = item[\"text\"]\n",
        "        nertags = item[\"nertags\"]\n",
        "        return nertags, text\n",
        "\n",
        "def fit_vocab(data_iter):\n",
        "\n",
        "  def update_counter(counter_obj):\n",
        "    sorted_by_freq_tuples = sorted(counter_obj.items(),\n",
        "                                  key=lambda x: x[1],\n",
        "                                  reverse=True)\n",
        "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "    return ordered_dict\n",
        "\n",
        "  counter_1 = Counter()\n",
        "  counter_2 = Counter()\n",
        "  for _nertags, _text in data_iter:\n",
        "    counter_1.update(_text)\n",
        "    counter_2.update(_nertags)\n",
        "\n",
        "  od1 = update_counter(counter_1)\n",
        "  od2 = update_counter(counter_2)\n",
        "\n",
        "  v1 = vocab(od1, specials=['<PAD>', '<unk>'])\n",
        "  v1.set_default_index(v1[\"<unk>\"])\n",
        "  v2 = vocab(od2, specials=['<PAD>'])\n",
        "\n",
        "  text_pipeline = lambda x: v1(x)\n",
        "  nertags_pipeline = lambda x: v2(x)\n",
        "\n",
        "  return text_pipeline, nertags_pipeline, v1, v2\n",
        "\n",
        "def collate_batch(batch):\n",
        "  nertags_list, text_list = [], []\n",
        "  for _nertags, _text in batch:\n",
        "    processed_nertags = torch.tensor(nertags_pipeline(_nertags),\n",
        "                                     dtype=torch.int64)\n",
        "    nertags_list.append(processed_nertags)\n",
        "    processed_text = torch.tensor(text_pipeline(_text),\n",
        "                                  dtype=torch.int64)\n",
        "    text_list.append(processed_text)\n",
        "  nertags_list = pad_sequence(nertags_list, batch_first=True).T\n",
        "  text_list = pad_sequence(text_list, batch_first=True).T\n",
        "  return nertags_list.to(device), text_list.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "id": "UXcpc7iRE6ch"
      },
      "outputs": [],
      "source": [
        "train_iter = TaggingDataset(\"train.txt\")\n",
        "dev_iter = TaggingDataset(\"dev.txt\")\n",
        "test_iter = TaggingDataset(\"test.txt\")\n",
        "\n",
        "text_pipeline, nertags_pipeline, TEXT, NER_TAGS = fit_vocab(train_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {
        "id": "sLFc1TeO_GLM"
      },
      "outputs": [],
      "source": [
        "# seteamos algunos valores de interes\n",
        "UNK_IDX = TEXT.vocab.get_stoi()['<unk>']\n",
        "PAD_IDX = TEXT.vocab.get_stoi()['<PAD>']\n",
        "\n",
        "PAD_TAG_IDX = NER_TAGS.get_stoi()['<PAD>']\n",
        "O_TAG_IDX = NER_TAGS.vocab.get_stoi()['O']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 398,
      "metadata": {
        "id": "mt1GQv8Vvb-J",
        "outputId": "db87ea12-079d-443c-b5ba-7fd0d1c28ec2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ],
      "source": [
        "#BATCH_SIZE = 22\n",
        "# Prueba 1\n",
        "#BATCH_SIZE = 32*(2**0)\n",
        "BATCH_SIZE = 32*(2**1)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using', device)\n",
        "\n",
        "dataloader_train = DataLoader(\n",
        "    train_iter, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
        ")\n",
        "dataloader_dev = DataLoader(\n",
        "    dev_iter, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
        ")\n",
        "dataloader_test = DataLoader(\n",
        "    test_iter, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8CdKZcY79lY"
      },
      "outputs": [],
      "source": [
        "example = next(iter(dataloader_train))\n",
        "text_example = example[1]\n",
        "\n",
        "# revisamos el primer ejemplo\n",
        "[TEXT.vocab.get_itos()[j] for j in text_example[:, 0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o63ov69_rX2T"
      },
      "outputs": [],
      "source": [
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 399,
      "metadata": {
        "id": "9mUOOLEWiicU"
      },
      "outputs": [],
      "source": [
        "# Definimos las m칠tricas\n",
        "\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
        "    \"\"\"\n",
        "    Calcula precision, recall y f1 de cada batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
        "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
        "\n",
        "    # filtramos <pad> para calcular los scores.\n",
        "    mask = [(y_true != pad_idx)]\n",
        "    y_pred = y_pred[mask]\n",
        "    y_true = y_true[mask]\n",
        "\n",
        "    # traemos a la cpu\n",
        "    y_pred = y_pred.view(-1).to('cpu').numpy()\n",
        "    y_true = y_true.to('cpu').numpy()\n",
        "    y_pred = [[NER_TAGS.vocab.get_itos()[v] for v in y_pred]]\n",
        "    y_true = [[NER_TAGS.vocab.get_itos()[v] for v in y_true]]\n",
        "\n",
        "    # calcular scores\n",
        "    f1 = f1_score(y_true, y_pred, mode='strict')\n",
        "    precision = precision_score(y_true, y_pred, mode='strict')\n",
        "    recall = recall_score(y_true, y_pred, mode='strict')\n",
        "\n",
        "    return precision, recall, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hod516H1aSG2"
      },
      "source": [
        "-------------------\n",
        "\n",
        "### **Modelo Baseline**\n",
        "\n",
        "Teniendo ya cargado los datos, toca definir nuestro modelo. Este baseline tendr치 una capa de embedding, unas cuantas LSTM y una capa de salida y usar치 dropout en el entrenamiento.\n",
        "\n",
        "Este constar치 de los siguientes pasos:\n",
        "\n",
        "1. Definir la clase que contendr치 la red.\n",
        "2. Definir los hiperpar치metros e inicializar la red.\n",
        "3. Definir el n칰mero de 칠pocas de entrenamiento\n",
        "4. Definir la funci칩n de loss.\n",
        "\n",
        "\n",
        "\n",
        "Recomendamos que para experimentar, encapsules los modelos en una sola variable y luego la fijes en model para entrenarla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMPL08XqaSG3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 embedding_dim,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout,\n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx,\n",
        "                                      )\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-crf\n",
        "from torchcrf import CRF\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "sEx631VWxFhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clase modificada para uso de Embeddings preentrenados\n",
        "class NER_RNN_EMB(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 embedding_dim,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout,\n",
        "                 pad_idx,\n",
        "                 pretrained_embeddings):\n",
        "        super(NER_RNN_EMB, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(self.embedding_dim,\n",
        "                            hidden_dim,\n",
        "                            num_layers=n_layers,\n",
        "                            bidirectional=bidirectional,\n",
        "                            dropout=dropout if n_layers > 1 else 0)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "loJ0xpyMs6n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NER_RNN_EMB_CRF(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx, pretrained_embeddings):\n",
        "        super(NER_RNN_EMB_CRF, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout if n_layers > 1 else 0)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.crf = CRF(output_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        return predictions\n",
        "\n",
        "    def loss(self, text, tags):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        loss = -self.crf(predictions, tags)\n",
        "        return loss\n",
        "\n",
        "    def decode(self, text):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        tags = self.crf.decode(predictions)\n",
        "        return tags\n"
      ],
      "metadata": {
        "id": "rGZcxCpwB8_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class NER_RNN_AT(nn.Module):"
      ],
      "metadata": {
        "id": "yzc-llA37J4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class NER_RNN_EMB_AT(nn.Module):"
      ],
      "metadata": {
        "id": "_WA-JeM1uosu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCl3530VaSG7"
      },
      "source": [
        "#### **Hiperpar치metros de la red**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHdi3QdOaSG8"
      },
      "outputs": [],
      "source": [
        "# tama침o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300  # dimensi칩n de los embeddings.\n",
        "HIDDEN_DIM = 256  # dimensi칩n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n칰mero de clases\n",
        "\n",
        "N_LAYERS = 3  # n칰mero de capas.\n",
        "DROPOUT = 0.6\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "baseline_model_name = 'baseline'  # nombre que tendr치 el modelo guardado..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 1: BUENO\n",
        "\n",
        "# tama침o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 250  # dimensi칩n de los embeddings.\n",
        "HIDDEN_DIM = 128*(2**0)  # dimensi칩n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n칰mero de clases\n",
        "\n",
        "N_LAYERS = 2  # n칰mero de capas.\n",
        "DROPOUT = 0.4\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model_1 = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model_1_model_name = 'MODEL_1'  # nombre que tendr치 el modelo guardado...model_1"
      ],
      "metadata": {
        "id": "6K3ZrttpgvwB"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 1: ENTRENAR M츼S\n",
        "\n",
        "# tama침o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 250  # dimensi칩n de los embeddings.\n",
        "HIDDEN_DIM = 128*(2**0)  # dimensi칩n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n칰mero de clases\n",
        "\n",
        "N_LAYERS = 4  # n칰mero de capas.\n",
        "DROPOUT = 0.7\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model_1 = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model_1_model_name = 'MODEL_1'  # nombre que tendr치 el modelo guardado...model_1"
      ],
      "metadata": {
        "id": "8c2GFUGGx-X0"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 1:\n",
        "\n",
        "# tama침o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 275   # dimensi칩n de los embeddings.\n",
        "HIDDEN_DIM = 128*(2**0)  # dimensi칩n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n칰mero de clases\n",
        "\n",
        "N_LAYERS = 4  # n칰mero de capas.\n",
        "DROPOUT = 0.7\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model_1 = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model_1_model_name = 'MODEL_1'  # nombre que tendr치 el modelo guardado...model_1"
      ],
      "metadata": {
        "id": "i-dvU9EM5Ab0"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-K8qsg3jW14c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 2:\n",
        "\n",
        "import torchtext.vocab as vocab\n",
        "# Suponiendo que tienes los vectores de palabras en el archivo 'cwlce.vec'\n",
        "EMBEDDING_FILE = '/content/drive/MyDrive/Universidad/11춿 Semestre/Natural Lenguage Processing/cwlce.vec'\n",
        "\n",
        "# Cargar los embeddings preentrenados utilizando la funci칩n load_embeddings\n",
        "embeddings = vocab.Vectors(EMBEDDING_FILE)\n",
        "EMBEDDING_DIM = embeddings.dim  # Dimensi칩n de los embeddings\n",
        "\n",
        "# Tama침o del vocabulario\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "\n",
        "HIDDEN_DIM = 128 # Dimensi칩n de las capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # N칰mero de clases\n",
        "\n",
        "N_LAYERS = 4  # N칰mero de capas\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Crear una instancia del modelo NER_RNN_EMB con los par치metros y los embeddings preentrenados\n",
        "model_2 = NER_RNN_EMB(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                      N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, embeddings.vectors)\n",
        "\n",
        "model_2_model_name = 'MODEL_2'  # nombre que tendr치 el modelo guardado...model_1"
      ],
      "metadata": {
        "id": "wU4Wh1IOtW8J"
      },
      "execution_count": 506,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 3:\n",
        "import torchtext.vocab as vocab\n",
        "# Suponiendo que tienes los vectores de palabras en el archivo 'cwlce.vec'\n",
        "EMBEDDING_FILE = '/content/drive/MyDrive/Universidad/11춿 Semestre/Natural Lenguage Processing/cwlce.vec'\n",
        "\n",
        "# Cargar los embeddings preentrenados utilizando la funci칩n load_embeddings\n",
        "embeddings = vocab.Vectors(EMBEDDING_FILE)\n",
        "EMBEDDING_DIM = embeddings.dim  # Dimensi칩n de los embeddings\n",
        "\n",
        "# Tama침o del vocabu  lario\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "\n",
        "HIDDEN_DIM = 128*(2**0)\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)\n",
        "N_LAYERS = 3\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Crear una instancia del modelo NER_RNN_CRF con los par치metros y los embeddings preentrenados\n",
        "model_3 = NER_RNN_EMB_CRF(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                    N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, embeddings.vectors)\n",
        "\n",
        "model_3_model_name = 'MODEL_3'"
      ],
      "metadata": {
        "id": "CSdwYlDZCEso"
      },
      "execution_count": 440,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 4:\n",
        "import torchtext.vocab as vocab\n",
        "# Suponiendo que tienes los vectores de palabras en el archivo 'cwlce.vec'\n",
        "EMBEDDING_FILE = '/content/drive/MyDrive/Universidad/11춿 Semestre/Natural Lenguage Processing/embeddings-l-model.vec'\n",
        "\n",
        "# Cargar los embeddings preentrenados utilizando la funci칩n load_embeddings\n",
        "embeddings = vocab.Vectors(EMBEDDING_FILE)\n",
        "EMBEDDING_DIM = embeddings.dim  # Dimensi칩n de los embeddings\n",
        "\n",
        "# Tama침o del vocabu  lario\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "\n",
        "HIDDEN_DIM = 128*(2**0)\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)\n",
        "N_LAYERS = 2\n",
        "DROPOUT = 0.4\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Crear una instancia del modelo NER_RNN_CRF con los par치metros y los embeddings preentrenados\n",
        "model_4 = NER_RNN_EMB_CRF(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                    N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, embeddings.vectors)\n",
        "\n",
        "model_4_model_name = 'MODEL_4'"
      ],
      "metadata": {
        "id": "73QRisPCux9x"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 5:\n"
      ],
      "metadata": {
        "id": "ibMDG95A7QCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2oLveacj7aw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlF1DhJeaSHA"
      },
      "outputs": [],
      "source": [
        "baseline_n_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 1: model_1\n",
        "#model_1_n_epochs = 30\n",
        "model_1_n_epochs = 100\n",
        "#model_1_n_epochs = 100"
      ],
      "metadata": {
        "id": "oljYFYmY5sJ_"
      },
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 2: model_2\n",
        "model_2_n_epochs = 90"
      ],
      "metadata": {
        "id": "QgYAV6rN5q6U"
      },
      "execution_count": 495,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 3: model_3\n",
        "model_3_n_epochs = 100"
      ],
      "metadata": {
        "id": "Qs13xFK2C4b4"
      },
      "execution_count": 452,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 4: model_4\n",
        "model_4_n_epochs = 25"
      ],
      "metadata": {
        "id": "vSYRQeRgu5HG"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 5: model_5\n",
        "model_5_n_epochs = 25"
      ],
      "metadata": {
        "id": "56lSRcfe7bxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3u4imJGaSHE"
      },
      "source": [
        "#### Definimos la funci칩n de loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G_4k99_aSHG"
      },
      "outputs": [],
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.get_stoi()['<PAD>']\n",
        "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 1: model_1\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.get_stoi()['<PAD>']\n",
        "model_1_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "metadata": {
        "id": "OOm_f5Jr5wBI"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 2: model_2\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.get_stoi()['<PAD>']\n",
        "model_2_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "metadata": {
        "id": "MlKx-Jhs5xhQ"
      },
      "execution_count": 402,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 3: model_3\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.get_stoi()['<PAD>']\n",
        "# Funci칩n de p칠rdida\n",
        "model_3_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "metadata": {
        "id": "Aml_dBROC7Lp"
      },
      "execution_count": 413,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 4: model_4\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.get_stoi()['<PAD>']\n",
        "# Funci칩n de p칠rdida\n",
        "model_4_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "metadata": {
        "id": "zKTueWunu-gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 5: model_5\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.get_stoi()['<PAD>']\n",
        "# Funci칩n de p칠rdida\n",
        "model_5_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "metadata": {
        "id": "4zk43mma74gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRYOEDiQaSHK"
      },
      "source": [
        "--------------------\n",
        "### Modelo 1\n",
        "\n",
        "En estas secciones pueden implementar nuevas redes al modificar los hiperpar치metros, la cantidad de 칠pocas de entrenamiento, el tama침o de los batches, loss, optimizador, etc... como tambi칠n definir nuevas arquitecturas de red (mediante la creaci칩n de clases nuevas)\n",
        "\n",
        "\n",
        "Al final de estas, hay 4 variables, las cuales deben setear con los modelos, 칠pocas de entrenamiento, loss y optimizador que deseen probar.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_1\n",
        "model_name = model_1_model_name\n",
        "criterion = model_1_criterion\n",
        "n_epochs = model_1_n_epochs"
      ],
      "metadata": {
        "id": "6kmgeNFkOst8"
      },
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV9oLkN1aSHO"
      },
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 534,
      "metadata": {
        "id": "KWPzETaNaSHP"
      },
      "outputs": [],
      "source": [
        "model = model_2\n",
        "model_name = model_2_model_name\n",
        "criterion = model_2_criterion\n",
        "n_epochs = model_2_n_epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpy3p7YaaSHT"
      },
      "source": [
        "---------------\n",
        "\n",
        "\n",
        "### Modelo 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 467,
      "metadata": {
        "id": "_w0CFjA8aSHU"
      },
      "outputs": [],
      "source": [
        "model = model_3\n",
        "model_name = model_3_model_name\n",
        "criterion = model_3_criterion\n",
        "n_epochs = model_3_n_epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8jWnkWD7_bT"
      },
      "source": [
        "---------------\n",
        "\n",
        "\n",
        "### Modelo 4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_4\n",
        "model_name = model_4_model_name\n",
        "criterion = model_4_criterion\n",
        "n_epochs = model_4_n_epochs"
      ],
      "metadata": {
        "id": "OovJciQqvDB-"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1b2I3kE7_vH"
      },
      "source": [
        "---------------\n",
        "\n",
        "\n",
        "### Modelo 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_5\n",
        "model_name = model_5_model_name\n",
        "criterion = model_5_criterion\n",
        "n_epochs = model_5_n_epochs"
      ],
      "metadata": {
        "id": "ferriqvI775E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPGdirx7aSHZ"
      },
      "source": [
        "------\n",
        "### **Entrenamos y evaluamos**\n",
        "\n",
        "\n",
        "**Importante** : Fijen el modelo, el n칰mero de 칠pocas de entrenamiento, la loss y el optimizador que usar치n para entrenar y evaluar en las siguientes variables!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8YlGnjxaSHZ"
      },
      "outputs": [],
      "source": [
        "model = baseline_model\n",
        "model_name = baseline_model_name\n",
        "criterion = baseline_criterion\n",
        "n_epochs = baseline_n_epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu_lXic2aSHd"
      },
      "source": [
        "\n",
        "\n",
        "#### **Inicializamos la red**\n",
        "\n",
        "Iniciamos los pesos de la red de forma aleatoria (Usando una distribuci칩n normal).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 569,
      "metadata": {
        "id": "Q-G_NWFcaSHe",
        "outputId": "fb72c16f-f6c8-469b-9069-e874e870437a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NER_RNN_EMB(\n",
              "  (embedding): Embedding(57112, 300)\n",
              "  (lstm): LSTM(300, 128, num_layers=4, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=12, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 569
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1)\n",
        "\n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 570,
      "metadata": {
        "id": "mjWDX2CJaSHh",
        "outputId": "facbade4-20de-4b62-bb85-4ef58d0e45b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 1,629,196 par치metros entrenables.\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} par치metros entrenables.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVqBqerlaSHk"
      },
      "source": [
        "Notar que definimos los embeddings que representan a \\<unk\\> y \\<pad\\>  como [0, 0, ..., 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVZvHtwpaSHq"
      },
      "source": [
        "#### **Definimos el optimizador**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH6o8_cTaSHq"
      },
      "outputs": [],
      "source": [
        "# Optimizador\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 1:\n",
        "# Optimizador\n",
        "optimizer = optim.Adam(model.parameters(),lr=1e-4,weight_decay=1e-7)"
      ],
      "metadata": {
        "id": "JwG7OPfc52iU"
      },
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 2: BUENO\n",
        "# Optimizador\n",
        "optimizer = optim.Adam(model.parameters(),lr=1e-2,weight_decay=1e-8)\n",
        "\n",
        "# PRUEBA 2:\n",
        "# Optimizador\n",
        "optimizer = optim.Adam(model.parameters(),lr=1e-2,weight_decay=1e-7,eps=1e-4)"
      ],
      "metadata": {
        "id": "4sgNxutc53nE"
      },
      "execution_count": 571,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 3:\n",
        "# Optimizador\n",
        "optimizer = optim.Adam(model.parameters(),lr=1e-3,weight_decay=1e-8)\n",
        "#optimizer = optim.SGD(model.parameters(),lr=1e-2)"
      ],
      "metadata": {
        "id": "1kWGRBQTGLhj"
      },
      "execution_count": 474,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 4:\n",
        "# Optimizador\n",
        "optimizer = optim.Adam(model.parameters(),lr=1e-2,weight_decay=1e-6)"
      ],
      "metadata": {
        "id": "by6yOgLdvLeH"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 5:\n",
        "# Optimizador\n",
        "optimizer = optim.Adam(model.parameters(),lr=1e-2,weight_decay=1e-8)"
      ],
      "metadata": {
        "id": "zENpYpa8ho-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz39wa78wGYR"
      },
      "source": [
        "#### **Enviamos el modelo a cuda**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqr0AJ6_iicR"
      },
      "outputs": [],
      "source": [
        "# Enviamos el modelo y la loss a cuda (en el caso en que est칠 disponible)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 1:\n",
        "# Enviamos el modelo y la loss a cuda (en el caso en que est칠 disponible)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "4XtY-zy456x6"
      },
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 2:\n",
        "# Enviamos el modelo y la loss a cuda (en el caso en que est칠 disponible)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "2zB9UzT25761"
      },
      "execution_count": 572,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 3:\n",
        "# Enviamos el modelo y la loss a cuda (en el caso en que est칠 disponible)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "8_gX7m-qDzFj"
      },
      "execution_count": 477,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 4:\n",
        "# Enviamos el modelo y la loss a cuda (en el caso en que est칠 disponible)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "dUMUggQJvO_6"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 5:\n",
        "# Enviamos el modelo y la loss a cuda (en el caso en que est칠 disponible)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "prAmhWjx8M1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xlq48WjiW6U"
      },
      "source": [
        "#### **Definimos el entrenamiento de la red**\n",
        "\n",
        "Algunos conceptos previos:\n",
        "\n",
        "- `epoch` : una pasada de entrenamiento completa de una dataset.\n",
        "- `batch`: una fracci칩n de la 칠poca. Se utilizan para entrenar mas r치pidamente la red. (mas eficiente pasar n datos que uno en cada ejecuci칩n del backpropagation)\n",
        "\n",
        "Esta funci칩n est치 encargada de entrenar la red en una 칠poca. Para esto, por cada batch de la 칠poca actual, predice los tags del texto, calcula su loss y luego hace backpropagation para actualizar los pesos de la red.\n",
        "\n",
        "Observaci칩n: En algunos comentarios aparecer치 el tama침o de los tensores entre corchetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 573,
      "metadata": {
        "id": "DV6YLt0oiicW"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Por cada batch del iterador de la 칠poca:\n",
        "    for tags, text in iterator:\n",
        "        # Reiniciamos los gradientes calculados en la iteraci칩n anterior\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Predecimos los tags del texto del batch.\n",
        "        predictions = model(text.to(device))\n",
        "\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "\n",
        "        # Reordenamos los datos para calcular la loss\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        #ipdb.set_trace()\n",
        "        tags = torch.reshape(tags, (-1,)).to(device)\n",
        "\n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "\n",
        "\n",
        "\n",
        "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "        loss = criterion(predictions, tags)\n",
        "\n",
        "        # Calculamos el accuracy\n",
        "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "        # Calculamos los gradientes\n",
        "        loss.backward()\n",
        "\n",
        "        # Actualizamos los par치metros de la red\n",
        "        optimizer.step()\n",
        "\n",
        "        # Actualizamos el loss y las m칠tricas\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_precision += precision\n",
        "        epoch_recall += recall\n",
        "        epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYNcwKnAz5Hf"
      },
      "source": [
        "#### **Definimos la funci칩n de evaluaci칩n**\n",
        "\n",
        "Evalua el rendimiento actual de la red usando los datos de validaci칩n.\n",
        "\n",
        "Por cada batch de estos datos, calcula y reporta el loss y las m칠tricas asociadas al conjunto de validaci칩n.\n",
        "Ya que las m칠tricas son calculadas por cada batch, estas son retornadas promediadas por el n칰mero de batches entregados. (ver linea del return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 574,
      "metadata": {
        "id": "WsRuiUuHiicY"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Indicamos que ahora no guardaremos los gradientes\n",
        "    with torch.no_grad():\n",
        "        # Por cada batch\n",
        "        for tags, text in iterator:\n",
        "            # Predecimos\n",
        "            predictions = model(text.to(device))\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = torch.reshape(tags, (-1,)).to(device)\n",
        "\n",
        "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            # Calculamos las m칠tricas\n",
        "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "            # Actualizamos el loss y las m칠tricas\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_precision += precision\n",
        "            epoch_recall += recall\n",
        "            epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 460,
      "metadata": {
        "id": "Xs-n9Y5yiica"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy3MVf5H0A94"
      },
      "source": [
        "\n",
        "#### **Entrenamiento de la red**\n",
        "\n",
        "En este cuadro de c칩digo ejecutaremos el entrenamiento de la red.\n",
        "Para esto, primero definiremos el n칰mero de 칠pocas y luego por cada 칠poca, ejecutaremos `train` y `evaluate`.\n",
        "\n",
        "**Importante: Reiniciar los pesos del modelo**\n",
        "\n",
        "Si ejecutas nuevamente esta celda, se seguira entrenando el mismo modelo una y otra vez.\n",
        "Para reiniciar el modelo se debe ejecutar nuevamente la celda que contiene la funci칩n `init_weights`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 575,
      "metadata": {
        "id": "iK5lQqpviicf",
        "outputId": "03c257f0-76a4-4cbc-8486-b84abdb11101",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 1.136 | Train f1: 0.05 | Train precision: 0.20 | Train recall: 0.03\n",
            "\t Val. Loss: 0.996 |  Val. f1: 0.19 |  Val. precision: 0.52 | Val. recall: 0.11\n",
            "Epoch: 02 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.952 | Train f1: 0.27 | Train precision: 0.55 | Train recall: 0.19\n",
            "\t Val. Loss: 0.804 |  Val. f1: 0.37 |  Val. precision: 0.66 | Val. recall: 0.25\n",
            "Epoch: 03 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.762 | Train f1: 0.44 | Train precision: 0.66 | Train recall: 0.33\n",
            "\t Val. Loss: 0.630 |  Val. f1: 0.54 |  Val. precision: 0.71 | Val. recall: 0.44\n",
            "Epoch: 04 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.649 | Train f1: 0.54 | Train precision: 0.71 | Train recall: 0.44\n",
            "\t Val. Loss: 0.560 |  Val. f1: 0.58 |  Val. precision: 0.75 | Val. recall: 0.48\n",
            "Epoch: 05 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.577 | Train f1: 0.59 | Train precision: 0.74 | Train recall: 0.49\n",
            "\t Val. Loss: 0.506 |  Val. f1: 0.62 |  Val. precision: 0.76 | Val. recall: 0.52\n",
            "Epoch: 06 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.529 | Train f1: 0.62 | Train precision: 0.75 | Train recall: 0.53\n",
            "\t Val. Loss: 0.470 |  Val. f1: 0.65 |  Val. precision: 0.77 | Val. recall: 0.57\n",
            "Epoch: 07 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.488 | Train f1: 0.66 | Train precision: 0.76 | Train recall: 0.58\n",
            "\t Val. Loss: 0.434 |  Val. f1: 0.68 |  Val. precision: 0.78 | Val. recall: 0.61\n",
            "Epoch: 08 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.453 | Train f1: 0.68 | Train precision: 0.78 | Train recall: 0.61\n",
            "\t Val. Loss: 0.430 |  Val. f1: 0.69 |  Val. precision: 0.75 | Val. recall: 0.64\n",
            "Epoch: 09 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.430 | Train f1: 0.70 | Train precision: 0.78 | Train recall: 0.63\n",
            "\t Val. Loss: 0.415 |  Val. f1: 0.70 |  Val. precision: 0.76 | Val. recall: 0.65\n",
            "Epoch: 10 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.408 | Train f1: 0.71 | Train precision: 0.79 | Train recall: 0.65\n",
            "\t Val. Loss: 0.406 |  Val. f1: 0.71 |  Val. precision: 0.74 | Val. recall: 0.68\n",
            "Epoch: 11 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.391 | Train f1: 0.72 | Train precision: 0.79 | Train recall: 0.66\n",
            "\t Val. Loss: 0.391 |  Val. f1: 0.72 |  Val. precision: 0.73 | Val. recall: 0.70\n",
            "Epoch: 12 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.369 | Train f1: 0.73 | Train precision: 0.80 | Train recall: 0.68\n",
            "\t Val. Loss: 0.385 |  Val. f1: 0.72 |  Val. precision: 0.73 | Val. recall: 0.71\n",
            "Epoch: 13 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.360 | Train f1: 0.74 | Train precision: 0.81 | Train recall: 0.69\n",
            "\t Val. Loss: 0.388 |  Val. f1: 0.73 |  Val. precision: 0.74 | Val. recall: 0.72\n",
            "Epoch: 14 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.344 | Train f1: 0.75 | Train precision: 0.80 | Train recall: 0.70\n",
            "\t Val. Loss: 0.371 |  Val. f1: 0.73 |  Val. precision: 0.76 | Val. recall: 0.71\n",
            "Epoch: 15 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.330 | Train f1: 0.76 | Train precision: 0.82 | Train recall: 0.72\n",
            "\t Val. Loss: 0.356 |  Val. f1: 0.74 |  Val. precision: 0.78 | Val. recall: 0.70\n",
            "Epoch: 16 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.319 | Train f1: 0.77 | Train precision: 0.81 | Train recall: 0.73\n",
            "\t Val. Loss: 0.358 |  Val. f1: 0.75 |  Val. precision: 0.78 | Val. recall: 0.73\n",
            "Epoch: 17 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.305 | Train f1: 0.78 | Train precision: 0.82 | Train recall: 0.74\n",
            "\t Val. Loss: 0.352 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.73\n",
            "Epoch: 18 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.301 | Train f1: 0.78 | Train precision: 0.82 | Train recall: 0.74\n",
            "\t Val. Loss: 0.353 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.74\n",
            "Epoch: 19 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.294 | Train f1: 0.79 | Train precision: 0.83 | Train recall: 0.75\n",
            "\t Val. Loss: 0.353 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.73\n",
            "Epoch: 20 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.283 | Train f1: 0.79 | Train precision: 0.83 | Train recall: 0.76\n",
            "\t Val. Loss: 0.360 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.73\n",
            "Epoch: 21 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.280 | Train f1: 0.79 | Train precision: 0.83 | Train recall: 0.76\n",
            "\t Val. Loss: 0.354 |  Val. f1: 0.75 |  Val. precision: 0.78 | Val. recall: 0.73\n",
            "Epoch: 22 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.268 | Train f1: 0.80 | Train precision: 0.83 | Train recall: 0.77\n",
            "\t Val. Loss: 0.353 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.73\n",
            "Epoch: 23 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.265 | Train f1: 0.80 | Train precision: 0.84 | Train recall: 0.77\n",
            "\t Val. Loss: 0.363 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.73\n",
            "Epoch: 24 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.263 | Train f1: 0.81 | Train precision: 0.84 | Train recall: 0.77\n",
            "\t Val. Loss: 0.365 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.73\n",
            "Epoch: 25 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.260 | Train f1: 0.81 | Train precision: 0.84 | Train recall: 0.78\n",
            "\t Val. Loss: 0.355 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Epoch: 26 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.249 | Train f1: 0.81 | Train precision: 0.85 | Train recall: 0.79\n",
            "\t Val. Loss: 0.359 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Epoch: 27 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.245 | Train f1: 0.82 | Train precision: 0.85 | Train recall: 0.79\n",
            "\t Val. Loss: 0.369 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Epoch: 28 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.242 | Train f1: 0.82 | Train precision: 0.85 | Train recall: 0.79\n",
            "\t Val. Loss: 0.358 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.73\n",
            "Epoch: 29 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.235 | Train f1: 0.83 | Train precision: 0.86 | Train recall: 0.80\n",
            "\t Val. Loss: 0.371 |  Val. f1: 0.76 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Epoch: 30 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.231 | Train f1: 0.83 | Train precision: 0.86 | Train recall: 0.80\n",
            "\t Val. Loss: 0.365 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Epoch: 31 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.228 | Train f1: 0.83 | Train precision: 0.86 | Train recall: 0.81\n",
            "\t Val. Loss: 0.373 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Epoch: 32 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.229 | Train f1: 0.83 | Train precision: 0.86 | Train recall: 0.80\n",
            "\t Val. Loss: 0.383 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Epoch: 33 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.225 | Train f1: 0.83 | Train precision: 0.86 | Train recall: 0.81\n",
            "\t Val. Loss: 0.386 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Epoch: 34 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.220 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.82\n",
            "\t Val. Loss: 0.378 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Epoch: 35 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.222 | Train f1: 0.83 | Train precision: 0.86 | Train recall: 0.81\n",
            "\t Val. Loss: 0.369 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Epoch: 36 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.213 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.82\n",
            "\t Val. Loss: 0.394 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Epoch: 37 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.215 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.82\n",
            "\t Val. Loss: 0.389 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Epoch: 38 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.212 | Train f1: 0.84 | Train precision: 0.87 | Train recall: 0.82\n",
            "\t Val. Loss: 0.382 |  Val. f1: 0.76 |  Val. precision: 0.80 | Val. recall: 0.72\n",
            "Epoch: 39 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.207 | Train f1: 0.84 | Train precision: 0.87 | Train recall: 0.82\n",
            "\t Val. Loss: 0.408 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.73\n",
            "Epoch: 40 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.208 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.82\n",
            "\t Val. Loss: 0.393 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.73\n",
            "Epoch: 41 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.202 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.83\n",
            "\t Val. Loss: 0.383 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.73\n",
            "Epoch: 42 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.203 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.83\n",
            "\t Val. Loss: 0.381 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Epoch: 43 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.195 | Train f1: 0.85 | Train precision: 0.88 | Train recall: 0.83\n",
            "\t Val. Loss: 0.385 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.75\n",
            "Epoch: 44 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.198 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.83\n",
            "\t Val. Loss: 0.378 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Epoch: 45 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.199 | Train f1: 0.85 | Train precision: 0.88 | Train recall: 0.83\n",
            "\t Val. Loss: 0.372 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.73\n",
            "Epoch: 46 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.195 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.83\n",
            "\t Val. Loss: 0.372 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Epoch: 47 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.201 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.83\n",
            "\t Val. Loss: 0.381 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Epoch: 48 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.202 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.83\n",
            "\t Val. Loss: 0.396 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Epoch: 49 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.195 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.83\n",
            "\t Val. Loss: 0.399 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Epoch: 50 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.194 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.83\n",
            "\t Val. Loss: 0.384 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Epoch: 51 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.189 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.84\n",
            "\t Val. Loss: 0.405 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Epoch: 52 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.191 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.84\n",
            "\t Val. Loss: 0.395 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Epoch: 53 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.187 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.84\n",
            "\t Val. Loss: 0.387 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.74\n",
            "Epoch: 54 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.183 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.84\n",
            "\t Val. Loss: 0.406 |  Val. f1: 0.78 |  Val. precision: 0.81 | Val. recall: 0.75\n",
            "Epoch: 55 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.184 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.84\n",
            "\t Val. Loss: 0.397 |  Val. f1: 0.78 |  Val. precision: 0.80 | Val. recall: 0.76\n",
            "Epoch: 56 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.182 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.84\n",
            "\t Val. Loss: 0.412 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Epoch: 57 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.177 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.85\n",
            "\t Val. Loss: 0.433 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.75\n",
            "Epoch: 58 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.179 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.85\n",
            "\t Val. Loss: 0.381 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Epoch: 59 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.178 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.85\n",
            "\t Val. Loss: 0.370 |  Val. f1: 0.78 |  Val. precision: 0.81 | Val. recall: 0.75\n",
            "Epoch: 60 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.178 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.85\n",
            "\t Val. Loss: 0.391 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.75\n",
            "Epoch: 61 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.176 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.85\n",
            "\t Val. Loss: 0.392 |  Val. f1: 0.78 |  Val. precision: 0.81 | Val. recall: 0.75\n",
            "Epoch: 62 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.169 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.85\n",
            "\t Val. Loss: 0.398 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.75\n",
            "Epoch: 63 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.175 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.85\n",
            "\t Val. Loss: 0.393 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.73\n",
            "Epoch: 64 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.174 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.85\n",
            "\t Val. Loss: 0.396 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.75\n",
            "Epoch: 65 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.175 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.85\n",
            "\t Val. Loss: 0.382 |  Val. f1: 0.78 |  Val. precision: 0.81 | Val. recall: 0.75\n",
            "Epoch: 66 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.173 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.85\n",
            "\t Val. Loss: 0.400 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.73\n",
            "Epoch: 67 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.167 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.86\n",
            "\t Val. Loss: 0.386 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.74\n",
            "Epoch: 68 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.174 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.85\n",
            "\t Val. Loss: 0.410 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Epoch: 69 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.169 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.85\n",
            "\t Val. Loss: 0.404 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.74\n",
            "Epoch: 70 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.172 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.85\n",
            "\t Val. Loss: 0.404 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.74\n",
            "Epoch: 71 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.171 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.85\n",
            "\t Val. Loss: 0.389 |  Val. f1: 0.78 |  Val. precision: 0.81 | Val. recall: 0.74\n",
            "Epoch: 72 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.165 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.86\n",
            "\t Val. Loss: 0.394 |  Val. f1: 0.78 |  Val. precision: 0.81 | Val. recall: 0.75\n",
            "Epoch: 73 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.166 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.86\n",
            "\t Val. Loss: 0.393 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.74\n",
            "Epoch: 74 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.160 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.87\n",
            "\t Val. Loss: 0.407 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.74\n",
            "Epoch: 75 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.164 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.86\n",
            "\t Val. Loss: 0.381 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.74\n",
            "Epoch: 76 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.162 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.86\n",
            "\t Val. Loss: 0.407 |  Val. f1: 0.78 |  Val. precision: 0.80 | Val. recall: 0.76\n",
            "Epoch: 77 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.160 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.86\n",
            "\t Val. Loss: 0.390 |  Val. f1: 0.78 |  Val. precision: 0.81 | Val. recall: 0.76\n",
            "Epoch: 78 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.158 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.87\n",
            "\t Val. Loss: 0.402 |  Val. f1: 0.78 |  Val. precision: 0.81 | Val. recall: 0.75\n",
            "Epoch: 79 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.157 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.86\n",
            "\t Val. Loss: 0.405 |  Val. f1: 0.78 |  Val. precision: 0.81 | Val. recall: 0.75\n",
            "Epoch: 80 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.155 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.87\n",
            "\t Val. Loss: 0.410 |  Val. f1: 0.78 |  Val. precision: 0.80 | Val. recall: 0.75\n",
            "Epoch: 81 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.161 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.86\n",
            "\t Val. Loss: 0.380 |  Val. f1: 0.78 |  Val. precision: 0.81 | Val. recall: 0.75\n",
            "Epoch: 82 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.158 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.86\n",
            "\t Val. Loss: 0.412 |  Val. f1: 0.78 |  Val. precision: 0.80 | Val. recall: 0.75\n",
            "Epoch: 83 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.153 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.87\n",
            "\t Val. Loss: 0.392 |  Val. f1: 0.78 |  Val. precision: 0.80 | Val. recall: 0.76\n",
            "Epoch: 84 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.154 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.87\n",
            "\t Val. Loss: 0.399 |  Val. f1: 0.78 |  Val. precision: 0.80 | Val. recall: 0.76\n",
            "Epoch: 85 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.156 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.87\n",
            "\t Val. Loss: 0.400 |  Val. f1: 0.78 |  Val. precision: 0.81 | Val. recall: 0.76\n",
            "Epoch: 86 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.158 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.87\n",
            "\t Val. Loss: 0.380 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.74\n",
            "Epoch: 87 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.157 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.87\n",
            "\t Val. Loss: 0.391 |  Val. f1: 0.78 |  Val. precision: 0.81 | Val. recall: 0.75\n",
            "Epoch: 88 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.192 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.84\n",
            "\t Val. Loss: 0.395 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.74\n",
            "Epoch: 89 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.180 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.84\n",
            "\t Val. Loss: 0.399 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.75\n",
            "Epoch: 90 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.166 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.86\n",
            "\t Val. Loss: 0.394 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Epoch: 91 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.169 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.86\n",
            "\t Val. Loss: 0.403 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.75\n",
            "Epoch: 92 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.166 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.86\n",
            "\t Val. Loss: 0.411 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.74\n",
            "Epoch: 93 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.167 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.86\n",
            "\t Val. Loss: 0.398 |  Val. f1: 0.78 |  Val. precision: 0.80 | Val. recall: 0.76\n",
            "Epoch: 94 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.164 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.86\n",
            "\t Val. Loss: 0.415 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.75\n",
            "Epoch: 95 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.162 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.86\n",
            "\t Val. Loss: 0.419 |  Val. f1: 0.77 |  Val. precision: 0.82 | Val. recall: 0.74\n",
            "Epoch: 96 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.160 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.86\n",
            "\t Val. Loss: 0.402 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.74\n",
            "Epoch: 97 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.156 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.87\n",
            "\t Val. Loss: 0.410 |  Val. f1: 0.77 |  Val. precision: 0.81 | Val. recall: 0.75\n",
            "Epoch: 98 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.153 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.87\n",
            "\t Val. Loss: 0.424 |  Val. f1: 0.78 |  Val. precision: 0.80 | Val. recall: 0.75\n",
            "Epoch: 99 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.152 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.87\n",
            "\t Val. Loss: 0.430 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.75\n",
            "Epoch: 100 | Epoch Time: 0m 5s\n",
            "\tTrain Loss: 0.154 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.87\n",
            "\t Val. Loss: 0.416 |  Val. f1: 0.78 |  Val. precision: 0.81 | Val. recall: 0.75\n"
          ]
        }
      ],
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Recuerdo: dataloader_train y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "    # Entrenar\n",
        "    train_loss, train_precision, train_recall, train_f1 = train(\n",
        "        model, dataloader_train, optimizer, criterion)\n",
        "\n",
        "    # Evaluar (valid = validaci칩n)\n",
        "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "        model, dataloader_dev, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de c칩digo.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "    # Si ya no mejoramos el loss de validaci칩n, terminamos de entrenar.\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(\n",
        "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "    )\n",
        "    print(\n",
        "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcZPraG-9duO"
      },
      "source": [
        "**Importante**: Recuerden que el 칰ltimo modelo entrenado no es el mejor (probablemente est칠 *overfitteado*), si no el que guardamos con la menor loss del conjunto de validaci칩n. Este problema lo pueden solucionar con *early stopping*.\n",
        "Para cargar el mejor modelo entrenado, ejecuten la siguiente celda.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 561,
      "metadata": {
        "id": "y27CNYfrjtQ-",
        "outputId": "b29a9abb-2c1d-4cef-b218-6f0c217c2cf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 561
        }
      ],
      "source": [
        "# cargar el mejor modelo entrenado.\n",
        "model.load_state_dict(torch.load('{}.pt'.format(model_name)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 533,
      "metadata": {
        "id": "oLuqFKFR9duO"
      },
      "outputs": [],
      "source": [
        "# Limpiar ram de cuda\n",
        "torch.cuda.empty_cache()\n",
        "del model\n",
        "del optimizer\n",
        "del criterion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, dataloader_dev, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ],
      "metadata": {
        "id": "TTlaGKaWa0TK",
        "outputId": "225ecc15-a59e-451a-ef98-95fed6d886bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 562,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val. Loss: 0.331 |  Val. f1: 0.77 | Val. precision: 0.79 | Val. recall: 0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 520,
      "metadata": {
        "id": "GzjsUwYfIXWw",
        "outputId": "b51ac9ab-20f4-474f-f850-88911a3e3189",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val. Loss: 0.346 |  Val. f1: 0.78 | Val. precision: 0.82 | Val. recall: 0.75\n"
          ]
        }
      ],
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, dataloader_dev, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 1:\n",
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, dataloader_dev, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ],
      "metadata": {
        "id": "VRrZsO4nQ-pd",
        "outputId": "0f4e0712-e50e-41a5-dc59-154fc1396e9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 424,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val. Loss: 0.338 |  Val. f1: 0.78 | Val. precision: 0.80 | Val. recall: 0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBctQHTh0lxD"
      },
      "source": [
        "#### **Evaluamos el set de validaci칩n con el modelo final**\n",
        "\n",
        "Estos son los resultados de predecir el dataset de evaluaci칩n con el *mejor* modelo entrenado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0gVbP8yiicj"
      },
      "outputs": [],
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, dataloader_dev, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PRUEBA 1:\n",
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, dataloader_dev, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ],
      "metadata": {
        "id": "Vu_oxr4kRGBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF1ysw_Kw6zz"
      },
      "source": [
        "### **Predecir datos para la competencia**\n",
        "\n",
        "Ahora, a partir de los datos de **test** y nuestro modelo entrenado, vamos a predecir las etiquetas que ser치n evaluadas en la competencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RBs3UU4wLk3"
      },
      "outputs": [],
      "source": [
        "def predict_labels(model, iterator, criterion, fields=(TEXT, NER_TAGS)):\n",
        "\n",
        "    # Extraemos los vocabularios.\n",
        "    text_field = fields[0]\n",
        "    nertags_field = fields[1]\n",
        "    tags_vocab = nertags_field.vocab.get_itos()\n",
        "    words_vocab = text_field.vocab.get_itos()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for tags, text in iterator:\n",
        "\n",
        "            text_batch = text\n",
        "            text_batch = torch.transpose(text_batch, 0, 1).tolist()\n",
        "\n",
        "            # Predecir los tags de las sentences del batch\n",
        "            predictions_batch = model(text)\n",
        "            predictions_batch = torch.transpose(predictions_batch, 0, 1)\n",
        "\n",
        "            # por cada oraci칩n predicha:\n",
        "            for sentence, sentence_prediction in zip(text_batch,\n",
        "                                                     predictions_batch):\n",
        "                for word_idx, word_predictions in zip(sentence,\n",
        "                                                      sentence_prediction):\n",
        "                    # Obtener el indice del tag con la probabilidad mas alta.\n",
        "                    argmax_index = word_predictions.topk(1)[1]\n",
        "\n",
        "                    current_tag = tags_vocab[argmax_index]\n",
        "                    # Obtenemos la palabra\n",
        "                    current_word = words_vocab[word_idx]\n",
        "\n",
        "                    if current_word != '<PAD>':\n",
        "                        predictions.append([current_word, current_tag])\n",
        "                predictions.append(['EOS', 'EOS'])\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "predictions = predict_labels(model, dataloader_test, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwQp1Ru8Oht8"
      },
      "source": [
        "### **Generar el archivo para la submission**\n",
        "\n",
        "No hay problema si aparecen unk en la salida. Estos no son relevantes para evaluarlos, usamos solo los tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPfZkjJGkWyq"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "\n",
        "if (os.path.isfile('./predictions.zip')):\n",
        "    os.remove('./predictions.zip')\n",
        "\n",
        "if (not os.path.isdir('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "f = open('predictions/predictions.txt', 'w')\n",
        "for i, (word, tag) in enumerate(predictions[:-1]):\n",
        "    if word=='EOS' and tag=='EOS': f.write('\\n')\n",
        "    else:\n",
        "      if i == len(predictions[:-1])-1:\n",
        "        f.write(word + ' ' + tag)\n",
        "      else: f.write(word + ' ' + tag + '\\n')\n",
        "\n",
        "f.close()\n",
        "\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZEWJXrNaSIf"
      },
      "source": [
        "## **Conclusiones**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAtK7y43V7Z_"
      },
      "source": [
        "    Escriba aqu칤 sus conclusiones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpSyNYKlEHtz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}