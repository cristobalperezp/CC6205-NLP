{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cristobalperezp/CC6205-NLP/blob/main/Tarea_2_Language_Modeling_y_Nai%CC%88ve_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2obO44rRIDIm"
      },
      "source": [
        "# **Tarea 2 - CC6205 Natural Language Processing üìö**\n",
        "\n",
        "**Integrantes:** Marcelo Rojas y Crist√≥bal P√©rez.\n",
        "\n",
        "**Fecha l√≠mite de entrega üìÜ:** Martes 18 de abril.\n",
        "\n",
        "**Tiempo estimado de dedicaci√≥n:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpupcv6QW2fd"
      },
      "source": [
        "Bienvenid@s a la segunda tarea del curso de Natural Language Processing (NLP). En esta tarea estaremos modelando probabil√≠sticamente el lenguaje mediante **Languaje Modeling** y clasificando textos mediante el m√©todo **Na√Øve Bayes**. Espec√≠ficamente, la tarea consta de una parte te√≥rica que busca evaluar conceptos vistos en clases sobre los **Language Models** y una parte pr√°ctica donde **programar√°n a mano** el m√©todo **Na√Øve Bayes**. \n",
        "\n",
        "**Instrucciones:**\n",
        "- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo Jupyter Notebook o el archivo .ipynb si lo ejecuto de forma local.\n",
        "- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n. \n",
        "- Est√° **PROHIBIDO** usar cualquier librer√≠a que implemente los algoritmos pedidos (Spacy, scikit, etc). S√≥lo se podr√°n utilizar las librer√≠as importadas al inicio de la secci√≥n de pr√°ctica.\n",
        "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso.\n",
        "\n",
        "Si a√∫n no han visto las clases, se recomienda visitar los links de las referencias.\n",
        "\n",
        "**Referencias:**\n",
        "\n",
        "Slides:\n",
        "    \n",
        "- [Language Models](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/lmslides.pdf) (slides by Michael Collins)\n",
        "- [Text Classification and Naive Bayes](https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf) (slides by Dan Jurafsky)\n",
        "\n",
        "Videos: \n",
        "\n",
        "- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 1](https://www.youtube.com/watch?v=9E2jJ6kcb4Y&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=4)\n",
        "- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 2](https://www.youtube.com/watch?v=ZWqbEQXLra0&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=5)\n",
        "- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 3](https://www.youtube.com/watch?v=tsumFqwFlaA&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=6)\n",
        "- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 4](https://www.youtube.com/watch?v=s3TWdv4sqkg&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=7)\n",
        "- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 1](https://www.youtube.com/watch?v=kG9BK9Oy1hU)\n",
        "- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 2](https://www.youtube.com/watch?v=Iqte5kKHvzE)\n",
        "- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 3](https://www.youtube.com/watch?v=TSJg0_X3Abk)\n",
        "\n",
        "` ` "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vY3fYRGVsB_v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JstKx3TiKcIp"
      },
      "source": [
        "---------------------------\n",
        "## Parte 1. Preguntas te√≥ricas (3 pts)\n",
        "\n",
        "En esta parte responder√°n preguntas **te√≥ricas** sobre los Lenguage Models. Para comprender como funcionan muchas de las t√©cnicas que veremos posteriormente durante el curso ser√° muy √∫til que dominen estos modelos y sus fundamentos.\n",
        "\n",
        "Contestar cada pregunta en su celda correspondiente y **no extenderse m√°s de 100 palabras** . üôè\n",
        "\n",
        "**Nota:** Las preguntas deben ser resueltas con desarrollo, pero sin c√≥digo. Por favor usen $\\LaTeX$ para las f√≥rmulas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHqcRJ7Vr_8x"
      },
      "source": [
        "### 1. Definici√≥n de modelo de lenguaje (1 pt)\n",
        "\n",
        "Un grupo de anarquistas primitivistas quiere destruir todas las empresas tecnol√≥gicas y departamentos de ciencias de computaci√≥n porque \"los modelos de lenguaje tienen emociones y como seres sintientes pueden aburrirse de estar sometidos a los humanos y pudieran llegar a conquistar el mundo\". Explique seg√∫n lo visto en clase qu√© es un modelo de lenguaje y por qu√© los anarquistas primitivistas est√°n equivocados. Mencione de qu√© clase/slide obtuvo la informaci√≥n para que el grupo insurrecto pueda buscar por s√≠ mismo la referencia y evitar la destrucci√≥n de nuestro querido DCC. \n",
        "\n",
        "Puede utilizar ChatGPT (debe indicarlo), pero debe mencionar seg√∫n lo visto en clase por qu√© la respuesta entregada por ChatGPT es correcta o no."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F5R3Ji7sHjt"
      },
      "source": [
        "` ` \n",
        "\n",
        "**Respuesta ChatGPT:** \n",
        "\n",
        "Un modelo de lenguaje es un algoritmo de aprendizaje autom√°tico que utiliza datos de texto para aprender a predecir la siguiente palabra o secuencia de palabras en una oraci√≥n. Los modelos de lenguaje son utilizados en una amplia variedad de aplicaciones, incluyendo el reconocimiento de voz, la traducci√≥n autom√°tica, la generaci√≥n de texto y la clasificaci√≥n de texto.\n",
        "\n",
        "Es importante se√±alar que los modelos de lenguaje no tienen emociones ni son seres sintientes. Son simplemente herramientas de software dise√±adas para procesar y analizar datos de texto. Aunque pueden ser programados para realizar tareas espec√≠ficas, no tienen la capacidad de \"aburrirse\" ni de \"conquistar el mundo\".\n",
        "\n",
        "La preocupaci√≥n expresada por los anarquistas primitivistas es infundada e irracional. La tecnolog√≠a y la ciencia de la computaci√≥n han proporcionado avances significativos en la mejora de la vida humana, desde el acceso a informaci√≥n y recursos, hasta la automatizaci√≥n de tareas y la soluci√≥n de problemas complejos. La destrucci√≥n de empresas tecnol√≥gicas y departamentos de ciencias de la computaci√≥n tendr√≠a graves consecuencias negativas para la sociedad, desde la p√©rdida de empleos y la interrupci√≥n de servicios cr√≠ticos, hasta el retroceso en el avance cient√≠fico y tecnol√≥gico.\n",
        "\n",
        "Es importante considerar cuidadosamente las consecuencias de nuestras acciones y decisiones. En lugar de destruir la tecnolog√≠a y la ciencia de la computaci√≥n, debemos trabajar juntos para garantizar que estas herramientas se utilicen de manera responsable y √©tica para el beneficio de la humanidad y el planeta en general.\n",
        "\n",
        "**Respuesta:** \n",
        "\n",
        "La respuesta dada por chatGPT es correcta, es decir, los anarquistas primitivistas est√°n equivocados, pues como se vi√≥ en clases (slides 1 y 2 de language models) un modelo de lenguaje es solo un modelamiento estad√≠stico (o probabil√≠stico) del lenguaje y solo se asocian probabilidades a palabras o secuencias de palabras a partir de lo cual se puede predecir o estimar, por lo que carecen de sentimiento.\n",
        "\n",
        "` ` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hwW-07MrRpt"
      },
      "source": [
        "\n",
        "### 2.1. Probabilidades (1 pt)\n",
        "\n",
        "Asuma que tenemos calculados los par√°metros $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$ para todas las posibles secuencias de tama√±o 4 que aparecen en un corpus $\\mathcal{C}$. Dado esto, muestre c√≥mo el modelo le asignar√≠a una probabilidad a la frase `una persona corriendo r√°pido`.\n",
        "\n",
        "No recomendamos utilizar herramientas como ChatGPT, pues suelen equivocarse en este tipo de preguntas, pero a√∫n as√≠ si lo hace, debe indicarlo. Cada vez que mencione contenidos del curso, cite la slide y la clase de la informaci√≥n.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzlQDAVqWNdX"
      },
      "source": [
        "` ` \n",
        "\n",
        "**Respuesta:** \n",
        "Siguiendo la idea de language model, lo que se quiere es calcular la probabilidad de la oraaci√≥n, esto es, $p(una,persona,corriendo,r√°pido)$. Como se tienen las probabilidades de n-gramas con $n=4$, la idea es usar markov de orden 3 (Los procesos de Markov se explican a partir de la slide 11).\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "p(una,persona,corriendo,r√°pido) = p(una) * p(persona|una) * p(corriendo|una,persona) * p(r√°pido|una,persona,corriendo)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Usando padding y Markov de orden 3 (Padding y stop word se explican en las slide 19 y 20):\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "p(una,persona,corriendo,r√°pido) = p(una,*,*,*) * p(persona|*,*,una) * p(corriendo|*,una,persona) * p(r√°pido|una,persona,corriendo)\n",
        "\\end{equation}\n",
        "\n",
        "` `"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAej_jqtVwm1"
      },
      "source": [
        "### 2.2 Estimando las probabilidades (1 pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXSFlCIex8kq"
      },
      "source": [
        "#### 2.2.1. Modelo simple (0.5 puntos)\n",
        "\n",
        "Si hubieses tenido que estimar las probabilidades condicionales (par√°metros del modelo) $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$ a partir de $\\mathcal{C}$, ¬øc√≥mo la definir√≠as siguiendo el principio de m√°xima verosimilitud?\n",
        "\n",
        "No recomendamos utilizar herramientas como ChatGPT, pues suelen equivocarse en este tipo de preguntas, pero a√∫n as√≠ si lo hace, debe indicarlo. Cada vez que mencione contenidos del curso, cite la slide y la clase de la informaci√≥n.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjNisxPzWsMG"
      },
      "source": [
        "` ` \n",
        "\n",
        "**Respuesta:** \n",
        "\n",
        "Siguiendo el principio de m√°xima verosimilitud, se tiene que la estimaci√≥n para $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$ se puede obtener contando los casos favorables y dividirlos por los casos totales, esto es:\n",
        "\n",
        "\\begin{equation}\n",
        "q(w_i | w_{i-3}, w_{i-2}, w_{i-1}) = \\frac{count(w_{i-3}, w_{i-2}, w_{i-1}, w_i)}{ count(w_{i-3}, w_{i-2}, w_{i-1})}\n",
        "\\end{equation}\n",
        "\n",
        "(Slide 25 de language models)\n",
        "\n",
        "` `"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwNkPIXure0C"
      },
      "source": [
        "#### 2.2.2. Modelo interpolado (0.5 puntos)\n",
        "Muestre c√≥mo se calcular√≠a $q(w_{i} | w_{i-3}, w_{i-2}, w_{i-1})$ usando un modelo que interpola modelos de lenguajes basados en 4-grams, trigramas, bigramas y unigramas. ¬øTe fue necesario definir nuevos par√°metros? En caso afirmativo, ¬øqu√© los diferencia de los par√°metros del modelo simple y qu√© propiedades deben cumplir?\n",
        "\n",
        "No recomendamos utilizar herramientas como ChatGPT, pues suelen equivocarse en este tipo de preguntas, pero a√∫n as√≠ si lo hace, debe indicarlo. Cada vez que mencione contenidos del curso, cite la slide y la clase de la informaci√≥n.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeLZAd0Tr9ne"
      },
      "source": [
        "` ` \n",
        "\n",
        "**Respuesta:** \n",
        "\n",
        "Supongamos que se tienen modelos de lenguajes basados en 4-grams, trigramas, bigramas y unigramas. Se conocen entonces sus par√°metros $q_{ml}(w_{i} | w_{i-3}, w_{i-2}, w_{i-1})$, $q_{ml}(w_{i} | w_{i-2}, w_{i-1})$, $q_{ml}(w_{i} | w_{i-1})$ y $q_{ml}(w_{i})$ respectivamente. Luego, usando interpolaci√≥n lineal:\n",
        "\n",
        "\\begin{equation}\n",
        "q(w_i | w_{i-3}, w_{i-2}, w_{i-1}) = Œª_1 q_{ml}(w_{i} | w_{i-3}, w_{i-2}, w_{i-1})+ Œª_2 q_{ml}(w_{i} |w_{i-2}, w_{i-1}) + Œª_3 q_{ml}(w_{i} | w_{i-1}) + Œª_4 q_{ml}(w_{i})\n",
        "\\end{equation}\n",
        "\n",
        "donde $Œª_1 + Œª_2 + Œª_3 + Œª_4 = 1$.\n",
        "\n",
        "Los par√°metros de los lenguague model se obtienen siguiendo el principio de m√°xima verosimilitud (como en la parte 2.2.1) y los par√°metros $Œª_n$ corresponden a h√≠perpar√°metros del modelo, es decir, no se pueden ajustar a partir de los datos del modelo. Los $Œª_n$ se buscan probando m√∫ltiples conjuntos de $Œª_n$ y se eligen los que minimizan la perplexity.\n",
        "\n",
        "\n",
        "(Slides 40 a la 50 de language models)\n",
        "\n",
        "` `\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdmB-07ZKfaa"
      },
      "source": [
        "-----------------------\n",
        "## Parte 2. Na√Øve Bayes (3 pts)\n",
        "En esta parte programaremos nuestro primer clasificador de documentos. Para esto implementaremos el m√©todo **Na√Øve Bayes Multinomial** usando **Laplace Smothing**.\n",
        "\n",
        "Por favor, documenten su c√≥digo. Escriban que hacen las funciones, que reciben, que entregan, etc. Si en alg√∫n lugar de su c√≥digo hacen algo que creen que debe ser explicado, lox comentarios son bienvenidos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpjjKnJUvRiA"
      },
      "source": [
        "### 2.1. Clase para clasificador (0.5 pt)\n",
        "\n",
        "Programa una clase `MyMultinomialNB` que en su inicializador reciba el parametro de generalizaci√≥n `alpha`.\n",
        "\n",
        "```python\n",
        "class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n",
        "  def __init__(self, ...):\n",
        "    ...\n",
        "```\n",
        "\n",
        "Para m√°s informacion sobre la construcci√≥n de esta clase puedes revisar [aqu√≠](https://sklearn-template.readthedocs.io/en/latest/user_guide.html#classifier) \n",
        "\n",
        "Una llamada de ejemplo para crear un objeto de tu clase ser√≠a:\n",
        "```python\n",
        "my_clf = MyMultinomialNB(alpha=1)\n",
        "``` \n",
        "lo que debiera crear un clasificador con par√°metro `alpha` igual a 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROG50eH0xfxO"
      },
      "source": [
        "### 2.2. Entrenamiento del clasificador (1 pt)\n",
        "\n",
        "Programa el entrenamiento de tu clasificador en el m√©todo `fit` de la clase `MyMultinomialNB`. La funci√≥n debiera recibir el par√°metro X que es un `DataFrame` de `pandas` con las columnas `words` y `class_`, donde `words` es una tupla con las palabras asociadas al cada documento y `class_` es el identificador de la clase a la que pertenece cada documento.\n",
        "\n",
        "Para computar el entrenamiento de nuestro clasificador debemos: \n",
        "- extraer el vocabulario,\n",
        "- determinar las probabilidades $p(c_j)$ para cada una de las clases posibles, \n",
        "- determinar las probabilidades $p(w_i|c_j)$ para cada una de las palabras y cada una de las clases usando **Laplace Smothing**.\n",
        "\n",
        "El resultado del metodo `fit` ser√° la misma instancia de nuestro clasificador `self`.\n",
        "\n",
        "```python\n",
        "class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n",
        "  def __init__(self, ...):\n",
        "    ...\n",
        "\n",
        "  def fit(self, X):\n",
        "    ...\n",
        "    return self\n",
        "```\n",
        "\n",
        "**Underflow prevention:** En vez de hacer muchas multiplicacions de `float`s, reempl√°cenlas por sumas de logaritmos para prevenir errores de precisi√≥n. Revisen la diapo 69 de las slides. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNouTCmR2FgY"
      },
      "source": [
        "### 2.3. Predicci√≥n (1 pt)\n",
        "\n",
        "Programa la predicci√≥n de tu clasificador en el m√©todo `predict` de la clase `MyMultinomialNB`. Al igual que la funci√≥n `fit`, `predict` debe recibir un `DataFrame` X con valores `None` en la columna `class_` y devolver una lista con las clases que maximizan la probabilidad de Bayes para cada uno de los elmentos de X (filas)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wyhFWeLgYDI"
      },
      "source": [
        "### Implementaci√≥n 2.1, 2.2 y 2.3 (2.5 pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYFEgTyw2ELL"
      },
      "source": [
        "# Ac√° implementar√°n las preguntas 2.1, 2.2 y 2.3,\n",
        "# tu c√≥digo debiera comenzar as√≠\n",
        "\n",
        "# importamos algunos paquetes necesarios, puede que necesites otros\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "\n",
        "class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n",
        "  def __init__(self, alpha=1.0):\n",
        "    \"\"\"\n",
        "        Constructor de la clase MyMultinomialNB.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        alpha : float, optional\n",
        "            Coeficiente de Laplace Smoothing. Por defecto es 1.0.\n",
        "    \"\"\"\n",
        "    # ac√° tu c√≥digo para inicializar el clasificador\n",
        "    self.alpha = alpha\n",
        "    self.vocab = None\n",
        "    self.classes = None\n",
        "    self.class_probs = None\n",
        "    self.word_probs = None\n",
        "\n",
        "  def fit(self, X):\n",
        "    \"\"\"\n",
        "        Ajusta el modelo a los datos.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pd.DataFrame\n",
        "          Matriz con los textos y clases.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "          Retorna la instancia.\n",
        "    \"\"\"\n",
        "    # ac√° tu c√≥digo para el entrenamiento del modelo\n",
        "\n",
        "    # Se extrae el voabulario\n",
        "    self.vocab = set()\n",
        "    for words in X['words']:\n",
        "      self.vocab.update(words)\n",
        "\n",
        "    # Se determina el n√∫mero de clases\n",
        "    self.classes = X['class_'].unique()\n",
        "    # Se determina el n√∫mero de documentos\n",
        "    total_docs = len(X)   \n",
        "\n",
        "    # Se determinan las probabilidades p(cj) para cada clase\n",
        "    self.class_probs = {}\n",
        "    for c_j in self.classes:\n",
        "      docs_j = X[ X['class_'] == c_j ]\n",
        "      prob_c_j = np.log( len(docs_j) / total_docs )\n",
        "      self.class_probs[c_j] = prob_c_j\n",
        "\n",
        "    # Se determinan las probabilidades p(wi|cj) para cada palabra y cada clase usando Laplace Smoothing\n",
        "    self.word_probs = {}\n",
        "    for c_j in self.classes:\n",
        "      docs_j = X[ X['class_'] == c_j ]\n",
        "      text_j = [word for words in docs_j['words'] for word in words]\n",
        "      n = len(text_j)\n",
        "\n",
        "      word_probs = {}\n",
        "      for word in self.vocab:\n",
        "        n_k = text_j.count(word)\n",
        "        word_prob = np.log( ( n_k + self.alpha ) / ( n + self.alpha * len(self.vocab )) )\n",
        "        word_probs[word] = word_prob\n",
        "      \n",
        "      self.word_probs[c_j] = word_probs\n",
        "    \n",
        "    return self\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "        Realiza la clasificaci√≥n en una matriz X.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pd.DataFrame\n",
        "          Matriz con los textos y clases.\n",
        "        Returns\n",
        "        -------\n",
        "        C : ndarray de forma (n_samples,)\n",
        "          Valores predichos para X.\n",
        "    \"\"\"\n",
        "    # Chequea que fit ha sido ejecutado anteriormente\n",
        "    check_is_fitted(self,['vocab', 'classes', 'class_probs', 'word_probs'])\n",
        "    \n",
        "    # ac√° tu c√≥digo para computar la predicci√≥n\n",
        "    predictions = []\n",
        "    for doc in X['words']:\n",
        "      max_prob = -np.inf\n",
        "      for c_j in self.classes:\n",
        "        word_prob = self.word_probs[c_j]\n",
        "        prob_c = self.class_probs[c_j]\n",
        "        for word in doc:\n",
        "          if word in word_prob:\n",
        "            prob_c = prob_c + word_prob[word]\n",
        "        if prob_c > max_prob:\n",
        "          max_prob = prob_c\n",
        "          predicted_c = c_j\n",
        "      predictions.append(predicted_c)\n",
        "    return predictions\n",
        "\n",
        "  def predict_proba(self,X):\n",
        "    \"\"\"\n",
        "        Devuelve estimaciones de probabilidad para el vector X.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pd.DataFrame\n",
        "          Matriz con los textos y clases.\n",
        "        Returns\n",
        "        -------\n",
        "        C : array-like of shape (n_samples, n_classes)\n",
        "          Devuelve la probabilidad de las muestras para cada clase en el modelo.\n",
        "    \"\"\"\n",
        "    # Chequea que fit ha sido ejecutado anteriormente\n",
        "    check_is_fitted(self,['vocab', 'classes', 'class_probs', 'word_probs'])\n",
        "\n",
        "    probs = []\n",
        "    # Mismo procedimiento que con predict\n",
        "    for doc in X['words']:\n",
        "      doc_probs = {}\n",
        "      for c_j in self.classes:\n",
        "        prob_c_j = self.class_probs[c_j]\n",
        "        word_probs = self.word_probs[c_j]\n",
        "        doc_prob = prob_c_j\n",
        "        for w_k in doc:\n",
        "          if w_k in word_probs:\n",
        "            doc_prob += word_probs[w_k]\n",
        "        doc_probs[c_j] = doc_prob\n",
        "      # Aqu√≠ la diferencia, por Underflow prevention, volvemos al expacio de probabilidades aplicando la exponencial y normalizamos para que la suma de 1\n",
        "      probs.append(\n",
        "          np.exp( list( doc_probs.values() ) ) / np.sum( np.exp( list( doc_probs.values() ) ) )\n",
        "      )\n",
        "    return probs    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(MyMultinomialNB())"
      ],
      "metadata": {
        "id": "-64G_Avow7f1",
        "outputId": "dbeacdda-a6b8-49e7-a316-e2fb4076ef57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on MyMultinomialNB in module __main__ object:\n",
            "\n",
            "class MyMultinomialNB(sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin)\n",
            " |  MyMultinomialNB(alpha=1.0)\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      MyMultinomialNB\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, alpha=1.0)\n",
            " |      Constructor de la clase MyMultinomialNB.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      alpha : float, optional\n",
            " |          Coeficiente de Laplace Smoothing. Por defecto es 1.0.\n",
            " |  \n",
            " |  fit(self, X)\n",
            " |      Ajusta el modelo a los datos.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : pd.DataFrame\n",
            " |        Matriz con los textos y clases.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |        Retorna la instancia.\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Realiza la clasificaci√≥n en una matriz X.\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : pd.DataFrame\n",
            " |        Matriz con los textos y clases.\n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : ndarray de forma (n_samples,)\n",
            " |        Valores predichos para X.\n",
            " |  \n",
            " |  predict_proba(self, X)\n",
            " |      Devuelve estimaciones de probabilidad para el vector X.\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : pd.DataFrame\n",
            " |        Matriz con los textos y clases.\n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array-like of shape (n_samples, n_classes)\n",
            " |        Devuelve la probabilidad de las muestras para cada clase en el modelo.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for `X`.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KOMJ-CS8PRP"
      },
      "source": [
        "### 2.4. Probando el clasificador (0.5 pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hucdz-R7xerG"
      },
      "source": [
        "A continuaci√≥n probar√°n el funcionamiento de su clasificador. Para esto, les presentamos un conjunto de documentos de entrenamiento `train_set` divididos en 2 categorias distintas. Ustedes deber√°n primero entrenar su clasificador usando el m√©todo `fit` de su clase y luego, clasificar los documentos del conjunto de prueva `test_set` usando el m√©todo `predict`.\n",
        "\n",
        "**NOTA:** Como pueden ver, los objetos `namedtuple`s tienen dos atributos: `words` donde est√°n las palabras del documento y `class_` donde se guarda la clase de ese documento. Estos objetos son inmutables, lo que quiere decir que si quieren modificar un documento y cambiarle la clase, tienen que crear otro documento. Otra cosa es que son tuplas como cualquier otra, es decir se pueden acceder usando indices como `doc[0]` o `doc[1]`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from collections import namedtuple"
      ],
      "metadata": {
        "id": "JjTnFLDGyCEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN1SdLycuK4W",
        "outputId": "6525be02-cd81-4143-a373-60c4d53fb8fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.1 Primer Caso: Clasificaci√≥n de ejemplo visto en clases (0.20 pt)"
      ],
      "metadata": {
        "id": "JfS5wXfwxx6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = namedtuple(\n",
        "    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",
        ")\n",
        "\n",
        "train_set = [['Chinese Beijing\tChinese', 'c'],\n",
        "             ['Chinese\tChinese\tShanghai','c'],\n",
        "             ['Chinese\tMacao','c'],\n",
        "             ['Tokyo\tJapan\tChinese','j']]\n",
        "\n",
        "train_set = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in train_set]\n",
        "X_train = pd.DataFrame(data=train_set)\n",
        "\n",
        "test_set = [['Chinese\tChinese\tChinese\tTokyo Japan', None]]\n",
        "test_set = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in test_set]\n",
        "X_test = pd.DataFrame(data=test_set)\n",
        "\n",
        "X_train = pd.DataFrame(data=train_set)\n",
        "print(\"Documentos de entrenamiento\")\n",
        "print(X_train)\n",
        "\n",
        "X_test = pd.DataFrame(data=test_set)\n",
        "print(\"\\nDocumentos de prueba:\")\n",
        "print(X_test)"
      ],
      "metadata": {
        "id": "5yXBv2Kqxyyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63567ea9-23a0-4f41-94e3-dc59407b566c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documentos de entrenamiento\n",
            "                          words class_\n",
            "0   (Chinese, Beijing, Chinese)      c\n",
            "1  (Chinese, Chinese, Shanghai)      c\n",
            "2              (Chinese, Macao)      c\n",
            "3       (Tokyo, Japan, Chinese)      j\n",
            "\n",
            "Documentos de prueba:\n",
            "                                       words class_\n",
            "0  (Chinese, Chinese, Chinese, Tokyo, Japan)   None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ac√° probar√°n su clasificador\n",
        "\n",
        "# inicializamos el clasificador\n",
        "my_clf = MyMultinomialNB(alpha=1)\n",
        "\n",
        "# entrenamos el clasificador para los datos de entrenamiento X_train\n",
        "my_clf.fit(X_train)\n",
        "\n",
        "# ac√° puedes ver el vocabulario extra√≠do por tu clasificador, \n",
        "# intenta tenerlo guardado en my_clf.vocab\n",
        "print('vocab: ', my_clf.vocab)\n",
        "\n",
        "# si implementaron el m√©todo predict_proba en el clasificador (no era obligatorio), \n",
        "# ac√° lo pueden probar\n",
        "print('\\nTest probs:')\n",
        "print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test)]))\n",
        "\n",
        "# obtengamos las predicciones \n",
        "print('\\nTest predictions:')\n",
        "print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_clf.predict(X_test), X_test['words'])]))"
      ],
      "metadata": {
        "id": "UAx0t3zQx2PJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2e12e76-198a-4bdb-d3aa-0bd5a8a4925d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab:  {'Macao', 'Shanghai', 'Tokyo', 'Beijing', 'Japan', 'Chinese'}\n",
            "\n",
            "Test probs:\n",
            "[0.68975861 0.31024139]\n",
            "\n",
            "Test predictions:\n",
            "c <- Chinese Chinese Chinese Tokyo Japan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta esperada:**\n",
        "\n",
        "**Nota:** No es necesario que obtenga exactamente la misma probabilidad, lo importante es que su clasificador genere la predicci√≥n expuesta.\n",
        "\n",
        "```python\n",
        "vocab:  ['Beijing', 'Chinese', 'Macao', 'Tokyo', 'Japan', 'Shanghai']\n",
        "\n",
        "Test probs:\n",
        "[0.68975861 0.31024139]\n",
        "\n",
        "Test predictions:\n",
        "c <- Chinese Chinese Chinese Tokyo Japan\n",
        "```"
      ],
      "metadata": {
        "id": "dMyCgXxvx29L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.2 Segundo Caso (0.30 pt)"
      ],
      "metadata": {
        "id": "mpG_t1wTx99m"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLi8PxdV2VQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef4d6c41-4cd3-4bb8-8862-3b105db22768"
      },
      "source": [
        "document = namedtuple(\n",
        "    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",
        ")\n",
        "\n",
        "train_set = (\n",
        "    document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n",
        "    document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n",
        "    document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n",
        "    document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n",
        "    document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n",
        "    document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=1),\n",
        "    document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n",
        "    document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1)\n",
        ")\n",
        "X_train = pd.DataFrame(data=train_set)\n",
        "print(\"Documentos de entrenamiento\")\n",
        "print(X_train)\n",
        "\n",
        "test_set = (document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),\n",
        "            document(words=('w03', 'w03', 'w04', 'w05', 'w01', 'w06', 'w09', 'w02'), class_=None))\n",
        "X_test = pd.DataFrame(data=test_set)\n",
        "print(\"\\nDocumentos de prueba:\")\n",
        "print(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documentos de entrenamiento\n",
            "                                           words  class_\n",
            "0            (w03, w01, w02, w06, w02, w08, w07)       0\n",
            "1  (w05, w04, w00, w06, w09, w07, w06, w09, w05)       1\n",
            "2  (w07, w06, w00, w08, w01, w08, w08, w09, w02)       0\n",
            "3            (w08, w09, w02, w06, w05, w08, w07)       1\n",
            "4            (w09, w08, w05, w08, w05, w00, w08)       1\n",
            "5            (w05, w05, w06, w01, w06, w08, w02)       1\n",
            "6            (w04, w03, w07, w05, w04, w00, w02)       0\n",
            "7       (w01, w00, w01, w04, w09, w02, w04, w07)       1\n",
            "\n",
            "Documentos de prueba:\n",
            "                                      words class_\n",
            "0  (w02, w09, w06, w01, w05, w04, w03, w03)   None\n",
            "1  (w03, w03, w04, w05, w01, w06, w09, w02)   None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXHwmOWB-4Aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "181f67f5-3074-4ada-8e8f-db2afde87655"
      },
      "source": [
        "# Ac√° probar√°n su clasificador\n",
        "\n",
        "# inicializamos el clasificador\n",
        "my_clf = MyMultinomialNB(alpha=1)\n",
        "\n",
        "# entrenamos el clasificador para los datos de entrenamiento X_train\n",
        "my_clf.fit(X_train)\n",
        "\n",
        "# ac√° puedes ver el vocabulario extra√≠do por tu clasificador, \n",
        "# intenta tenerlo guardado en my_clf.vocab\n",
        "print('vocab: ', my_clf.vocab)\n",
        "\n",
        "# si implementaron el m√©todo predict_proba en el clasificador (no era obligatorio), \n",
        "# ac√° lo pueden probar\n",
        "print('\\nTest probs:')\n",
        "print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test)]))\n",
        "\n",
        "# obtengamos las predicciones \n",
        "print('\\nTest predictions:')\n",
        "print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_clf.predict(X_test), X_test['words'])]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab:  {'w04', 'w07', 'w09', 'w05', 'w02', 'w06', 'w08', 'w00', 'w03', 'w01'}\n",
            "\n",
            "Test probs:\n",
            "[0.76018101 0.23981899]\n",
            "[0.76018101 0.23981899]\n",
            "\n",
            "Test predictions:\n",
            "0 <- w02 w09 w06 w01 w05 w04 w03 w03\n",
            "0 <- w03 w03 w04 w05 w01 w06 w09 w02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tDZnmns_1dW"
      },
      "source": [
        "#### 2.4.3 (OPCIONAL) Oraciones reales\n",
        "\n",
        "Aqu√≠ intentaremos entrenar un clasificador para determinar cuando una oracion en ingl√©s es interrogativa, afirmativa o negativa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCWi3oytd2nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "831c7b3c-5f27-4925-e6b9-68f794c5e3b1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "document = namedtuple(\n",
        "    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",
        ")\n",
        "\n",
        "train_set2 = [\n",
        "              ['Do you have plenty of time?', '?'],\n",
        "              ['Does she have enough money?','?'],\n",
        "              ['Did they have any useful advice?','?'],\n",
        "              ['What day is today?','?'],\n",
        "              [\"I don't have much time\",'-'],\n",
        "              [\"She doesn't have any money\",'-'],\n",
        "              [\"They didn't have any advice to offer\",'-'],\n",
        "              ['Have you plenty of time?','?'],\n",
        "              ['Has she enough money?','?'],\n",
        "              ['Had they any useful advice?','?'],\n",
        "              [\"I haven't much time\",'-'],\n",
        "              [\"She hasn't any money\",'-'],\n",
        "              [\"He hadn't any advice to offer\",'-'],\n",
        "              ['How are you?','?'],\n",
        "              ['How do you make questions in English?','?'],\n",
        "              ['How long have you lived here?','?'],\n",
        "              ['How often do you go to the cinema?','?'],\n",
        "              ['How much is this dress?','?'],\n",
        "              ['How old are you?','?'],\n",
        "              ['How many people came to the meeting?','?'],\n",
        "              ['I‚Äôm from France','+'],\n",
        "              ['I come from the UK','+'],\n",
        "              ['My phone number is 61709832145','+'],\n",
        "              ['I work as a tour guide for a local tour company','+'],\n",
        "              ['I‚Äôm not dating anyone','-'],\n",
        "              ['I live with my wife and children','+'],\n",
        "              ['I often do morning exercises at 6am','+'],\n",
        "              ['I run everyday','+'],\n",
        "              ['She walks very slowly','+'],\n",
        "              ['They eat a lot of meat daily','+'],\n",
        "              ['We were in France that day', '+'],\n",
        "              ['He speaks very fast', '+'],\n",
        "              ['They told us they came back early', '+'],\n",
        "              [\"I told her I'll be there\", '+']\n",
        "]\n",
        "train_set2 = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in train_set2]\n",
        "X_train2 = pd.DataFrame(data=train_set2)\n",
        "print(\"Documentos de entrenamiento:\")\n",
        "print(X_train2)\n",
        "\n",
        "test_set2 = [\n",
        "             ['Do you know who lives here?','?'],\n",
        "             ['What time is it?','?'],\n",
        "             ['Can you tell me where she comes from?','?'],\n",
        "             ['How are you?','?'],\n",
        "             ['I fill good today', '+'],\n",
        "             ['There is a lot of history here','+'],\n",
        "             ['I love programming','+'],\n",
        "             ['He told us not to make so much noise','+'],  # interesing case\n",
        "             ['We were asked not to park in front of the house','+'],  # interesing case\n",
        "             [\"I don't have much time\",'-'],\n",
        "             [\"She doesn't have any money\",'-'],\n",
        "             [\"They didn't have any advice to offer\",'-'],\n",
        "             ['I am not really sure','+']\n",
        "]\n",
        "test_set2 = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in test_set2]\n",
        "X_test2 = pd.DataFrame(data=test_set2)\n",
        "print(\"\\nDocumentos de prueba:\")\n",
        "print(X_test2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documentos de entrenamiento:\n",
            "                                                words class_\n",
            "0                (Do, you, have, plenty, of, time, ?)      ?\n",
            "1                 (Does, she, have, enough, money, ?)      ?\n",
            "2           (Did, they, have, any, useful, advice, ?)      ?\n",
            "3                           (What, day, is, today, ?)      ?\n",
            "4                      (I, do, n't, have, much, time)      -\n",
            "5                  (She, does, n't, have, any, money)      -\n",
            "6      (They, did, n't, have, any, advice, to, offer)      -\n",
            "7                    (Have, you, plenty, of, time, ?)      ?\n",
            "8                        (Has, she, enough, money, ?)      ?\n",
            "9                 (Had, they, any, useful, advice, ?)      ?\n",
            "10                         (I, have, n't, much, time)      -\n",
            "11                        (She, has, n't, any, money)      -\n",
            "12             (He, had, n't, any, advice, to, offer)      -\n",
            "13                                 (How, are, you, ?)      ?\n",
            "14    (How, do, you, make, questions, in, English, ?)      ?\n",
            "15             (How, long, have, you, lived, here, ?)      ?\n",
            "16      (How, often, do, you, go, to, the, cinema, ?)      ?\n",
            "17                    (How, much, is, this, dress, ?)      ?\n",
            "18                            (How, old, are, you, ?)      ?\n",
            "19     (How, many, people, came, to, the, meeting, ?)      ?\n",
            "20                            (I, ‚Äô, m, from, France)      +\n",
            "21                           (I, come, from, the, UK)      +\n",
            "22               (My, phone, number, is, 61709832145)      +\n",
            "23  (I, work, as, a, tour, guide, for, a, local, t...      +\n",
            "24                     (I, ‚Äô, m, not, dating, anyone)      -\n",
            "25           (I, live, with, my, wife, and, children)      +\n",
            "26        (I, often, do, morning, exercises, at, 6am)      +\n",
            "27                                 (I, run, everyday)      +\n",
            "28                         (She, walks, very, slowly)      +\n",
            "29               (They, eat, a, lot, of, meat, daily)      +\n",
            "30                  (We, were, in, France, that, day)      +\n",
            "31                           (He, speaks, very, fast)      +\n",
            "32          (They, told, us, they, came, back, early)      +\n",
            "33                  (I, told, her, I, 'll, be, there)      +\n",
            "\n",
            "Documentos de prueba:\n",
            "                                                words class_\n",
            "0                (Do, you, know, who, lives, here, ?)      ?\n",
            "1                             (What, time, is, it, ?)      ?\n",
            "2    (Can, you, tell, me, where, she, comes, from, ?)      ?\n",
            "3                                  (How, are, you, ?)      ?\n",
            "4                              (I, fill, good, today)      +\n",
            "5              (There, is, a, lot, of, history, here)      +\n",
            "6                              (I, love, programming)      +\n",
            "7      (He, told, us, not, to, make, so, much, noise)      +\n",
            "8   (We, were, asked, not, to, park, in, front, of...      +\n",
            "9                      (I, do, n't, have, much, time)      -\n",
            "10                 (She, does, n't, have, any, money)      -\n",
            "11     (They, did, n't, have, any, advice, to, offer)      -\n",
            "12                         (I, am, not, really, sure)      +\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wdp22w2ArUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e7e24f3-f0f2-40e6-b478-62b2ad3d09ad"
      },
      "source": [
        "# Ac√° probar√°n su clasificador y computaremos algunas m√©tricas de evaluacion\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# inicializamos el clasificador\n",
        "my_clf2 = MyMultinomialNB(alpha=1)\n",
        "\n",
        "# entrenamos el clasificador para los datos de entrenamiento X_train2\n",
        "my_clf2.fit(X_train2)\n",
        "\n",
        "# ac√° puedes ver el vocabulario extra√≠do por tu clasificador, \n",
        "# intenta tenerlo guardado en my_clf.vocab\n",
        "print('vocab: ', len(my_clf2.vocab), my_clf2.vocab)\n",
        "\n",
        "# si implementaron el m√©todo predict_proba en el clasificador (no era obligatorio), \n",
        "# ac√° lo pueden probar\n",
        "# print('\\nTest probs:')\n",
        "# print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test2)]))\n",
        "\n",
        "# obtengamos las predicciones para X_test2\n",
        "print('\\nTest predictions:')\n",
        "my_y_preds = my_clf2.predict(X_test2) # know no est√° en el dataframe o dict\n",
        "print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_y_preds, X_test2['words'])]))\n",
        "print(classification_report(y_true=X_test2['class_'], y_pred=my_y_preds, target_names=['?', '+', '-']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab:  109 {'‚Äô', 'Did', 'guide', 'What', 'go', 'exercises', 'today', 'us', '61709832145', 'is', 'were', 'I', 'number', 'her', 'dress', 'Has', 'from', 'do', 'day', 'and', 'plenty', 'came', 'come', 'How', 'everyday', 'meeting', 'children', 'long', 'the', 'walks', 'very', 'Does', 'are', 'has', '6am', 'often', 'that', 'cinema', 'be', 'of', 'm', 'any', 'lived', 'at', 'anyone', 'old', 'make', 'eat', 'Have', 'phone', \"'ll\", 'work', 'enough', 'a', 'she', 'not', 'English', 'my', 'live', 'dating', 'speaks', 'fast', 'We', 'as', 'morning', 'UK', 'daily', 'there', 'Had', 'with', 'lot', 'offer', 'in', 'My', 'much', 'many', 'She', 'questions', 'run', 'slowly', 'to', 'people', 'tour', 'meat', 'here', 'local', 'He', 'back', \"n't\", 'for', 'this', 'did', 'have', 'company', 'you', 'wife', 'They', 'time', 'Do', 'they', 'early', 'advice', 'France', 'money', 'does', 'useful', '?', 'had', 'told'}\n",
            "\n",
            "Test predictions:\n",
            "? <- Do you know who lives here ?\n",
            "? <- What time is it ?\n",
            "? <- Can you tell me where she comes from ?\n",
            "? <- How are you ?\n",
            "+ <- I fill good today\n",
            "+ <- There is a lot of history here\n",
            "+ <- I love programming\n",
            "- <- He told us not to make so much noise\n",
            "? <- We were asked not to park in front of the house\n",
            "- <- I do n't have much time\n",
            "- <- She does n't have any money\n",
            "- <- They did n't have any advice to offer\n",
            "+ <- I am not really sure\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           ?       1.00      0.67      0.80         6\n",
            "           +       0.75      1.00      0.86         3\n",
            "           -       0.80      1.00      0.89         4\n",
            "\n",
            "    accuracy                           0.85        13\n",
            "   macro avg       0.85      0.89      0.85        13\n",
            "weighted avg       0.88      0.85      0.84        13\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta aproximada:**\n",
        "\n",
        "**Nota:** No es necesario que obtenga exactamente los mismos resultados.\n",
        "\n",
        "```python\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           ?       1.00      0.67      0.80         6\n",
        "           +       0.75      1.00      0.86         3\n",
        "           -       0.80      1.00      0.89         4\n",
        "\n",
        "    accuracy                           0.85        13\n",
        "   macro avg       0.85      0.89      0.85        13\n",
        "weighted avg       0.88      0.85      0.84        13\n",
        "```"
      ],
      "metadata": {
        "id": "WXbg6sNTdAlO"
      }
    }
  ]
}