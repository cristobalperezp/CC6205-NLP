{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cristobalperezp/CC6205-NLP/blob/main/Tarea_4_enunciado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwaDuQqCOyLJ"
      },
      "source": [
        "# **Tarea 4 - CC6205 Natural Language Processing 📚**\n",
        "\n",
        "**Integrantes:**\n",
        "\n",
        "**Fecha límite de entrega 📆:** Martes 13 de junio.\n",
        "\n",
        "**Tiempo estimado de dedicación:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4lL5hGw07yP"
      },
      "source": [
        "Bienvenid@s a la cuarta tarea del curso de Natural Language Processing (NLP). \n",
        "En esta tarea estaremos tratando el problema de **tagging** (generación de secuencias de etiquetas del mismo largo que la secuencia de input), el uso de **Convolutional Neural Networks** y **Recurrent Neural Networks**, e implementaremos una red usando PyTorch. \n",
        "\n",
        "Usen $\\LaTeX$ para las fórmulas matemáticas. En la parte de programación pueden usar lo que quieran, pero la [Auxiliar 3](https://youtu.be/36WTXvg3zh0) les puede ser de *gran ayuda*.\n",
        "\n",
        "**Instrucciones:**\n",
        "- La tarea se realiza en grupos de **máximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a través de u-cursos a más tardar el día estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo Jupyter Notebook.\n",
        "- Al momento de la revisión tu código será ejecutado. Por favor verifica que tu entrega no tenga errores de compilación.\n",
        "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a través del canal de Discord del curso.\n",
        "\n",
        "Si aún no han visto las clases, se recomienda visitar los links de las referencias.\n",
        "\n",
        "**Referencias:**\n",
        "\n",
        "- [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/Tjgb-yQOg54), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)       \n",
        "- [MEMMs and CRFs](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-CRF.pdf): [notes 1](http://www.cs.columbia.edu/~mcollins/crf.pdf), [notes 2](http://www.cs.columbia.edu/~mcollins/fb.pdf), [video 1](https://youtu.be/qlI-4lSUDkg), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/ZpUwDy6o28Y)\n",
        "- [Convolutional Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-CNN.pdf): [video](https://youtu.be/lLZW5Fn40r8)\n",
        "- [Recurrent Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-RNN.pdf): [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMM) and Conditional Random Field(CRF) (1,5 puntos)"
      ],
      "metadata": {
        "id": "ANqzQ3G9WNw3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWXD3D7RYKJ-"
      },
      "source": [
        "### Pregunta 1 (1 pt)\n",
        "Para un problema de POS tagging se define el conjunto de etiquetas $S = \\{ \\text{DET}, \\text{NOUN}, \\text{VERB}, \\text{ADP} \\}$ y se tiene un Hidden Markov Model con los siguientes parámetros estimados a partir de un corpus de entrenamiento:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "q(\\text{NOUN}| \\text{ VERB}, \\text{DET}) &= 0.3 \\\\\n",
        "q(\\text{NOUN}|\\ w, \\text{DET}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n",
        "q(\\text{DET}| \\text{ VERB}, \\text{NOUN}) &= 0.4 \\\\\n",
        "q(\\text{DET}|\\ w, \\text{NOUN}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n",
        "e(the|\\text{ DET}) &= 0.5 \\\\\n",
        "e(pasta|\\text{ NOUN}) &= 0.6\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "Luego para la oración: `the man is pouring sauce on the pasta`, se tiene una tabla de programación dinámica con los siguientes valores:\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\pi(7,\\text{DET},\\text{DET})&=0.1\\\\\n",
        "\\pi(7,\\text{NOUN},\\text{DET})&=0.2\\\\\n",
        "\\pi(7,\\text{VERB},\\text{DET})&=0.01\\\\\n",
        "\\pi(7,\\text{ADP},\\text{DET})&=0.5\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "Con esta información, calcule el valor de $\\pi(8,\\text{DET},\\text{NOUN})$. Puede dejar el resultado expresado como una fracción.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EzgysW9kGi-"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "`Necesitamos calcular entonces:`\n",
        "\n",
        "$$\\pi(8,DET,NOUN) = \\max_{w \\in S_{k-2} }  \\pi(7,w,DET)*q(NOUN|w,DET)*e(pasta|NOUN) $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "con $S_{k-2} = \\{ DET,NOUN,VERB,ADP \\}$, luego debemos calcular cada combinación:\n"
      ],
      "metadata": {
        "id": "hLWkDPzFQlGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\pi(7,DET,DET)*q(NOUN|DET,DET)*e(pasta|NOUN) = 0.1*0*0.6=0$$\n",
        "\n",
        "$$\\pi(7,NOUN,DET)*q(NOUN|NOUN,DET)*e(pasta|NOUN) = 0.2*0*0.6=0$$\n",
        "\n",
        "$$\\pi(7,VERB,DET)*q(NOUN|VERB,DET)*e(pasta|NOUN) = 0.01*0.3*0.6=0.0018$$\n",
        "\n",
        "$$\\pi(7,ADP,DET)*q(NOUN|ADP,DET)*e(pasta|NOUN) = 0.5*0*0.6=0$$"
      ],
      "metadata": {
        "id": "m8V8qjCwQ2n1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, tomando máximo:\n",
        "\n",
        "$$\\pi(8,DET,NOUN) = 0.0018$$"
      ],
      "metadata": {
        "id": "VBVkBSXYSJ-O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiwJb_vmkKLZ"
      },
      "source": [
        "### Pregunta 2 (0.5 pts)\n",
        "Comente  sobre las similitudes o diferencias entre los HMMs, MEMMs y CRFs. Para esto, responda las siguientes preguntas.\n",
        "\n",
        "#### 2.1. ¿Para qué tipo de tarea sirven? Dé dos ejemplo de este tipo de tarea y descríbalos brevemente. (0.1 pts)\n",
        "\n",
        "**Respuesta:** \n",
        "\n",
        "`Estos modelos se utilizan para modelar y analizar secuencias, principalmente textos, resolviendo tareas de etiquetado y clasificación de secuencias, como POS tagging, que consiste en asignar una etiqueta gramatical a cada palabra en una oración, NER, dónde el objetivo es identificar y clasificar nombres de personas, organizaicones, lugares, etc. También en parsing, dónde se asigna una estructura sintáctica a una oración.`\n",
        "\n",
        "#### 2.2. ¿Qué modelos usan features? ¿Qué ventajas conlleva esto? (0.1 pts)\n",
        "\n",
        "**Respuesta:** \n",
        "\n",
        "`Los modelos que utilizan features son los MEMM's y CRF's, añadiendo caracerísticas inferibles adicionales, además de las secuencias de entrada, permitiendo mejorar la precisión de clasificación de secuencias. Entre las ventajas que conlleva esto, es obtener representaciones mucho más ricas qu en HMM's, además de poder incorporar conocimiento experto y reducir sensibilidad en las probabilidades de transición.`\n",
        "\n",
        "#### 2.3. ¿Cómo maneja cada uno de los modelos las palabras con baja frecuencia en el set de train? (0.1 pts)\n",
        "\n",
        "**Respuesta:** \n",
        "\n",
        "`Los HMM's pueden incorporar un suavizado de Laplace para asignar probabilidades a las palabras poco frecuentes, MEMM's al utilizar features adicionales, se puede incorporar selección de características o reducción de dimensionalidad. CRF's pueden incorporar regularización, penalizando características poco frecuentes.`\n",
        "\n",
        "#### 2.4. ¿Qué le permite a los CRF realizar decisiones globales? ¿Qué diferencia con respecto a los MEMMs permite lograr esto? ¿Por qué los HMMs tampoco son capaces de tomar decisiones globales? (0.1 pts)\n",
        "\n",
        "**Respuesta:** \n",
        "\n",
        "`Lo que le permite a CRF realizar decisiones globales es que estos consideran dependencias entre las etiquetas en toda la secuencia, la principal diferencia con MEMM's es que modelan directamente la distribución condicional de las etiquetas dada la secuencia completa de observaciones, lo que se aprecia en las normalizaciones utilizadas, es decir, el denominador de cada softmax utilizada. Los HMM's como asumen dependencia de primer o segundo orden, las etiquetas actuales dependen de una o dos anteriores, no pudiendo considerar información de la secuencia total.`\n",
        "\n",
        "#### 2.5 Dado una secuencia de $x_1, ..., x_m$ ¿Cuántas posibles secuencias de etiquetas se pueden generar para un conjunto de etiquetas $S$ con $|S|=k$ ? ¿Analizarlas todas sería computacionalmente tratable? (0.1 pts)\n",
        "\n",
        "**Respuesta:**\n",
        "\n",
        "`El número total de posibles secuencias de etiquetas que se pueden generar es k^m, para cada una de las m observaciones hay k opciones posibles de etiquetas. Computacionalmente se vuelve complejo analizar todas, si se tienen 10 etiquetas y una secuencia de largo 10, entonces hay 10^10 = 10.000.000.000 posibles secuencias, viendo que este número crece exponencialmente si la secuencia es cada vez más larga, complejizando el análisis de estas.`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks (0,5 puntos)"
      ],
      "metadata": {
        "id": "44ACHHZIWGF1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClRAHR95Y8aB"
      },
      "source": [
        "### Pregunta 3 (0,5 puntos)\n",
        "\n",
        "Considere la frase $w_{1..7}=$ `El agua moja y el fuego quema` $=[El, agua, moja, y, el, fuego, quema]$.\n",
        "\n",
        "La siguiente matriz de embeddings, donde la i-ésima fila corresponde al vector de embedding de la i-ésima palabra, ordenadas según aparecen en la frase. (vectores de largo 2).\n",
        "\\begin{equation}\n",
        "E = \\begin{pmatrix}\n",
        "2 & 2\\\\\n",
        "0 & -2\\\\\n",
        "0 & 1\\\\\n",
        "-2 & 1\\\\\n",
        "1 & 0\\\\\n",
        "-1 & 1\\\\\n",
        "1 & 1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Los siguientes 3 filtros\n",
        "\\begin{equation}\n",
        "U = \\begin{pmatrix}\n",
        "-1 & 1 & 0\\\\\n",
        "1 & 1 & 0\\\\\n",
        "0 & 0 & -1\\\\\n",
        "1 & -1 & -1\\\\\n",
        "-1 & -1 & 1\\\\\n",
        "1 & 0 & -1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Y la función de activación\n",
        "\\begin{equation}\n",
        "tanh = \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
        "\\end{equation}\n",
        "\n",
        "Usando estos paramátros escriba los pasos para calcular la representación (vector) resultante de aplicar la operación de convolución (sin padding) + max pooling. ¿De qué tamaño sería la ventana que debemos usar?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlQ30Arkq0u4"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "`Escriba su respuesta aquí`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks (1 punto)\n"
      ],
      "metadata": {
        "id": "A0rCwen3WREC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0et78Z4oKIq"
      },
      "source": [
        "### Pregunta 4 (0,5 puntos)\n",
        "Usando los embeddings de dos dimensiones de la pregunta anteror, la oración `el fuego quema` la podemos representar por una secuencia de vectores $(\\vec{x}_1,\\vec{x}_2,\\vec{x}_3)$, con $\\vec{x}_i \\in \\mathbb{R}^{d_x}$ y $d_x=2$.\n",
        "\n",
        "Tenemos una red recurrente *Elman* definidad como: \n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\vec{s}_i &= R_{SRNN}\\left (\\vec{x}_i, \\vec{s}_{i-1}\\right ) = g \\left (\\vec{s}_{i-1}W^s + \\vec{x}_i W^x + \\vec{b}\\right ) \\\\\n",
        "\\vec{y}_i &= O_{SRNN}\\left(\\vec{s}_i\\right) = \\vec{s}_i \\\\\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "donde\n",
        "\\begin{equation}\n",
        "\\vec{s}_i, \\vec{y}_i \\in \\mathbb{R}^{d_s}, \\quad W^x \\in \\mathbb{R}^{d_x \\times d_s}, \\quad W^s \\in \\mathbb{R}^{d_s \\times d_s}, \\quad \\vec{b} \\in \\mathbb{R}^{d_s},\n",
        "\\end{equation}\n",
        "y los vectores de estado $s_i$ son de tres dimensiones, $ds= 3$.\n",
        "\n",
        "Sea\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\vec{s}_0 &= [0,0,0]\\\\\n",
        "W^x &= \\begin{pmatrix}\n",
        "0 &  0 & 1\\\\\n",
        "1 & -1 & 0\n",
        "\\end{pmatrix} \\\\\n",
        "W^s &= \\begin{pmatrix}\n",
        "1 & 0 &  1\\\\\n",
        "0 & 1 & -1\\\\\n",
        "1 & 1 &  1\n",
        "\\end{pmatrix} \\\\\n",
        "\\vec{b} &= [0, 0, 0] \\\\\n",
        "g(x) &= ReLu(x) = max(0, x)\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "<br>\n",
        "\n",
        "Calcule manualmente los valores de los vectores $\\vec{s}_1, \\vec{s}_2,\\vec{s}_3$ y de $\\vec{y}_1, \\vec{y}_2,\\vec{y}_3$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fim2W8JioPhL"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "`Escriba su respuesta aquí`\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4rAT6ELxRZW"
      },
      "source": [
        "### Pregunta 5 (0.5 puntos)\n",
        "¿De qué forma las RNN y las CNN logran aprender representaciones específicas\n",
        "para la tarea objetivo? Compare la forma en que las RNN y las CNN aprenden con los modelos que usan *features* diseñadas manualmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6AXbQSgA_t8"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "`Escriba su respuesta aquí`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pregunta 6: Redes Neuronales con Pytorch (3 puntos) 💬\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.anda.cl/wp-content/uploads/2021/03/0_5vNAtimPjYQr4W72.gif\" alt=\"chatbot\" width=\"400\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "FRJkBpjWyHnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta sección de la tarea deberán implementar un Chatbot que sea capaz de generar una conversación *“básica”* utilizando un dataset de *Star Wars*. **El objetivo** de esta pregunta es que puedan aplicar lo aprendido sobre redes neuronales utilizando Pytorch en un ejemplo práctico.  Durante el desarrollo, se espera que puedan diseñar un bot (que tendrá por atrás un clasificador) que sea capaz de clasificar diferentes etiquetas, cosa que una vez identificada la etiqueta entregue una respuesta acorde a lo preguntado.\n",
        "\n",
        "**Aviso:** Antes de comenzar con una descripción mas profunda de esta sección, les recomendamos que visualicen y se familiaricen con el dataset entregado, de esta forma comprenderán mejor la descripción del enunciado (aquí una pequeña ayudita 🆘)."
      ],
      "metadata": {
        "id": "GEla92bUymrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "example_data = pd.read_json('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/star_wars_chatbot.json')\n",
        "print(\"Cantidad de tags: \", example_data['intents'].shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eKOGlMs3Dx-",
        "outputId": "0c8737e6-e9a5-4f18-bde0-f2b5c0b7c8f4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de tags:  16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, ejemplos del contenido del primer registro:"
      ],
      "metadata": {
        "id": "V-6fCE5fHkNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_data['intents'][0]['patterns']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axsi27BpHGOx",
        "outputId": "d98093dc-6e6d-45fc-a956-e29200748c70"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi',\n",
              " 'Hey',\n",
              " 'How are you',\n",
              " 'Is anyone there?',\n",
              " 'Hello',\n",
              " 'Good day',\n",
              " \"What's up\",\n",
              " 'Yo!',\n",
              " 'Howdy',\n",
              " 'Nice to meet you.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_data['intents'][0]['responses']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV0vGdwoHeg3",
        "outputId": "06bc9a06-932e-4df2-e7bd-6d5de461ecb9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hey',\n",
              " 'Hello, thanks for visiting.',\n",
              " 'Hi there, what can I do for you?',\n",
              " 'Hi there, how can I help?',\n",
              " 'Hello, there.',\n",
              " 'Hello Dear',\n",
              " 'Ooooo Hello, looking for someone or something?',\n",
              " 'Yes, I am here.',\n",
              " 'Listening carefully.',\n",
              " 'Ok, I am with you.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_data['intents'][0]['tag']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0BnYez1oGtx3",
        "outputId": "486ac8dc-e157-45f3-9321-29edd28d6bfa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'greeting'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Del dataset cargado podemos notar que este viene en un formato `JSON`, por lo que sus datos están almacenados en diccionarios. Las llaves de los diccionarios no son aleatorias y estos nos sirven para identificar puntos relevantes en el desarrollo del bot. A continuación, se realiza una pequeña descripción de las llaves:\n",
        "\n",
        "- `patterns`: Almacena los patrones con los que entrenaremos el modelo 😮, en otras palabras, es el corpus de entrenamiento que contiene solo preguntas o expresiones que deberá responder el bot.\n",
        "- `responses`: Son las respuestas 🙋 relacionadas a los `patterns`, estas las utilizaremos en una etapa posterior a la clasificación, para dar una respuesta aleatoría al usuario.\n",
        "- `tag`: Son las labels con las que entrenaremos nuestro modelo 💻. \n",
        "\n",
        "En síntesis, las `keys` relevantes para el entrenamiento de nuestra red neuronal serán `patterns` (corpus) y `tag` (etiquetas)."
      ],
      "metadata": {
        "id": "v6BvAWCw3zPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explicación de la tarea a realizar:"
      ],
      "metadata": {
        "id": "KlOAdMjSSzNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicación de la tarea a realizar**: Implemente una Class llamada `CNNClassifier` que sea capaz de entrenar un modelo de texto a través de una red neuronal Feed Forward y una arquitectura convolucional (CNN 1D) [`torch.nn.Conv1d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#conv1d) . Para el diseño de las redes tienen completa libertad, pero se le aconseja que se guíen de la última auxiliar para la construcción. Es **importantísimo** que el modelo a crear posea una capa de `Embedding` que se genere en base al entrenamiento del modelo. Creado el modelo, construya una función batch para cargar los datos de entrenamiento del modelo.\n",
        "\n",
        "Construido el modelo, compare los resultados obtenidos para una red feed forward y una cnn. Para la comprobación de sus resultados ejecute el chatbot y pruebelo, ¿qué configuración tiene mejores resultados?, ¿a qué se deberan estos resultados?\n",
        "\n",
        "Ojo que un ejemplo de prueba con el chatbot puede ser (agregue mas preguntas ud):\n",
        "\n",
        "```\n",
        "Let's chat! (type 'finish_chat' to finish the chat)\n",
        "You: hi\n",
        "GA-97: Yes, I am here.\n",
        "You: can you tell me a joke?\n",
        "GA-97: Have you tried the gluten-free Wookiee treats? No, but I heard they are a little Chewy.\n",
        "```\n",
        "\n",
        "El resto del código referido a la ejecución del chat se los entregamos, por lo que no deberían tener mayores problemas 😸 (en caso de tener problemas con su código, puede modificar cualquier parte sugerida siempre y cuando cumpla lo solicitado).\n",
        "\n",
        "**Igual [mucho texto](https://i0.wp.com/elgeneracionalpost.com/wp-content/uploads/2020/07/mucho-texto.jpg?fit=1280%2C720&ssl=1).... En resumen, ¿Qué se solicita?:**\n",
        "\n",
        "- [ ] Diseñar una red neuronal Feed Forward.\n",
        "- [ ] Diseñar un red convolucional.\n",
        "- [ ] Utilizar una capa de embeddings para generar representaciones vectoriales del corpus.\n",
        "- [ ] Crear el método forward de la clase `CNNClassifier`.\n",
        "- [ ] Crear la función BATCH.\n",
        "- [ ] Probar el modelo y comparar los resultados obtenidos con la red Feed Forward y la red CNN. Comente sus resultados de forma cualitativa, señalando con qué tipo de red obtuvo mejores resultados con el chatbot.\n",
        "\n",
        "**Nota-1:** El modelo creado debe tener la opción de entrenar a traves de una feed forward y una CNN. Esto no significa que entrenará una FF y una CN, el modelo deberá recibir un booleano que especifique que tipo de red utilizará.\n",
        "\n",
        "**Nota-2:** El dataset se descargará automáticamente en la sección `Carga de Dataset 📚`, no os preocupéis."
      ],
      "metadata": {
        "id": "9yGApnWVI4cO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pasemos al Código 🦾\n",
        "\n",
        "Esqueleto propuesto (se **RECOMIENDA** que cambien **SOLO** la red neuronal y la función Batch) 🦴:"
      ],
      "metadata": {
        "id": "a4bKfAdEy3oD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Instalamos librerias necesarias e importamos 😀"
      ],
      "metadata": {
        "id": "RUwxivx2MpMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esto toma su tiempo en ejecutarse\n",
        "%%capture\n",
        "!pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torchtext==0.9.0"
      ],
      "metadata": {
        "id": "TjSZkBsk1H4f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "from random import choice\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torch.optim import SGD, lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from itertools import zip_longest\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "RfZ6SL-Q1Kwd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Carga de Dataset 📚"
      ],
      "metadata": {
        "id": "oj-Epe7XJLrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we obtain the dataset\n",
        "!wget 'https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/star_wars_chatbot.json'"
      ],
      "metadata": {
        "id": "hvlLqYRrVN6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset using json\n",
        "with open('star_wars_chatbot.json', 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Create a vocab with the dataset and get the number of classes that have\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "vocab = build_vocab_from_iterator(tokenizer(x) for list_words in dataset['intents'] for x in list_words['patterns'])\n",
        "num_classes = len(dataset['intents'])\n",
        "\n",
        "# Define a list with the labels\n",
        "labels = sorted(set([tag for tag in [intents['tag'] for intents in dataset['intents']]]))\n",
        "# Define a train_list where we can find the info in the format: [(tag_0, text_0)...,(tag_n-1, text_n-1)]\n",
        "train_list = [(labels.index(intents['tag']), text) for intents in dataset['intents'] for text in intents['patterns']]"
      ],
      "metadata": {
        "id": "MbbIsFUG1TXW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Default index y pad\n",
        "vocab.set_default_index(0)\n",
        "vocab.insert_token('<pad>', 1)\n",
        "# stoi\n",
        "stoi = vocab.get_stoi()"
      ],
      "metadata": {
        "id": "Q_M7w1zQnIwF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Creación del modelo (2 puntos en total)"
      ],
      "metadata": {
        "id": "a52SUNKPJQxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construya el modelo\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=32, num_classes=10, \n",
        "                 use_cnn=False, cnn_pool_channels=24, cnn_kernel_size=3):\n",
        "      super().__init__()\n",
        "      \n",
        "      self.use_cnn = use_cnn\n",
        "\n",
        "      # Creamos la capa de embedding\n",
        "      self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "      if self.use_cnn:\n",
        "        # Creamos la capa de convolución\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_channels=1,\n",
        "            out_channels=cnn_pool_channels,\n",
        "            kernel_size=cnn_kernel_size * embed_dim,\n",
        "            stride=embed_dim,\n",
        "        )\n",
        "\n",
        "        # Creamos la capa lineal\n",
        "        self.fc = nn.Linear(cnn_pool_channels, num_classes)\n",
        "      \n",
        "      else:\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "      \n",
        "      # Inicializamos los pesos de las capas\n",
        "      self.init_weights()\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "      # Definimos el rango de los valores iniciales de los pesos\n",
        "      initrange = 0.5\n",
        "\n",
        "      # Inicializamos los pesos de la capa de embedding\n",
        "      self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "      # Inicializamos los pesos de la capa lineal\n",
        "      self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "      # Inicializamos los sesgos de la capa lineal en cero\n",
        "      self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "      # Preparamos el input de la capa de embeddings a partir de text y offsets\n",
        "      text = torch.tensor(\n",
        "          list(\n",
        "              zip(\n",
        "                  *zip_longest(\n",
        "                      *([text[o:offsets[i+1]] for i, o in enumerate(offsets[:-1])] + [text[offsets[-1]:len(texts)]]), \n",
        "                      fillvalue=vocab[\"<pad>\"]\n",
        "                      )\n",
        "                  )\n",
        "              )\n",
        "          ).to(text.device)\n",
        "      \n",
        "      # Obtenemos la representación de la frase a partir de la capa de embedding\n",
        "      h = self.embedding(text)\n",
        "\n",
        "      if self.use_cnn:\n",
        "        # Aplicamos la capa de convolución\n",
        "        h = h.view(h.size(0), 1, -1)\n",
        "        h = torch.relu(self.conv(h))\n",
        "        h = h.mean(dim=2)\n",
        "      \n",
        "      else:\n",
        "        h = h.mean(dim=1)\n",
        "      \n",
        "\n",
        "      # Obtenemos el resultado final a partir de la capa lineal\n",
        "      output = self.fc(h)\n",
        "\n",
        "      return output"
      ],
      "metadata": {
        "id": "n-vQ24tMJG5H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Función Batch 👷 (0,5 puntos)"
      ],
      "metadata": {
        "id": "dGN-T0JoJtmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batch(batch):\n",
        "  label = torch.tensor([entry[0] for entry in batch])\n",
        "  texts = [tokenizer(entry[1]) for entry in batch]\n",
        "  offsets = [0] + [len(text) for text in texts]\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  big_text = torch.cat([torch.tensor([vocab[t] if t in stoi else 0 for t in text]) for text in texts])\n",
        "  return big_text, offsets, label\n"
      ],
      "metadata": {
        "id": "iAVpXX7ykhsC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Entrenamiento 🥊"
      ],
      "metadata": {
        "id": "YChwpNrrNRBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"GPU is avaible: {device}\")\n",
        "\n",
        "# Define the different inputs in our model\n",
        "num_epochs = 1000\n",
        "BATCH_SIZE = 16\n",
        "LR = 1e-1\n",
        "INPUT_SIZE = len(vocab)\n",
        "OUTPUT_SIZE = num_classes\n",
        "USE_CNN = False\n",
        "\n",
        "# Define model, optimizer, loss and scheduler (Q: ¿What is it?)\n",
        "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=USE_CNN).to(device)\n",
        "optimizer = SGD(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda epoch: .9 ** (epoch // 10)])\n",
        "\n",
        "print(f'train: {len(train_list)} elements')\n",
        "\n",
        "# We train the model using the intents\n",
        "loss_list= []\n",
        "for epoch in range(1, num_epochs):\n",
        "  train_loader = DataLoader(train_list, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for i, (texts, offsets, cls) in enumerate(train_loader):\n",
        "    texts = texts.to(device)\n",
        "    offsets = offsets.to(device)\n",
        "    cls = cls.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(texts, offsets)\n",
        "    loss = criterion(output, cls)\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  loss_list.append(loss.item())\n",
        "  sys.stdout.write('\\rEpoch: {0:03d} \\t iter-Loss: {1:.3f}\\n'.format(epoch+1, loss.item()))\n",
        "\n",
        "print(f'final loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5eRWRD_J0Km",
        "outputId": "7e725bc6-ff73-4647-8f72-04ce2534cf7b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is avaible: cuda\n",
            "train: 97 elements\n",
            "\rEpoch: 002 \t iter-Loss: 2.688\n",
            "\rEpoch: 003 \t iter-Loss: 2.563\n",
            "\rEpoch: 004 \t iter-Loss: 2.442\n",
            "\rEpoch: 005 \t iter-Loss: 2.325\n",
            "\rEpoch: 006 \t iter-Loss: 2.210\n",
            "\rEpoch: 007 \t iter-Loss: 2.100\n",
            "\rEpoch: 008 \t iter-Loss: 1.992\n",
            "\rEpoch: 009 \t iter-Loss: 1.888\n",
            "\rEpoch: 010 \t iter-Loss: 1.787\n",
            "\rEpoch: 011 \t iter-Loss: 1.689\n",
            "Epoch: 012 \t iter-Loss: 1.594\n",
            "Epoch: 013 \t iter-Loss: 1.502\n",
            "Epoch: 014 \t iter-Loss: 1.414\n",
            "Epoch: 015 \t iter-Loss: 1.330\n",
            "Epoch: 016 \t iter-Loss: 1.249\n",
            "Epoch: 017 \t iter-Loss: 1.172\n",
            "Epoch: 018 \t iter-Loss: 1.099\n",
            "Epoch: 019 \t iter-Loss: 1.029\n",
            "Epoch: 020 \t iter-Loss: 0.964\n",
            "Epoch: 021 \t iter-Loss: 0.903\n",
            "Epoch: 022 \t iter-Loss: 0.845\n",
            "Epoch: 023 \t iter-Loss: 0.791\n",
            "Epoch: 024 \t iter-Loss: 0.742\n",
            "Epoch: 025 \t iter-Loss: 0.695\n",
            "Epoch: 026 \t iter-Loss: 0.652\n",
            "Epoch: 027 \t iter-Loss: 0.613\n",
            "Epoch: 028 \t iter-Loss: 0.576\n",
            "Epoch: 029 \t iter-Loss: 0.542\n",
            "Epoch: 030 \t iter-Loss: 0.511\n",
            "Epoch: 031 \t iter-Loss: 0.483\n",
            "Epoch: 032 \t iter-Loss: 0.456\n",
            "Epoch: 033 \t iter-Loss: 0.432\n",
            "Epoch: 034 \t iter-Loss: 0.410\n",
            "Epoch: 035 \t iter-Loss: 0.389\n",
            "Epoch: 036 \t iter-Loss: 0.370\n",
            "Epoch: 037 \t iter-Loss: 0.352\n",
            "Epoch: 038 \t iter-Loss: 0.336\n",
            "Epoch: 039 \t iter-Loss: 0.321\n",
            "Epoch: 040 \t iter-Loss: 0.307\n",
            "Epoch: 041 \t iter-Loss: 0.294\n",
            "Epoch: 042 \t iter-Loss: 0.282\n",
            "Epoch: 043 \t iter-Loss: 0.271\n",
            "Epoch: 044 \t iter-Loss: 0.260\n",
            "Epoch: 045 \t iter-Loss: 0.250\n",
            "Epoch: 046 \t iter-Loss: 0.241\n",
            "Epoch: 047 \t iter-Loss: 0.232\n",
            "Epoch: 048 \t iter-Loss: 0.224\n",
            "Epoch: 049 \t iter-Loss: 0.217\n",
            "Epoch: 050 \t iter-Loss: 0.210\n",
            "Epoch: 051 \t iter-Loss: 0.203\n",
            "Epoch: 052 \t iter-Loss: 0.196\n",
            "Epoch: 053 \t iter-Loss: 0.190\n",
            "Epoch: 054 \t iter-Loss: 0.185\n",
            "Epoch: 055 \t iter-Loss: 0.179\n",
            "Epoch: 056 \t iter-Loss: 0.174\n",
            "Epoch: 057 \t iter-Loss: 0.169\n",
            "Epoch: 058 \t iter-Loss: 0.165\n",
            "Epoch: 059 \t iter-Loss: 0.160\n",
            "Epoch: 060 \t iter-Loss: 0.156\n",
            "Epoch: 061 \t iter-Loss: 0.152\n",
            "Epoch: 062 \t iter-Loss: 0.148\n",
            "Epoch: 063 \t iter-Loss: 0.145\n",
            "Epoch: 064 \t iter-Loss: 0.141\n",
            "Epoch: 065 \t iter-Loss: 0.138\n",
            "Epoch: 066 \t iter-Loss: 0.135\n",
            "Epoch: 067 \t iter-Loss: 0.132\n",
            "Epoch: 068 \t iter-Loss: 0.129\n",
            "Epoch: 069 \t iter-Loss: 0.126\n",
            "Epoch: 070 \t iter-Loss: 0.123\n",
            "Epoch: 071 \t iter-Loss: 0.121\n",
            "Epoch: 072 \t iter-Loss: 0.118\n",
            "Epoch: 073 \t iter-Loss: 0.116\n",
            "Epoch: 074 \t iter-Loss: 0.113\n",
            "Epoch: 075 \t iter-Loss: 0.111\n",
            "Epoch: 076 \t iter-Loss: 0.109\n",
            "Epoch: 077 \t iter-Loss: 0.107\n",
            "Epoch: 078 \t iter-Loss: 0.105\n",
            "Epoch: 079 \t iter-Loss: 0.103\n",
            "Epoch: 080 \t iter-Loss: 0.101\n",
            "Epoch: 081 \t iter-Loss: 0.099\n",
            "Epoch: 082 \t iter-Loss: 0.097\n",
            "Epoch: 083 \t iter-Loss: 0.095\n",
            "Epoch: 084 \t iter-Loss: 0.094\n",
            "Epoch: 085 \t iter-Loss: 0.092\n",
            "Epoch: 086 \t iter-Loss: 0.091\n",
            "Epoch: 087 \t iter-Loss: 0.089\n",
            "Epoch: 088 \t iter-Loss: 0.087\n",
            "Epoch: 089 \t iter-Loss: 0.086\n",
            "Epoch: 090 \t iter-Loss: 0.085\n",
            "Epoch: 091 \t iter-Loss: 0.083\n",
            "Epoch: 092 \t iter-Loss: 0.082\n",
            "Epoch: 093 \t iter-Loss: 0.081\n",
            "Epoch: 094 \t iter-Loss: 0.079\n",
            "Epoch: 095 \t iter-Loss: 0.078\n",
            "Epoch: 096 \t iter-Loss: 0.077\n",
            "Epoch: 097 \t iter-Loss: 0.076\n",
            "Epoch: 098 \t iter-Loss: 0.075\n",
            "Epoch: 099 \t iter-Loss: 0.074\n",
            "Epoch: 100 \t iter-Loss: 0.072\n",
            "Epoch: 101 \t iter-Loss: 0.071\n",
            "Epoch: 102 \t iter-Loss: 0.070\n",
            "Epoch: 103 \t iter-Loss: 0.069\n",
            "Epoch: 104 \t iter-Loss: 0.068\n",
            "Epoch: 105 \t iter-Loss: 0.067\n",
            "Epoch: 106 \t iter-Loss: 0.066\n",
            "Epoch: 107 \t iter-Loss: 0.066\n",
            "Epoch: 108 \t iter-Loss: 0.065\n",
            "Epoch: 109 \t iter-Loss: 0.064\n",
            "Epoch: 110 \t iter-Loss: 0.063\n",
            "Epoch: 111 \t iter-Loss: 0.062\n",
            "Epoch: 112 \t iter-Loss: 0.061\n",
            "Epoch: 113 \t iter-Loss: 0.060\n",
            "Epoch: 114 \t iter-Loss: 0.060\n",
            "Epoch: 115 \t iter-Loss: 0.059\n",
            "Epoch: 116 \t iter-Loss: 0.058\n",
            "Epoch: 117 \t iter-Loss: 0.057\n",
            "Epoch: 118 \t iter-Loss: 0.057\n",
            "Epoch: 119 \t iter-Loss: 0.056\n",
            "Epoch: 120 \t iter-Loss: 0.055\n",
            "Epoch: 121 \t iter-Loss: 0.055\n",
            "Epoch: 122 \t iter-Loss: 0.054\n",
            "Epoch: 123 \t iter-Loss: 0.053\n",
            "Epoch: 124 \t iter-Loss: 0.053\n",
            "Epoch: 125 \t iter-Loss: 0.052\n",
            "Epoch: 126 \t iter-Loss: 0.051\n",
            "Epoch: 127 \t iter-Loss: 0.051\n",
            "Epoch: 128 \t iter-Loss: 0.050\n",
            "Epoch: 129 \t iter-Loss: 0.049\n",
            "Epoch: 130 \t iter-Loss: 0.049\n",
            "Epoch: 131 \t iter-Loss: 0.048\n",
            "Epoch: 132 \t iter-Loss: 0.048\n",
            "Epoch: 133 \t iter-Loss: 0.047\n",
            "Epoch: 134 \t iter-Loss: 0.047\n",
            "Epoch: 135 \t iter-Loss: 0.046\n",
            "Epoch: 136 \t iter-Loss: 0.046\n",
            "Epoch: 137 \t iter-Loss: 0.045\n",
            "Epoch: 138 \t iter-Loss: 0.045\n",
            "Epoch: 139 \t iter-Loss: 0.044\n",
            "Epoch: 140 \t iter-Loss: 0.044\n",
            "Epoch: 141 \t iter-Loss: 0.043\n",
            "Epoch: 142 \t iter-Loss: 0.043\n",
            "Epoch: 143 \t iter-Loss: 0.042\n",
            "Epoch: 144 \t iter-Loss: 0.042\n",
            "Epoch: 145 \t iter-Loss: 0.041\n",
            "Epoch: 146 \t iter-Loss: 0.041\n",
            "Epoch: 147 \t iter-Loss: 0.041\n",
            "Epoch: 148 \t iter-Loss: 0.040\n",
            "Epoch: 149 \t iter-Loss: 0.040\n",
            "Epoch: 150 \t iter-Loss: 0.039\n",
            "Epoch: 151 \t iter-Loss: 0.039\n",
            "Epoch: 152 \t iter-Loss: 0.038\n",
            "Epoch: 153 \t iter-Loss: 0.038\n",
            "Epoch: 154 \t iter-Loss: 0.038\n",
            "Epoch: 155 \t iter-Loss: 0.037\n",
            "Epoch: 156 \t iter-Loss: 0.037\n",
            "Epoch: 157 \t iter-Loss: 0.037\n",
            "Epoch: 158 \t iter-Loss: 0.036\n",
            "Epoch: 159 \t iter-Loss: 0.036\n",
            "Epoch: 160 \t iter-Loss: 0.036\n",
            "Epoch: 161 \t iter-Loss: 0.035\n",
            "Epoch: 162 \t iter-Loss: 0.035\n",
            "Epoch: 163 \t iter-Loss: 0.035\n",
            "Epoch: 164 \t iter-Loss: 0.034\n",
            "Epoch: 165 \t iter-Loss: 0.034\n",
            "Epoch: 166 \t iter-Loss: 0.034\n",
            "Epoch: 167 \t iter-Loss: 0.033\n",
            "Epoch: 168 \t iter-Loss: 0.033\n",
            "Epoch: 169 \t iter-Loss: 0.033\n",
            "Epoch: 170 \t iter-Loss: 0.032\n",
            "Epoch: 171 \t iter-Loss: 0.032\n",
            "Epoch: 172 \t iter-Loss: 0.032\n",
            "Epoch: 173 \t iter-Loss: 0.031\n",
            "Epoch: 174 \t iter-Loss: 0.031\n",
            "Epoch: 175 \t iter-Loss: 0.031\n",
            "Epoch: 176 \t iter-Loss: 0.031\n",
            "Epoch: 177 \t iter-Loss: 0.030\n",
            "Epoch: 178 \t iter-Loss: 0.030\n",
            "Epoch: 179 \t iter-Loss: 0.030\n",
            "Epoch: 180 \t iter-Loss: 0.030\n",
            "Epoch: 181 \t iter-Loss: 0.029\n",
            "Epoch: 182 \t iter-Loss: 0.029\n",
            "Epoch: 183 \t iter-Loss: 0.029\n",
            "Epoch: 184 \t iter-Loss: 0.029\n",
            "Epoch: 185 \t iter-Loss: 0.028\n",
            "Epoch: 186 \t iter-Loss: 0.028\n",
            "Epoch: 187 \t iter-Loss: 0.028\n",
            "Epoch: 188 \t iter-Loss: 0.028\n",
            "Epoch: 189 \t iter-Loss: 0.027\n",
            "Epoch: 190 \t iter-Loss: 0.027\n",
            "Epoch: 191 \t iter-Loss: 0.027\n",
            "Epoch: 192 \t iter-Loss: 0.027\n",
            "Epoch: 193 \t iter-Loss: 0.026\n",
            "Epoch: 194 \t iter-Loss: 0.026\n",
            "Epoch: 195 \t iter-Loss: 0.026\n",
            "Epoch: 196 \t iter-Loss: 0.026\n",
            "Epoch: 197 \t iter-Loss: 0.026\n",
            "Epoch: 198 \t iter-Loss: 0.025\n",
            "Epoch: 199 \t iter-Loss: 0.025\n",
            "Epoch: 200 \t iter-Loss: 0.025\n",
            "Epoch: 201 \t iter-Loss: 0.025\n",
            "Epoch: 202 \t iter-Loss: 0.025\n",
            "Epoch: 203 \t iter-Loss: 0.024\n",
            "Epoch: 204 \t iter-Loss: 0.024\n",
            "Epoch: 205 \t iter-Loss: 0.024\n",
            "Epoch: 206 \t iter-Loss: 0.024\n",
            "Epoch: 207 \t iter-Loss: 0.024\n",
            "Epoch: 208 \t iter-Loss: 0.023\n",
            "Epoch: 209 \t iter-Loss: 0.023\n",
            "Epoch: 210 \t iter-Loss: 0.023\n",
            "Epoch: 211 \t iter-Loss: 0.023\n",
            "Epoch: 212 \t iter-Loss: 0.023\n",
            "Epoch: 213 \t iter-Loss: 0.023\n",
            "Epoch: 214 \t iter-Loss: 0.022\n",
            "Epoch: 215 \t iter-Loss: 0.022\n",
            "Epoch: 216 \t iter-Loss: 0.022\n",
            "Epoch: 217 \t iter-Loss: 0.022\n",
            "Epoch: 218 \t iter-Loss: 0.022\n",
            "Epoch: 219 \t iter-Loss: 0.022\n",
            "Epoch: 220 \t iter-Loss: 0.021\n",
            "Epoch: 221 \t iter-Loss: 0.021\n",
            "Epoch: 222 \t iter-Loss: 0.021\n",
            "Epoch: 223 \t iter-Loss: 0.021\n",
            "Epoch: 224 \t iter-Loss: 0.021\n",
            "Epoch: 225 \t iter-Loss: 0.021\n",
            "Epoch: 226 \t iter-Loss: 0.020\n",
            "Epoch: 227 \t iter-Loss: 0.020\n",
            "Epoch: 228 \t iter-Loss: 0.020\n",
            "Epoch: 229 \t iter-Loss: 0.020\n",
            "Epoch: 230 \t iter-Loss: 0.020\n",
            "Epoch: 231 \t iter-Loss: 0.020\n",
            "Epoch: 232 \t iter-Loss: 0.020\n",
            "Epoch: 233 \t iter-Loss: 0.019\n",
            "Epoch: 234 \t iter-Loss: 0.019\n",
            "Epoch: 235 \t iter-Loss: 0.019\n",
            "Epoch: 236 \t iter-Loss: 0.019\n",
            "Epoch: 237 \t iter-Loss: 0.019\n",
            "Epoch: 238 \t iter-Loss: 0.019\n",
            "Epoch: 239 \t iter-Loss: 0.019\n",
            "Epoch: 240 \t iter-Loss: 0.018\n",
            "Epoch: 241 \t iter-Loss: 0.018\n",
            "Epoch: 242 \t iter-Loss: 0.018\n",
            "Epoch: 243 \t iter-Loss: 0.018\n",
            "Epoch: 244 \t iter-Loss: 0.018\n",
            "Epoch: 245 \t iter-Loss: 0.018\n",
            "Epoch: 246 \t iter-Loss: 0.018\n",
            "Epoch: 247 \t iter-Loss: 0.018\n",
            "Epoch: 248 \t iter-Loss: 0.017\n",
            "Epoch: 249 \t iter-Loss: 0.017\n",
            "Epoch: 250 \t iter-Loss: 0.017\n",
            "Epoch: 251 \t iter-Loss: 0.017\n",
            "Epoch: 252 \t iter-Loss: 0.017\n",
            "Epoch: 253 \t iter-Loss: 0.017\n",
            "Epoch: 254 \t iter-Loss: 0.017\n",
            "Epoch: 255 \t iter-Loss: 0.017\n",
            "Epoch: 256 \t iter-Loss: 0.017\n",
            "Epoch: 257 \t iter-Loss: 0.016\n",
            "Epoch: 258 \t iter-Loss: 0.016\n",
            "Epoch: 259 \t iter-Loss: 0.016\n",
            "Epoch: 260 \t iter-Loss: 0.016\n",
            "Epoch: 261 \t iter-Loss: 0.016\n",
            "Epoch: 262 \t iter-Loss: 0.016\n",
            "Epoch: 263 \t iter-Loss: 0.016\n",
            "Epoch: 264 \t iter-Loss: 0.016\n",
            "Epoch: 265 \t iter-Loss: 0.016\n",
            "Epoch: 266 \t iter-Loss: 0.015\n",
            "Epoch: 267 \t iter-Loss: 0.015\n",
            "Epoch: 268 \t iter-Loss: 0.015\n",
            "Epoch: 269 \t iter-Loss: 0.015\n",
            "Epoch: 270 \t iter-Loss: 0.015\n",
            "Epoch: 271 \t iter-Loss: 0.015\n",
            "Epoch: 272 \t iter-Loss: 0.015\n",
            "Epoch: 273 \t iter-Loss: 0.015\n",
            "Epoch: 274 \t iter-Loss: 0.015\n",
            "Epoch: 275 \t iter-Loss: 0.015\n",
            "Epoch: 276 \t iter-Loss: 0.014\n",
            "Epoch: 277 \t iter-Loss: 0.014\n",
            "Epoch: 278 \t iter-Loss: 0.014\n",
            "Epoch: 279 \t iter-Loss: 0.014\n",
            "Epoch: 280 \t iter-Loss: 0.014\n",
            "Epoch: 281 \t iter-Loss: 0.014\n",
            "Epoch: 282 \t iter-Loss: 0.014\n",
            "Epoch: 283 \t iter-Loss: 0.014\n",
            "Epoch: 284 \t iter-Loss: 0.014\n",
            "Epoch: 285 \t iter-Loss: 0.014\n",
            "Epoch: 286 \t iter-Loss: 0.014\n",
            "Epoch: 287 \t iter-Loss: 0.013\n",
            "Epoch: 288 \t iter-Loss: 0.013\n",
            "Epoch: 289 \t iter-Loss: 0.013\n",
            "Epoch: 290 \t iter-Loss: 0.013\n",
            "Epoch: 291 \t iter-Loss: 0.013\n",
            "Epoch: 292 \t iter-Loss: 0.013\n",
            "Epoch: 293 \t iter-Loss: 0.013\n",
            "Epoch: 294 \t iter-Loss: 0.013\n",
            "Epoch: 295 \t iter-Loss: 0.013\n",
            "Epoch: 296 \t iter-Loss: 0.013\n",
            "Epoch: 297 \t iter-Loss: 0.013\n",
            "Epoch: 298 \t iter-Loss: 0.013\n",
            "Epoch: 299 \t iter-Loss: 0.012\n",
            "Epoch: 300 \t iter-Loss: 0.012\n",
            "Epoch: 301 \t iter-Loss: 0.012\n",
            "Epoch: 302 \t iter-Loss: 0.012\n",
            "Epoch: 303 \t iter-Loss: 0.012\n",
            "Epoch: 304 \t iter-Loss: 0.012\n",
            "Epoch: 305 \t iter-Loss: 0.012\n",
            "Epoch: 306 \t iter-Loss: 0.012\n",
            "Epoch: 307 \t iter-Loss: 0.012\n",
            "Epoch: 308 \t iter-Loss: 0.012\n",
            "Epoch: 309 \t iter-Loss: 0.012\n",
            "Epoch: 310 \t iter-Loss: 0.012\n",
            "Epoch: 311 \t iter-Loss: 0.012\n",
            "Epoch: 312 \t iter-Loss: 0.011\n",
            "Epoch: 313 \t iter-Loss: 0.011\n",
            "Epoch: 314 \t iter-Loss: 0.011\n",
            "Epoch: 315 \t iter-Loss: 0.011\n",
            "Epoch: 316 \t iter-Loss: 0.011\n",
            "Epoch: 317 \t iter-Loss: 0.011\n",
            "Epoch: 318 \t iter-Loss: 0.011\n",
            "Epoch: 319 \t iter-Loss: 0.011\n",
            "Epoch: 320 \t iter-Loss: 0.011\n",
            "Epoch: 321 \t iter-Loss: 0.011\n",
            "Epoch: 322 \t iter-Loss: 0.011\n",
            "Epoch: 323 \t iter-Loss: 0.011\n",
            "Epoch: 324 \t iter-Loss: 0.011\n",
            "Epoch: 325 \t iter-Loss: 0.011\n",
            "Epoch: 326 \t iter-Loss: 0.010\n",
            "Epoch: 327 \t iter-Loss: 0.010\n",
            "Epoch: 328 \t iter-Loss: 0.010\n",
            "Epoch: 329 \t iter-Loss: 0.010\n",
            "Epoch: 330 \t iter-Loss: 0.010\n",
            "Epoch: 331 \t iter-Loss: 0.010\n",
            "Epoch: 332 \t iter-Loss: 0.010\n",
            "Epoch: 333 \t iter-Loss: 0.010\n",
            "Epoch: 334 \t iter-Loss: 0.010\n",
            "Epoch: 335 \t iter-Loss: 0.010\n",
            "Epoch: 336 \t iter-Loss: 0.010\n",
            "Epoch: 337 \t iter-Loss: 0.010\n",
            "Epoch: 338 \t iter-Loss: 0.010\n",
            "Epoch: 339 \t iter-Loss: 0.010\n",
            "Epoch: 340 \t iter-Loss: 0.010\n",
            "Epoch: 341 \t iter-Loss: 0.010\n",
            "Epoch: 342 \t iter-Loss: 0.010\n",
            "Epoch: 343 \t iter-Loss: 0.009\n",
            "Epoch: 344 \t iter-Loss: 0.009\n",
            "Epoch: 345 \t iter-Loss: 0.009\n",
            "Epoch: 346 \t iter-Loss: 0.009\n",
            "Epoch: 347 \t iter-Loss: 0.009\n",
            "Epoch: 348 \t iter-Loss: 0.009\n",
            "Epoch: 349 \t iter-Loss: 0.009\n",
            "Epoch: 350 \t iter-Loss: 0.009\n",
            "Epoch: 351 \t iter-Loss: 0.009\n",
            "Epoch: 352 \t iter-Loss: 0.009\n",
            "Epoch: 353 \t iter-Loss: 0.009\n",
            "Epoch: 354 \t iter-Loss: 0.009\n",
            "Epoch: 355 \t iter-Loss: 0.009\n",
            "Epoch: 356 \t iter-Loss: 0.009\n",
            "Epoch: 357 \t iter-Loss: 0.009\n",
            "Epoch: 358 \t iter-Loss: 0.009\n",
            "Epoch: 359 \t iter-Loss: 0.009\n",
            "Epoch: 360 \t iter-Loss: 0.009\n",
            "Epoch: 361 \t iter-Loss: 0.008\n",
            "Epoch: 362 \t iter-Loss: 0.008\n",
            "Epoch: 363 \t iter-Loss: 0.008\n",
            "Epoch: 364 \t iter-Loss: 0.008\n",
            "Epoch: 365 \t iter-Loss: 0.008\n",
            "Epoch: 366 \t iter-Loss: 0.008\n",
            "Epoch: 367 \t iter-Loss: 0.008\n",
            "Epoch: 368 \t iter-Loss: 0.008\n",
            "Epoch: 369 \t iter-Loss: 0.008\n",
            "Epoch: 370 \t iter-Loss: 0.008\n",
            "Epoch: 371 \t iter-Loss: 0.008\n",
            "Epoch: 372 \t iter-Loss: 0.008\n",
            "Epoch: 373 \t iter-Loss: 0.008\n",
            "Epoch: 374 \t iter-Loss: 0.008\n",
            "Epoch: 375 \t iter-Loss: 0.008\n",
            "Epoch: 376 \t iter-Loss: 0.008\n",
            "Epoch: 377 \t iter-Loss: 0.008\n",
            "Epoch: 378 \t iter-Loss: 0.008\n",
            "Epoch: 379 \t iter-Loss: 0.008\n",
            "Epoch: 380 \t iter-Loss: 0.008\n",
            "Epoch: 381 \t iter-Loss: 0.008\n",
            "Epoch: 382 \t iter-Loss: 0.008\n",
            "Epoch: 383 \t iter-Loss: 0.007\n",
            "Epoch: 384 \t iter-Loss: 0.007\n",
            "Epoch: 385 \t iter-Loss: 0.007\n",
            "Epoch: 386 \t iter-Loss: 0.007\n",
            "Epoch: 387 \t iter-Loss: 0.007\n",
            "Epoch: 388 \t iter-Loss: 0.007\n",
            "Epoch: 389 \t iter-Loss: 0.007\n",
            "Epoch: 390 \t iter-Loss: 0.007\n",
            "Epoch: 391 \t iter-Loss: 0.007\n",
            "Epoch: 392 \t iter-Loss: 0.007\n",
            "Epoch: 393 \t iter-Loss: 0.007\n",
            "Epoch: 394 \t iter-Loss: 0.007\n",
            "Epoch: 395 \t iter-Loss: 0.007\n",
            "Epoch: 396 \t iter-Loss: 0.007\n",
            "Epoch: 397 \t iter-Loss: 0.007\n",
            "Epoch: 398 \t iter-Loss: 0.007\n",
            "Epoch: 399 \t iter-Loss: 0.007\n",
            "Epoch: 400 \t iter-Loss: 0.007\n",
            "Epoch: 401 \t iter-Loss: 0.007\n",
            "Epoch: 402 \t iter-Loss: 0.007\n",
            "Epoch: 403 \t iter-Loss: 0.007\n",
            "Epoch: 404 \t iter-Loss: 0.007\n",
            "Epoch: 405 \t iter-Loss: 0.007\n",
            "Epoch: 406 \t iter-Loss: 0.007\n",
            "Epoch: 407 \t iter-Loss: 0.007\n",
            "Epoch: 408 \t iter-Loss: 0.006\n",
            "Epoch: 409 \t iter-Loss: 0.006\n",
            "Epoch: 410 \t iter-Loss: 0.006\n",
            "Epoch: 411 \t iter-Loss: 0.006\n",
            "Epoch: 412 \t iter-Loss: 0.006\n",
            "Epoch: 413 \t iter-Loss: 0.006\n",
            "Epoch: 414 \t iter-Loss: 0.006\n",
            "Epoch: 415 \t iter-Loss: 0.006\n",
            "Epoch: 416 \t iter-Loss: 0.006\n",
            "Epoch: 417 \t iter-Loss: 0.006\n",
            "Epoch: 418 \t iter-Loss: 0.006\n",
            "Epoch: 419 \t iter-Loss: 0.006\n",
            "Epoch: 420 \t iter-Loss: 0.006\n",
            "Epoch: 421 \t iter-Loss: 0.006\n",
            "Epoch: 422 \t iter-Loss: 0.006\n",
            "Epoch: 423 \t iter-Loss: 0.006\n",
            "Epoch: 424 \t iter-Loss: 0.006\n",
            "Epoch: 425 \t iter-Loss: 0.006\n",
            "Epoch: 426 \t iter-Loss: 0.006\n",
            "Epoch: 427 \t iter-Loss: 0.006\n",
            "Epoch: 428 \t iter-Loss: 0.006\n",
            "Epoch: 429 \t iter-Loss: 0.006\n",
            "Epoch: 430 \t iter-Loss: 0.006\n",
            "Epoch: 431 \t iter-Loss: 0.006\n",
            "Epoch: 432 \t iter-Loss: 0.006\n",
            "Epoch: 433 \t iter-Loss: 0.006\n",
            "Epoch: 434 \t iter-Loss: 0.006\n",
            "Epoch: 435 \t iter-Loss: 0.006\n",
            "Epoch: 436 \t iter-Loss: 0.006\n",
            "Epoch: 437 \t iter-Loss: 0.006\n",
            "Epoch: 438 \t iter-Loss: 0.006\n",
            "Epoch: 439 \t iter-Loss: 0.005\n",
            "Epoch: 440 \t iter-Loss: 0.005\n",
            "Epoch: 441 \t iter-Loss: 0.005\n",
            "Epoch: 442 \t iter-Loss: 0.005\n",
            "Epoch: 443 \t iter-Loss: 0.005\n",
            "Epoch: 444 \t iter-Loss: 0.005\n",
            "Epoch: 445 \t iter-Loss: 0.005\n",
            "Epoch: 446 \t iter-Loss: 0.005\n",
            "Epoch: 447 \t iter-Loss: 0.005\n",
            "Epoch: 448 \t iter-Loss: 0.005\n",
            "Epoch: 449 \t iter-Loss: 0.005\n",
            "Epoch: 450 \t iter-Loss: 0.005\n",
            "Epoch: 451 \t iter-Loss: 0.005\n",
            "Epoch: 452 \t iter-Loss: 0.005\n",
            "Epoch: 453 \t iter-Loss: 0.005\n",
            "Epoch: 454 \t iter-Loss: 0.005\n",
            "Epoch: 455 \t iter-Loss: 0.005\n",
            "Epoch: 456 \t iter-Loss: 0.005\n",
            "Epoch: 457 \t iter-Loss: 0.005\n",
            "Epoch: 458 \t iter-Loss: 0.005\n",
            "Epoch: 459 \t iter-Loss: 0.005\n",
            "Epoch: 460 \t iter-Loss: 0.005\n",
            "Epoch: 461 \t iter-Loss: 0.005\n",
            "Epoch: 462 \t iter-Loss: 0.005\n",
            "Epoch: 463 \t iter-Loss: 0.005\n",
            "Epoch: 464 \t iter-Loss: 0.005\n",
            "Epoch: 465 \t iter-Loss: 0.005\n",
            "Epoch: 466 \t iter-Loss: 0.005\n",
            "Epoch: 467 \t iter-Loss: 0.005\n",
            "Epoch: 468 \t iter-Loss: 0.005\n",
            "Epoch: 469 \t iter-Loss: 0.005\n",
            "Epoch: 470 \t iter-Loss: 0.005\n",
            "Epoch: 471 \t iter-Loss: 0.005\n",
            "Epoch: 472 \t iter-Loss: 0.005\n",
            "Epoch: 473 \t iter-Loss: 0.005\n",
            "Epoch: 474 \t iter-Loss: 0.005\n",
            "Epoch: 475 \t iter-Loss: 0.005\n",
            "Epoch: 476 \t iter-Loss: 0.005\n",
            "Epoch: 477 \t iter-Loss: 0.005\n",
            "Epoch: 478 \t iter-Loss: 0.005\n",
            "Epoch: 479 \t iter-Loss: 0.004\n",
            "Epoch: 480 \t iter-Loss: 0.004\n",
            "Epoch: 481 \t iter-Loss: 0.004\n",
            "Epoch: 482 \t iter-Loss: 0.004\n",
            "Epoch: 483 \t iter-Loss: 0.004\n",
            "Epoch: 484 \t iter-Loss: 0.004\n",
            "Epoch: 485 \t iter-Loss: 0.004\n",
            "Epoch: 486 \t iter-Loss: 0.004\n",
            "Epoch: 487 \t iter-Loss: 0.004\n",
            "Epoch: 488 \t iter-Loss: 0.004\n",
            "Epoch: 489 \t iter-Loss: 0.004\n",
            "Epoch: 490 \t iter-Loss: 0.004\n",
            "Epoch: 491 \t iter-Loss: 0.004\n",
            "Epoch: 492 \t iter-Loss: 0.004\n",
            "Epoch: 493 \t iter-Loss: 0.004\n",
            "Epoch: 494 \t iter-Loss: 0.004\n",
            "Epoch: 495 \t iter-Loss: 0.004\n",
            "Epoch: 496 \t iter-Loss: 0.004\n",
            "Epoch: 497 \t iter-Loss: 0.004\n",
            "Epoch: 498 \t iter-Loss: 0.004\n",
            "Epoch: 499 \t iter-Loss: 0.004\n",
            "Epoch: 500 \t iter-Loss: 0.004\n",
            "Epoch: 501 \t iter-Loss: 0.004\n",
            "Epoch: 502 \t iter-Loss: 0.004\n",
            "Epoch: 503 \t iter-Loss: 0.004\n",
            "Epoch: 504 \t iter-Loss: 0.004\n",
            "Epoch: 505 \t iter-Loss: 0.004\n",
            "Epoch: 506 \t iter-Loss: 0.004\n",
            "Epoch: 507 \t iter-Loss: 0.004\n",
            "Epoch: 508 \t iter-Loss: 0.004\n",
            "Epoch: 509 \t iter-Loss: 0.004\n",
            "Epoch: 510 \t iter-Loss: 0.004\n",
            "Epoch: 511 \t iter-Loss: 0.004\n",
            "Epoch: 512 \t iter-Loss: 0.004\n",
            "Epoch: 513 \t iter-Loss: 0.004\n",
            "Epoch: 514 \t iter-Loss: 0.004\n",
            "Epoch: 515 \t iter-Loss: 0.004\n",
            "Epoch: 516 \t iter-Loss: 0.004\n",
            "Epoch: 517 \t iter-Loss: 0.004\n",
            "Epoch: 518 \t iter-Loss: 0.004\n",
            "Epoch: 519 \t iter-Loss: 0.004\n",
            "Epoch: 520 \t iter-Loss: 0.004\n",
            "Epoch: 521 \t iter-Loss: 0.004\n",
            "Epoch: 522 \t iter-Loss: 0.004\n",
            "Epoch: 523 \t iter-Loss: 0.004\n",
            "Epoch: 524 \t iter-Loss: 0.004\n",
            "Epoch: 525 \t iter-Loss: 0.004\n",
            "Epoch: 526 \t iter-Loss: 0.004\n",
            "Epoch: 527 \t iter-Loss: 0.004\n",
            "Epoch: 528 \t iter-Loss: 0.004\n",
            "Epoch: 529 \t iter-Loss: 0.004\n",
            "Epoch: 530 \t iter-Loss: 0.004\n",
            "Epoch: 531 \t iter-Loss: 0.004\n",
            "Epoch: 532 \t iter-Loss: 0.004\n",
            "Epoch: 533 \t iter-Loss: 0.004\n",
            "Epoch: 534 \t iter-Loss: 0.003\n",
            "Epoch: 535 \t iter-Loss: 0.003\n",
            "Epoch: 536 \t iter-Loss: 0.003\n",
            "Epoch: 537 \t iter-Loss: 0.003\n",
            "Epoch: 538 \t iter-Loss: 0.003\n",
            "Epoch: 539 \t iter-Loss: 0.003\n",
            "Epoch: 540 \t iter-Loss: 0.003\n",
            "Epoch: 541 \t iter-Loss: 0.003\n",
            "Epoch: 542 \t iter-Loss: 0.003\n",
            "Epoch: 543 \t iter-Loss: 0.003\n",
            "Epoch: 544 \t iter-Loss: 0.003\n",
            "Epoch: 545 \t iter-Loss: 0.003\n",
            "Epoch: 546 \t iter-Loss: 0.003\n",
            "Epoch: 547 \t iter-Loss: 0.003\n",
            "Epoch: 548 \t iter-Loss: 0.003\n",
            "Epoch: 549 \t iter-Loss: 0.003\n",
            "Epoch: 550 \t iter-Loss: 0.003\n",
            "Epoch: 551 \t iter-Loss: 0.003\n",
            "Epoch: 552 \t iter-Loss: 0.003\n",
            "Epoch: 553 \t iter-Loss: 0.003\n",
            "Epoch: 554 \t iter-Loss: 0.003\n",
            "Epoch: 555 \t iter-Loss: 0.003\n",
            "Epoch: 556 \t iter-Loss: 0.003\n",
            "Epoch: 557 \t iter-Loss: 0.003\n",
            "Epoch: 558 \t iter-Loss: 0.003\n",
            "Epoch: 559 \t iter-Loss: 0.003\n",
            "Epoch: 560 \t iter-Loss: 0.003\n",
            "Epoch: 561 \t iter-Loss: 0.003\n",
            "Epoch: 562 \t iter-Loss: 0.003\n",
            "Epoch: 563 \t iter-Loss: 0.003\n",
            "Epoch: 564 \t iter-Loss: 0.003\n",
            "Epoch: 565 \t iter-Loss: 0.003\n",
            "Epoch: 566 \t iter-Loss: 0.003\n",
            "Epoch: 567 \t iter-Loss: 0.003\n",
            "Epoch: 568 \t iter-Loss: 0.003\n",
            "Epoch: 569 \t iter-Loss: 0.003\n",
            "Epoch: 570 \t iter-Loss: 0.003\n",
            "Epoch: 571 \t iter-Loss: 0.003\n",
            "Epoch: 572 \t iter-Loss: 0.003\n",
            "Epoch: 573 \t iter-Loss: 0.003\n",
            "Epoch: 574 \t iter-Loss: 0.003\n",
            "Epoch: 575 \t iter-Loss: 0.003\n",
            "Epoch: 576 \t iter-Loss: 0.003\n",
            "Epoch: 577 \t iter-Loss: 0.003\n",
            "Epoch: 578 \t iter-Loss: 0.003\n",
            "Epoch: 579 \t iter-Loss: 0.003\n",
            "Epoch: 580 \t iter-Loss: 0.003\n",
            "Epoch: 581 \t iter-Loss: 0.003\n",
            "Epoch: 582 \t iter-Loss: 0.003\n",
            "Epoch: 583 \t iter-Loss: 0.003\n",
            "Epoch: 584 \t iter-Loss: 0.003\n",
            "Epoch: 585 \t iter-Loss: 0.003\n",
            "Epoch: 586 \t iter-Loss: 0.003\n",
            "Epoch: 587 \t iter-Loss: 0.003\n",
            "Epoch: 588 \t iter-Loss: 0.003\n",
            "Epoch: 589 \t iter-Loss: 0.003\n",
            "Epoch: 590 \t iter-Loss: 0.003\n",
            "Epoch: 591 \t iter-Loss: 0.003\n",
            "Epoch: 592 \t iter-Loss: 0.003\n",
            "Epoch: 593 \t iter-Loss: 0.003\n",
            "Epoch: 594 \t iter-Loss: 0.003\n",
            "Epoch: 595 \t iter-Loss: 0.003\n",
            "Epoch: 596 \t iter-Loss: 0.003\n",
            "Epoch: 597 \t iter-Loss: 0.003\n",
            "Epoch: 598 \t iter-Loss: 0.003\n",
            "Epoch: 599 \t iter-Loss: 0.003\n",
            "Epoch: 600 \t iter-Loss: 0.003\n",
            "Epoch: 601 \t iter-Loss: 0.003\n",
            "Epoch: 602 \t iter-Loss: 0.003\n",
            "Epoch: 603 \t iter-Loss: 0.003\n",
            "Epoch: 604 \t iter-Loss: 0.003\n",
            "Epoch: 605 \t iter-Loss: 0.003\n",
            "Epoch: 606 \t iter-Loss: 0.003\n",
            "Epoch: 607 \t iter-Loss: 0.003\n",
            "Epoch: 608 \t iter-Loss: 0.003\n",
            "Epoch: 609 \t iter-Loss: 0.003\n",
            "Epoch: 610 \t iter-Loss: 0.003\n",
            "Epoch: 611 \t iter-Loss: 0.003\n",
            "Epoch: 612 \t iter-Loss: 0.003\n",
            "Epoch: 613 \t iter-Loss: 0.003\n",
            "Epoch: 614 \t iter-Loss: 0.003\n",
            "Epoch: 615 \t iter-Loss: 0.003\n",
            "Epoch: 616 \t iter-Loss: 0.003\n",
            "Epoch: 617 \t iter-Loss: 0.003\n",
            "Epoch: 618 \t iter-Loss: 0.003\n",
            "Epoch: 619 \t iter-Loss: 0.003\n",
            "Epoch: 620 \t iter-Loss: 0.002\n",
            "Epoch: 621 \t iter-Loss: 0.002\n",
            "Epoch: 622 \t iter-Loss: 0.002\n",
            "Epoch: 623 \t iter-Loss: 0.002\n",
            "Epoch: 624 \t iter-Loss: 0.002\n",
            "Epoch: 625 \t iter-Loss: 0.002\n",
            "Epoch: 626 \t iter-Loss: 0.002\n",
            "Epoch: 627 \t iter-Loss: 0.002\n",
            "Epoch: 628 \t iter-Loss: 0.002\n",
            "Epoch: 629 \t iter-Loss: 0.002\n",
            "Epoch: 630 \t iter-Loss: 0.002\n",
            "Epoch: 631 \t iter-Loss: 0.002\n",
            "Epoch: 632 \t iter-Loss: 0.002\n",
            "Epoch: 633 \t iter-Loss: 0.002\n",
            "Epoch: 634 \t iter-Loss: 0.002\n",
            "Epoch: 635 \t iter-Loss: 0.002\n",
            "Epoch: 636 \t iter-Loss: 0.002\n",
            "Epoch: 637 \t iter-Loss: 0.002\n",
            "Epoch: 638 \t iter-Loss: 0.002\n",
            "Epoch: 639 \t iter-Loss: 0.002\n",
            "Epoch: 640 \t iter-Loss: 0.002\n",
            "Epoch: 641 \t iter-Loss: 0.002\n",
            "Epoch: 642 \t iter-Loss: 0.002\n",
            "Epoch: 643 \t iter-Loss: 0.002\n",
            "Epoch: 644 \t iter-Loss: 0.002\n",
            "Epoch: 645 \t iter-Loss: 0.002\n",
            "Epoch: 646 \t iter-Loss: 0.002\n",
            "Epoch: 647 \t iter-Loss: 0.002\n",
            "Epoch: 648 \t iter-Loss: 0.002\n",
            "Epoch: 649 \t iter-Loss: 0.002\n",
            "Epoch: 650 \t iter-Loss: 0.002\n",
            "Epoch: 651 \t iter-Loss: 0.002\n",
            "Epoch: 652 \t iter-Loss: 0.002\n",
            "Epoch: 653 \t iter-Loss: 0.002\n",
            "Epoch: 654 \t iter-Loss: 0.002\n",
            "Epoch: 655 \t iter-Loss: 0.002\n",
            "Epoch: 656 \t iter-Loss: 0.002\n",
            "Epoch: 657 \t iter-Loss: 0.002\n",
            "Epoch: 658 \t iter-Loss: 0.002\n",
            "Epoch: 659 \t iter-Loss: 0.002\n",
            "Epoch: 660 \t iter-Loss: 0.002\n",
            "Epoch: 661 \t iter-Loss: 0.002\n",
            "Epoch: 662 \t iter-Loss: 0.002\n",
            "Epoch: 663 \t iter-Loss: 0.002\n",
            "Epoch: 664 \t iter-Loss: 0.002\n",
            "Epoch: 665 \t iter-Loss: 0.002\n",
            "Epoch: 666 \t iter-Loss: 0.002\n",
            "Epoch: 667 \t iter-Loss: 0.002\n",
            "Epoch: 668 \t iter-Loss: 0.002\n",
            "Epoch: 669 \t iter-Loss: 0.002\n",
            "Epoch: 670 \t iter-Loss: 0.002\n",
            "Epoch: 671 \t iter-Loss: 0.002\n",
            "Epoch: 672 \t iter-Loss: 0.002\n",
            "Epoch: 673 \t iter-Loss: 0.002\n",
            "Epoch: 674 \t iter-Loss: 0.002\n",
            "Epoch: 675 \t iter-Loss: 0.002\n",
            "Epoch: 676 \t iter-Loss: 0.002\n",
            "Epoch: 677 \t iter-Loss: 0.002\n",
            "Epoch: 678 \t iter-Loss: 0.002\n",
            "Epoch: 679 \t iter-Loss: 0.002\n",
            "Epoch: 680 \t iter-Loss: 0.002\n",
            "Epoch: 681 \t iter-Loss: 0.002\n",
            "Epoch: 682 \t iter-Loss: 0.002\n",
            "Epoch: 683 \t iter-Loss: 0.002\n",
            "Epoch: 684 \t iter-Loss: 0.002\n",
            "Epoch: 685 \t iter-Loss: 0.002\n",
            "Epoch: 686 \t iter-Loss: 0.002\n",
            "Epoch: 687 \t iter-Loss: 0.002\n",
            "Epoch: 688 \t iter-Loss: 0.002\n",
            "Epoch: 689 \t iter-Loss: 0.002\n",
            "Epoch: 690 \t iter-Loss: 0.002\n",
            "Epoch: 691 \t iter-Loss: 0.002\n",
            "Epoch: 692 \t iter-Loss: 0.002\n",
            "Epoch: 693 \t iter-Loss: 0.002\n",
            "Epoch: 694 \t iter-Loss: 0.002\n",
            "Epoch: 695 \t iter-Loss: 0.002\n",
            "Epoch: 696 \t iter-Loss: 0.002\n",
            "Epoch: 697 \t iter-Loss: 0.002\n",
            "Epoch: 698 \t iter-Loss: 0.002\n",
            "Epoch: 699 \t iter-Loss: 0.002\n",
            "Epoch: 700 \t iter-Loss: 0.002\n",
            "Epoch: 701 \t iter-Loss: 0.002\n",
            "Epoch: 702 \t iter-Loss: 0.002\n",
            "Epoch: 703 \t iter-Loss: 0.002\n",
            "Epoch: 704 \t iter-Loss: 0.002\n",
            "Epoch: 705 \t iter-Loss: 0.002\n",
            "Epoch: 706 \t iter-Loss: 0.002\n",
            "Epoch: 707 \t iter-Loss: 0.002\n",
            "Epoch: 708 \t iter-Loss: 0.002\n",
            "Epoch: 709 \t iter-Loss: 0.002\n",
            "Epoch: 710 \t iter-Loss: 0.002\n",
            "Epoch: 711 \t iter-Loss: 0.002\n",
            "Epoch: 712 \t iter-Loss: 0.002\n",
            "Epoch: 713 \t iter-Loss: 0.002\n",
            "Epoch: 714 \t iter-Loss: 0.002\n",
            "Epoch: 715 \t iter-Loss: 0.002\n",
            "Epoch: 716 \t iter-Loss: 0.002\n",
            "Epoch: 717 \t iter-Loss: 0.002\n",
            "Epoch: 718 \t iter-Loss: 0.002\n",
            "Epoch: 719 \t iter-Loss: 0.002\n",
            "Epoch: 720 \t iter-Loss: 0.002\n",
            "Epoch: 721 \t iter-Loss: 0.002\n",
            "Epoch: 722 \t iter-Loss: 0.002\n",
            "Epoch: 723 \t iter-Loss: 0.002\n",
            "Epoch: 724 \t iter-Loss: 0.002\n",
            "Epoch: 725 \t iter-Loss: 0.002\n",
            "Epoch: 726 \t iter-Loss: 0.002\n",
            "Epoch: 727 \t iter-Loss: 0.002\n",
            "Epoch: 728 \t iter-Loss: 0.002\n",
            "Epoch: 729 \t iter-Loss: 0.002\n",
            "Epoch: 730 \t iter-Loss: 0.002\n",
            "Epoch: 731 \t iter-Loss: 0.002\n",
            "Epoch: 732 \t iter-Loss: 0.002\n",
            "Epoch: 733 \t iter-Loss: 0.002\n",
            "Epoch: 734 \t iter-Loss: 0.002\n",
            "Epoch: 735 \t iter-Loss: 0.002\n",
            "Epoch: 736 \t iter-Loss: 0.002\n",
            "Epoch: 737 \t iter-Loss: 0.002\n",
            "Epoch: 738 \t iter-Loss: 0.002\n",
            "Epoch: 739 \t iter-Loss: 0.002\n",
            "Epoch: 740 \t iter-Loss: 0.002\n",
            "Epoch: 741 \t iter-Loss: 0.002\n",
            "Epoch: 742 \t iter-Loss: 0.002\n",
            "Epoch: 743 \t iter-Loss: 0.002\n",
            "Epoch: 744 \t iter-Loss: 0.002\n",
            "Epoch: 745 \t iter-Loss: 0.002\n",
            "Epoch: 746 \t iter-Loss: 0.002\n",
            "Epoch: 747 \t iter-Loss: 0.002\n",
            "Epoch: 748 \t iter-Loss: 0.002\n",
            "Epoch: 749 \t iter-Loss: 0.002\n",
            "Epoch: 750 \t iter-Loss: 0.002\n",
            "Epoch: 751 \t iter-Loss: 0.002\n",
            "Epoch: 752 \t iter-Loss: 0.002\n",
            "Epoch: 753 \t iter-Loss: 0.002\n",
            "Epoch: 754 \t iter-Loss: 0.002\n",
            "Epoch: 755 \t iter-Loss: 0.002\n",
            "Epoch: 756 \t iter-Loss: 0.002\n",
            "Epoch: 757 \t iter-Loss: 0.002\n",
            "Epoch: 758 \t iter-Loss: 0.002\n",
            "Epoch: 759 \t iter-Loss: 0.002\n",
            "Epoch: 760 \t iter-Loss: 0.002\n",
            "Epoch: 761 \t iter-Loss: 0.002\n",
            "Epoch: 762 \t iter-Loss: 0.002\n",
            "Epoch: 763 \t iter-Loss: 0.002\n",
            "Epoch: 764 \t iter-Loss: 0.002\n",
            "Epoch: 765 \t iter-Loss: 0.002\n",
            "Epoch: 766 \t iter-Loss: 0.002\n",
            "Epoch: 767 \t iter-Loss: 0.002\n",
            "Epoch: 768 \t iter-Loss: 0.002\n",
            "Epoch: 769 \t iter-Loss: 0.002\n",
            "Epoch: 770 \t iter-Loss: 0.002\n",
            "Epoch: 771 \t iter-Loss: 0.002\n",
            "Epoch: 772 \t iter-Loss: 0.002\n",
            "Epoch: 773 \t iter-Loss: 0.002\n",
            "Epoch: 774 \t iter-Loss: 0.002\n",
            "Epoch: 775 \t iter-Loss: 0.002\n",
            "Epoch: 776 \t iter-Loss: 0.002\n",
            "Epoch: 777 \t iter-Loss: 0.002\n",
            "Epoch: 778 \t iter-Loss: 0.002\n",
            "Epoch: 779 \t iter-Loss: 0.002\n",
            "Epoch: 780 \t iter-Loss: 0.002\n",
            "Epoch: 781 \t iter-Loss: 0.002\n",
            "Epoch: 782 \t iter-Loss: 0.002\n",
            "Epoch: 783 \t iter-Loss: 0.002\n",
            "Epoch: 784 \t iter-Loss: 0.002\n",
            "Epoch: 785 \t iter-Loss: 0.002\n",
            "Epoch: 786 \t iter-Loss: 0.002\n",
            "Epoch: 787 \t iter-Loss: 0.002\n",
            "Epoch: 788 \t iter-Loss: 0.002\n",
            "Epoch: 789 \t iter-Loss: 0.001\n",
            "Epoch: 790 \t iter-Loss: 0.001\n",
            "Epoch: 791 \t iter-Loss: 0.001\n",
            "Epoch: 792 \t iter-Loss: 0.001\n",
            "Epoch: 793 \t iter-Loss: 0.001\n",
            "Epoch: 794 \t iter-Loss: 0.001\n",
            "Epoch: 795 \t iter-Loss: 0.001\n",
            "Epoch: 796 \t iter-Loss: 0.001\n",
            "Epoch: 797 \t iter-Loss: 0.001\n",
            "Epoch: 798 \t iter-Loss: 0.001\n",
            "Epoch: 799 \t iter-Loss: 0.001\n",
            "Epoch: 800 \t iter-Loss: 0.001\n",
            "Epoch: 801 \t iter-Loss: 0.001\n",
            "Epoch: 802 \t iter-Loss: 0.001\n",
            "Epoch: 803 \t iter-Loss: 0.001\n",
            "Epoch: 804 \t iter-Loss: 0.001\n",
            "Epoch: 805 \t iter-Loss: 0.001\n",
            "Epoch: 806 \t iter-Loss: 0.001\n",
            "Epoch: 807 \t iter-Loss: 0.001\n",
            "Epoch: 808 \t iter-Loss: 0.001\n",
            "Epoch: 809 \t iter-Loss: 0.001\n",
            "Epoch: 810 \t iter-Loss: 0.001\n",
            "Epoch: 811 \t iter-Loss: 0.001\n",
            "Epoch: 812 \t iter-Loss: 0.001\n",
            "Epoch: 813 \t iter-Loss: 0.001\n",
            "Epoch: 814 \t iter-Loss: 0.001\n",
            "Epoch: 815 \t iter-Loss: 0.001\n",
            "Epoch: 816 \t iter-Loss: 0.001\n",
            "Epoch: 817 \t iter-Loss: 0.001\n",
            "Epoch: 818 \t iter-Loss: 0.001\n",
            "Epoch: 819 \t iter-Loss: 0.001\n",
            "Epoch: 820 \t iter-Loss: 0.001\n",
            "Epoch: 821 \t iter-Loss: 0.001\n",
            "Epoch: 822 \t iter-Loss: 0.001\n",
            "Epoch: 823 \t iter-Loss: 0.001\n",
            "Epoch: 824 \t iter-Loss: 0.001\n",
            "Epoch: 825 \t iter-Loss: 0.001\n",
            "Epoch: 826 \t iter-Loss: 0.001\n",
            "Epoch: 827 \t iter-Loss: 0.001\n",
            "Epoch: 828 \t iter-Loss: 0.001\n",
            "Epoch: 829 \t iter-Loss: 0.001\n",
            "Epoch: 830 \t iter-Loss: 0.001\n",
            "Epoch: 831 \t iter-Loss: 0.001\n",
            "Epoch: 832 \t iter-Loss: 0.001\n",
            "Epoch: 833 \t iter-Loss: 0.001\n",
            "Epoch: 834 \t iter-Loss: 0.001\n",
            "Epoch: 835 \t iter-Loss: 0.001\n",
            "Epoch: 836 \t iter-Loss: 0.001\n",
            "Epoch: 837 \t iter-Loss: 0.001\n",
            "Epoch: 838 \t iter-Loss: 0.001\n",
            "Epoch: 839 \t iter-Loss: 0.001\n",
            "Epoch: 840 \t iter-Loss: 0.001\n",
            "Epoch: 841 \t iter-Loss: 0.001\n",
            "Epoch: 842 \t iter-Loss: 0.001\n",
            "Epoch: 843 \t iter-Loss: 0.001\n",
            "Epoch: 844 \t iter-Loss: 0.001\n",
            "Epoch: 845 \t iter-Loss: 0.001\n",
            "Epoch: 846 \t iter-Loss: 0.001\n",
            "Epoch: 847 \t iter-Loss: 0.001\n",
            "Epoch: 848 \t iter-Loss: 0.001\n",
            "Epoch: 849 \t iter-Loss: 0.001\n",
            "Epoch: 850 \t iter-Loss: 0.001\n",
            "Epoch: 851 \t iter-Loss: 0.001\n",
            "Epoch: 852 \t iter-Loss: 0.001\n",
            "Epoch: 853 \t iter-Loss: 0.001\n",
            "Epoch: 854 \t iter-Loss: 0.001\n",
            "Epoch: 855 \t iter-Loss: 0.001\n",
            "Epoch: 856 \t iter-Loss: 0.001\n",
            "Epoch: 857 \t iter-Loss: 0.001\n",
            "Epoch: 858 \t iter-Loss: 0.001\n",
            "Epoch: 859 \t iter-Loss: 0.001\n",
            "Epoch: 860 \t iter-Loss: 0.001\n",
            "Epoch: 861 \t iter-Loss: 0.001\n",
            "Epoch: 862 \t iter-Loss: 0.001\n",
            "Epoch: 863 \t iter-Loss: 0.001\n",
            "Epoch: 864 \t iter-Loss: 0.001\n",
            "Epoch: 865 \t iter-Loss: 0.001\n",
            "Epoch: 866 \t iter-Loss: 0.001\n",
            "Epoch: 867 \t iter-Loss: 0.001\n",
            "Epoch: 868 \t iter-Loss: 0.001\n",
            "Epoch: 869 \t iter-Loss: 0.001\n",
            "Epoch: 870 \t iter-Loss: 0.001\n",
            "Epoch: 871 \t iter-Loss: 0.001\n",
            "Epoch: 872 \t iter-Loss: 0.001\n",
            "Epoch: 873 \t iter-Loss: 0.001\n",
            "Epoch: 874 \t iter-Loss: 0.001\n",
            "Epoch: 875 \t iter-Loss: 0.001\n",
            "Epoch: 876 \t iter-Loss: 0.001\n",
            "Epoch: 877 \t iter-Loss: 0.001\n",
            "Epoch: 878 \t iter-Loss: 0.001\n",
            "Epoch: 879 \t iter-Loss: 0.001\n",
            "Epoch: 880 \t iter-Loss: 0.001\n",
            "Epoch: 881 \t iter-Loss: 0.001\n",
            "Epoch: 882 \t iter-Loss: 0.001\n",
            "Epoch: 883 \t iter-Loss: 0.001\n",
            "Epoch: 884 \t iter-Loss: 0.001\n",
            "Epoch: 885 \t iter-Loss: 0.001\n",
            "Epoch: 886 \t iter-Loss: 0.001\n",
            "Epoch: 887 \t iter-Loss: 0.001\n",
            "Epoch: 888 \t iter-Loss: 0.001\n",
            "Epoch: 889 \t iter-Loss: 0.001\n",
            "Epoch: 890 \t iter-Loss: 0.001\n",
            "Epoch: 891 \t iter-Loss: 0.001\n",
            "Epoch: 892 \t iter-Loss: 0.001\n",
            "Epoch: 893 \t iter-Loss: 0.001\n",
            "Epoch: 894 \t iter-Loss: 0.001\n",
            "Epoch: 895 \t iter-Loss: 0.001\n",
            "Epoch: 896 \t iter-Loss: 0.001\n",
            "Epoch: 897 \t iter-Loss: 0.001\n",
            "Epoch: 898 \t iter-Loss: 0.001\n",
            "Epoch: 899 \t iter-Loss: 0.001\n",
            "Epoch: 900 \t iter-Loss: 0.001\n",
            "Epoch: 901 \t iter-Loss: 0.001\n",
            "Epoch: 902 \t iter-Loss: 0.001\n",
            "Epoch: 903 \t iter-Loss: 0.001\n",
            "Epoch: 904 \t iter-Loss: 0.001\n",
            "Epoch: 905 \t iter-Loss: 0.001\n",
            "Epoch: 906 \t iter-Loss: 0.001\n",
            "Epoch: 907 \t iter-Loss: 0.001\n",
            "Epoch: 908 \t iter-Loss: 0.001\n",
            "Epoch: 909 \t iter-Loss: 0.001\n",
            "Epoch: 910 \t iter-Loss: 0.001\n",
            "Epoch: 911 \t iter-Loss: 0.001\n",
            "Epoch: 912 \t iter-Loss: 0.001\n",
            "Epoch: 913 \t iter-Loss: 0.001\n",
            "Epoch: 914 \t iter-Loss: 0.001\n",
            "Epoch: 915 \t iter-Loss: 0.001\n",
            "Epoch: 916 \t iter-Loss: 0.001\n",
            "Epoch: 917 \t iter-Loss: 0.001\n",
            "Epoch: 918 \t iter-Loss: 0.001\n",
            "Epoch: 919 \t iter-Loss: 0.001\n",
            "Epoch: 920 \t iter-Loss: 0.001\n",
            "Epoch: 921 \t iter-Loss: 0.001\n",
            "Epoch: 922 \t iter-Loss: 0.001\n",
            "Epoch: 923 \t iter-Loss: 0.001\n",
            "Epoch: 924 \t iter-Loss: 0.001\n",
            "Epoch: 925 \t iter-Loss: 0.001\n",
            "Epoch: 926 \t iter-Loss: 0.001\n",
            "Epoch: 927 \t iter-Loss: 0.001\n",
            "Epoch: 928 \t iter-Loss: 0.001\n",
            "Epoch: 929 \t iter-Loss: 0.001\n",
            "Epoch: 930 \t iter-Loss: 0.001\n",
            "Epoch: 931 \t iter-Loss: 0.001\n",
            "Epoch: 932 \t iter-Loss: 0.001\n",
            "Epoch: 933 \t iter-Loss: 0.001\n",
            "Epoch: 934 \t iter-Loss: 0.001\n",
            "Epoch: 935 \t iter-Loss: 0.001\n",
            "Epoch: 936 \t iter-Loss: 0.001\n",
            "Epoch: 937 \t iter-Loss: 0.001\n",
            "Epoch: 938 \t iter-Loss: 0.001\n",
            "Epoch: 939 \t iter-Loss: 0.001\n",
            "Epoch: 940 \t iter-Loss: 0.001\n",
            "Epoch: 941 \t iter-Loss: 0.001\n",
            "Epoch: 942 \t iter-Loss: 0.001\n",
            "Epoch: 943 \t iter-Loss: 0.001\n",
            "Epoch: 944 \t iter-Loss: 0.001\n",
            "Epoch: 945 \t iter-Loss: 0.001\n",
            "Epoch: 946 \t iter-Loss: 0.001\n",
            "Epoch: 947 \t iter-Loss: 0.001\n",
            "Epoch: 948 \t iter-Loss: 0.001\n",
            "Epoch: 949 \t iter-Loss: 0.001\n",
            "Epoch: 950 \t iter-Loss: 0.001\n",
            "Epoch: 951 \t iter-Loss: 0.001\n",
            "Epoch: 952 \t iter-Loss: 0.001\n",
            "Epoch: 953 \t iter-Loss: 0.001\n",
            "Epoch: 954 \t iter-Loss: 0.001\n",
            "Epoch: 955 \t iter-Loss: 0.001\n",
            "Epoch: 956 \t iter-Loss: 0.001\n",
            "Epoch: 957 \t iter-Loss: 0.001\n",
            "Epoch: 958 \t iter-Loss: 0.001\n",
            "Epoch: 959 \t iter-Loss: 0.001\n",
            "Epoch: 960 \t iter-Loss: 0.001\n",
            "Epoch: 961 \t iter-Loss: 0.001\n",
            "Epoch: 962 \t iter-Loss: 0.001\n",
            "Epoch: 963 \t iter-Loss: 0.001\n",
            "Epoch: 964 \t iter-Loss: 0.001\n",
            "Epoch: 965 \t iter-Loss: 0.001\n",
            "Epoch: 966 \t iter-Loss: 0.001\n",
            "Epoch: 967 \t iter-Loss: 0.001\n",
            "Epoch: 968 \t iter-Loss: 0.001\n",
            "Epoch: 969 \t iter-Loss: 0.001\n",
            "Epoch: 970 \t iter-Loss: 0.001\n",
            "Epoch: 971 \t iter-Loss: 0.001\n",
            "Epoch: 972 \t iter-Loss: 0.001\n",
            "Epoch: 973 \t iter-Loss: 0.001\n",
            "Epoch: 974 \t iter-Loss: 0.001\n",
            "Epoch: 975 \t iter-Loss: 0.001\n",
            "Epoch: 976 \t iter-Loss: 0.001\n",
            "Epoch: 977 \t iter-Loss: 0.001\n",
            "Epoch: 978 \t iter-Loss: 0.001\n",
            "Epoch: 979 \t iter-Loss: 0.001\n",
            "Epoch: 980 \t iter-Loss: 0.001\n",
            "Epoch: 981 \t iter-Loss: 0.001\n",
            "Epoch: 982 \t iter-Loss: 0.001\n",
            "Epoch: 983 \t iter-Loss: 0.001\n",
            "Epoch: 984 \t iter-Loss: 0.001\n",
            "Epoch: 985 \t iter-Loss: 0.001\n",
            "Epoch: 986 \t iter-Loss: 0.001\n",
            "Epoch: 987 \t iter-Loss: 0.001\n",
            "Epoch: 988 \t iter-Loss: 0.001\n",
            "Epoch: 989 \t iter-Loss: 0.001\n",
            "Epoch: 990 \t iter-Loss: 0.001\n",
            "Epoch: 991 \t iter-Loss: 0.001\n",
            "Epoch: 992 \t iter-Loss: 0.001\n",
            "Epoch: 993 \t iter-Loss: 0.001\n",
            "Epoch: 994 \t iter-Loss: 0.001\n",
            "Epoch: 995 \t iter-Loss: 0.001\n",
            "Epoch: 996 \t iter-Loss: 0.001\n",
            "Epoch: 997 \t iter-Loss: 0.001\n",
            "Epoch: 998 \t iter-Loss: 0.001\n",
            "Epoch: 999 \t iter-Loss: 0.001\n",
            "Epoch: 1000 \t iter-Loss: 0.001\n",
            "final loss: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### A probar! 🧪"
      ],
      "metadata": {
        "id": "9dlS4_X-L3DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is working?, Try the next example!\n",
        "qText = \"'Do you know any joke?'\" # this must classify the label \"funny\"\n",
        "\n",
        "# Esto se tuvo que cambiar:\n",
        "#X = torch.tensor([vocab.stoi[t] for t in tokenizer(qText)]).to(device)\n",
        "# Por esto:\n",
        "X = torch.tensor([stoi[t] for t in tokenizer(qText)]).to(device)\n",
        "\n",
        "model.eval()\n",
        "output = model(X, torch.tensor([0], dtype=torch.long).to(device))\n",
        "_, predicted = torch.max(output, dim=1)\n",
        "labels[predicted]"
      ],
      "metadata": {
        "id": "mRrW7I6alY6O",
        "outputId": "083f2491-d8a6-4fca-a235-1a251b47f49a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'funny'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya pero prometiste hacer un chatbot, no una simple clasificación.... "
      ],
      "metadata": {
        "id": "udemze3zL549"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Guardamos modelo 🦺 (opcional)"
      ],
      "metadata": {
        "id": "OpSYGx2tL0tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We save de model using pytorch (this is optional, just to learn how to do this in pytorch)\n",
        "data = {\n",
        "\"model_state\": model.state_dict(),\n",
        "\"input_size\": INPUT_SIZE,\n",
        "\"output_size\": OUTPUT_SIZE,\n",
        "\"use_cnn\": USE_CNN,\n",
        "\"labels\": labels\n",
        "        }\n",
        "\n",
        "FILE = \"data.pth\"\n",
        "torch.save(data, FILE)\n",
        "\n",
        "print(f'training complete. file saved to {FILE}')"
      ],
      "metadata": {
        "id": "ZBC4TyiqLzDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4144c2d-7799-4417-8c10-97c3241d4df0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training complete. file saved to data.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Chatbot 💬"
      ],
      "metadata": {
        "id": "ZYClbTtsMCjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "with open('star_wars_chatbot.json', 'r') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "\n",
        "FILE = \"data.pth\"\n",
        "data = torch.load(FILE)\n",
        "\n",
        "INPUT_SIZE = data[\"input_size\"]\n",
        "OUTPUT_SIZE = data[\"output_size\"]\n",
        "USE_CNN = data[\"use_cnn\"]\n",
        "labels = data['labels']\n",
        "model_state = data[\"model_state\"]\n",
        "\n",
        "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=USE_CNN).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()\n",
        "\n",
        "# Dictionary with the answers\n",
        "responses = {key['tag']: key['responses'] for key in dataset['intents']}\n",
        "\n",
        "bot_name = \"GA-97\"\n",
        "print(\"Let's chat! (type 'finish_chat' to finish the chat)\")\n",
        "while True:\n",
        "    q_text = input(\"You: \")\n",
        "    q_text = q_text\n",
        "    if q_text == 'finish_chat':\n",
        "        break\n",
        "\n",
        "    # Esto se tuvo que cambiar\n",
        "    #X = torch.tensor([vocab.stoi[t] for t in tokenizer(q_text)]).to(device)\n",
        "    # Por esto:\n",
        "    X = torch.tensor([stoi[t] for t in tokenizer(q_text)]).to(device)\n",
        "    output = model(X, torch.tensor([0], dtype=torch.long).to(device))\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "    tag = labels[predicted.item()]\n",
        "\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    prob = probs[0][predicted.item()]\n",
        "    if prob.item() > 0.50:\n",
        "      print(f\"{bot_name}: {random.choice(responses[tag])}\")\n",
        "    else:\n",
        "      print(f\"{bot_name}: My model can't understand you...\")"
      ],
      "metadata": {
        "id": "2F2RZYvEmUaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comente los resultados aquí (0,5 puntos)"
      ],
      "metadata": {
        "id": "5Hu2QTuSURCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "``Comente los resultados aquí``"
      ],
      "metadata": {
        "id": "fdFV63WVUX32"
      }
    }
  ]
}